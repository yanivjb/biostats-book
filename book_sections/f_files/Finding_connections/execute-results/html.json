{
  "hash": "4b3ac18e91fb475c3c539a6e85da5867",
  "result": {
    "engine": "knitr",
    "markdown": "## •  18. F-inding connecTions  {.unnumbered  #finding_connections}\n\n\n\n::: {.cell}\n\n:::\n\n\n::: {.motivation style=\"background-color: #ffe6f7; padding: 10px; border: 1px solid #ddd; border-radius: 5px;\"}\n\n**Motivating Scenario:**\nIn the previous chapter we compared two groups as a two sample t-test. In this chapter, we basically did the same thing as an ANOVA. You want to know the relationship between these approaches.\n\n**Learning Goals: By the end of this subchapter, you should be able to:**\n\n* Explain why a two-sample t-test and a one-way ANOVA with two groups are mathematically equivalent.\n* Describe how **t** and **F** statistics are related and what each measures.\n* Connect $r$ (correlation) and $R^2$ (proportion of variance explained).  \n* Recognize these equivalences as special cases of the general linear model framework. \n\n\n\n:::\n\n---\n\n> Should I conduct an ANOVA or t-test?\n\nThis question is commonly asked by students with data in hand, and even by professors on an exam to evaluate students' understanding. The answer is -- for a case with a binary explanatory variable it makes absolutely no difference. \n\nReassuringly whether conducting and ANOVA or a t-test on a continuous response and a binary explanatory variable we get the exact same p-value. This is reassuring, as we would like our answers to be robust to such an arbitrary choice. It's up to you whether you prefer to present results in terms of the number of standard errors separating the groups, or as a ratio of mean squares. \n\nHere we go through some connections between these modeling approaches. \n\n\n::: {.callout-note}\n**This section is fully optional** I just think it's cool.\n:::\n\n### Comparing F and t \n\nLet's consider our example from this chapter -- comparing the admixture proportion of pink and white flowers at Squirrel Mountain. \n\nA t-test provides the following results: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nt.test(admix_proportion ~ petal_color, data = clarkia_hz, var.equal=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tTwo Sample t-test\n\ndata:  admix_proportion by petal_color\nt = 8.7486, df = 44, p-value = 3.486e-11\nalternative hypothesis: true difference in means between group pink and group white is not equal to 0\n95 percent confidence interval:\n 0.009949917 0.015906235\nsample estimates:\n mean in group pink mean in group white \n        0.018958210         0.006030134 \n```\n\n\n:::\n:::\n\n---\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm(admix_proportion ~ petal_color, data = clarkia_hz) |>\n  anova()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: admix_proportion\n            Df    Sum Sq    Mean Sq F value    Pr(>F)    \npetal_color  1 0.0018312 0.00183122  76.539 3.486e-11 ***\nResiduals   44 0.0010527 0.00002393                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n---\n\n**The most obvious similarities here are:**    \n\n- The p-values are identical, and \n- $\\text{df}_\\text{error}$ for the ANOVA equals $\\text{df}$ for the t-test. .  \n\n**But wait, there's more:** \n\n- Additionally the F value (76.539), is simply the t-value (8.7486) squared.   \n- Finally, although R did not show us this,  the Mean Squares error of  0.0000239 equals the pooled variance: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# finding the pooled variance\nclarkia_hz |>\n    filter(!is.na(petal_color))|>\n    group_by(petal_color)|>\n    mutate(mean_admix = mean(admix_proportion ))|>\n    ungroup()|>\n    mutate(deviation = admix_proportion - mean_admix) |>\n    summarise(pooled_var = sum(deviation^2) / (n() - 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  pooled_var\n       <dbl>\n1  0.0000239\n```\n\n\n:::\n:::\n\n\n---\n\nThis is all to say that the two sample t-test and an ANOVA with two groups are essentially identical. I hope this makes us feel good about moving on to more complex ANOVAs, and about linear models more broadly.\n\n:::note\n**Why does $F = t^2$?**\n\n- t measures the difference between group means in standard error units (i.e., how many standard deviations of the sampling distribution separate the groups).  \n\n- F compares the variance between groups to the variance within groups, so it's measured on the variance scale.  \n\nBecause variance is just the square of a standard deviation, F is simply the square of the corresponding t when there are two groups.\n\n:::\n\n### $R^2$ is the square of $r$  \n\nRemember that $R^2$ is the \"proportion of variance explained\" (i.e. the proportion of variance in our response variable that we can pin on our explanatory variable).\n\n:::aside\n$$R^2 = \\frac{\\text{SS}_\\text{model}}{\\text{SS}_\\text{total}}$$. \n\n:::\n\nWe can find $R^2$ from the output of our linear model: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(broom)\naov(admix_proportion ~ petal_color, data = clarkia_hz) |>\n    glance()|> \n    kable()\n```\n\n::: {.cell-output-display}\n\n\n|   logLik|       AIC|       BIC|  deviance| nobs| r.squared|\n|--------:|---------:|---------:|---------:|----:|---------:|\n| 180.4843| -354.9686| -349.4827| 0.0010527|   46| 0.6349717|\n\n\n:::\n:::\n\n\n---\n\n\nYou also may remember that $r$ is the correlation between variables. If we assign a numeric value of zero to the explanatory variable for one category (e.g., white flowers), and one for the other category (e.g., pink flowers), we can calculate a correlation ($r$). If we square this correlation $r$ squared equals $R^2$!\n\n:::aside\n$$r = \\frac{ \\text{cov}_{x,y} }{s_x \\times s_y}$$ \n:::\n\n\n\n\n\n\n::: {.cell .column-margin layout-align=\"center\"}\n::: {.cell-output-display}\n![The spiderman meme for ANOVA / regression](https://www.accountingexperiments.com/media/anova_regression_hu8d7b78fdb8970f110a404e1ae11b4c8f_365153_1200x1200_fit_q85_h2_lanczos_3.webp){fig-align='center' fig-alt='Two identical Spider-Man characters point at each other. One is labeled \"ANOVA,\" the other \"Regression.\"' width=95%}\n:::\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nclarkia_hz |> \n  filter(!is.na(petal_color))|>\n  mutate(petal_color_num = ifelse(petal_color == \"white\", 0 ,1))|>\n  summarise(r = cor(petal_color_num , admix_proportion),\n            r.squared = r^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n      r r.squared\n  <dbl>     <dbl>\n1 0.797     0.635\n```\n\n\n:::\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}