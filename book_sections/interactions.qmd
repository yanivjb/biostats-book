# 23. Interactions {.unnumbered #interactions}



```{r}
#| echo: false
#| message: false
#| warning: false
library(ggExtra)
library(knitr)
library(dplyr)
library(kableExtra)
library(ggfortify)
library(emmeans)
library(readr)
library(stringr)
library(infer)
library(ggthemes)
library(DT)
library(webexercises)
library(ggplot2)
library(tidyr)
library(cowplot)
library(gridExtra)
library(janitor)
library(forcats)
library(broom)
source("../_common.R") 

hz_link <- "https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/zones_df_admix_weight_cutoff0.9_gps_dist2het_phenos_excl_GC.csv"

clarkia_hz <- read_csv(hz_link)|> 
  select(-weighted)|>
  rename(admix_proportion = `cutoff0.9`) |>
  dplyr::filter(!is.na(admix_proportion))|>
  clean_names() |>
  mutate(site = if_else(site == "SAW", "SR", site)) |>
  dplyr::filter(subsp=="P")|>
  mutate(site = fct_reorder(site, mean_petal_area_sq_cm))
gg_color_hue <- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}
dung <- tibble(female = c("uk",rep(c("swiss","swiss","uk","uk"), each= 15)[-1]),
               male = rep(c("uk","swiss","uk","swiss"), each= 15),
               sire.2nd.male =  c(74, 77, 72, 81, 75, 76, 78, 68, 81, 74, 71, 58,  
                  67, 63, 83, 74, 63, 51, 63, 69, 61, 34, 51, 62, 61, 69, 59, 72,  
                  51, 56, 58, 73, 88, 80, 67, 65, 95, 72, 75, 69, 70, 56, 66, 60,
                  81, 92, 96, 98, 99, 85, 84, 93, 75, 93, 99, 92, 93, 93, 100, 97))

```






<span style="color: Blue;font-size:22px;">   Motivating scenarios:  </span>  <span style="color: Black;font-size:18px;">   We have numerous explanatory variables and want to develop an synthetic model. We are particularly interested in hypotheses in which the reponse variable may depend on an interaction between explanatory variables.  </span>

**Learning goals: By the end of this chapter you should be able to**    

- Write down and interpret longer and more complex linear models.   
- Interpret  models with interactions and run them in R R.      
- Calculate Type I, II, and II sums of squares and recognize when one is most appropriate.   



## Review  of Linear  Models  

A linear model predicts the response variable, as $\widehat{Y_i}$ by adding up all components of the model.  


```{r}
#| column: margin
#| echo: false
include_graphics("https://raw.githubusercontent.com/allisonhorst/stats-illustrations/master/other-stats-artwork/dragon_predict_mlr.png")
```


\begin{equation}
\hat{Y_i} = a + b_1  y_{1,i} + b_2 y_{2,i} + \dots{}
(\#eq:predlong)
\end{equation}



Linear models we have seen   

We have  recently  extended our single factor linear models (e.g. one and two sample t-tests, ANOVAs, regression, etc.) to include more factors:   

- We could include a linear term and its squared value to predict a quantitative outcome in  a polynomial regression.  $\widehat{Y_{i}} = a + b_{1,i} \times y_1 +  b_{2,i} \times y_1^2$...  
- A linear model in which we predict a quantitative response as a function of two categorical variables (two factor ANOVA without an interaction)      
- A linear model in which we predict a quantitative response as a function of one continuous variable that we want o account for (aka a covariate) before  considering the effect of a categorical predictor (ANCOVA).      

Here we look at  these in more detail. Specifically, we consider   

- Cases in which the response is predicted by an interaction between explanatory variables    
- Different ways to attribute sums of squares, depending on your goals / motivation.  



## Statistical interactions    


```{r, echo=FALSE}
#| column: margin
tibble( yumm = c(0,5,0,-5), drink = c("Water","OJ","Water","OJ"), dental_hygine = c("floss", "floss","toothpaste","toothpaste")) %>% mutate(drink = fct_rev(drink)) %>%
  ggplot(aes(x = drink, y = yumm, color =  dental_hygine , group  =  dental_hygine ))+
  geom_line( show.legend = FALSE,linewidth = 2)+
  geom_point( size = 5 , show.legend = FALSE )+
  geom_text(data = tibble( yumm = c(4.75,-4.75), x = c(1.6,1.6), dental_hygine = c("floss","toothpaste")),
  aes(label = dental_hygine,x = x), show.legend = FALSE,size = 10)+
  theme(axis.text = element_text(size = 20),
        axis.title = element_text(size = 24))

```

I like orange juice in the morning. I also like the taste of minty fresh toothpaste. But drinking orange juice after brushing my teeth tastes terrible.    

**This is an example of an interaction.**    

- On its own, toothpaste tastes good.
- On its own OJ is even better.
- Put them together, and you have something gross.   




This is an extreme case. More broadly, an interaction is any case in which the slope of two lines differ -- that is to say when the effect of one variable  on an outcome depends on the value of another variable.


```{r, echo=FALSE}
#| column: margin
tibble( danger = c(.2,.01,15,1), intoxicated = c("Yes","No","Yes","No"), activity = c("sleeping", "sleeping","driving","driving")) %>%
  ggplot(aes(x = intoxicated, y = danger, color =  activity, group  =  activity ))+
  geom_line( show.legend = FALSE,linewidth = 2)+
  geom_point( size = 5 , show.legend = FALSE )+
  geom_text(data = .%>% dplyr::filter(intoxicated == "Yes"),aes(label =activity), position = position_nudge( x= c(.3,.3)), show.legend = FALSE,size = 10)+
  theme(axis.text = element_text(size = 20),
        axis.title = element_text(size = 24))
```




**Another example of an interaction** Getting intoxicated is a bit dangerous. Driving is a bit dangerous. Driving while intoxicated is more dangerous than adding these up individually.


###  Visualizing main & interactive effects

```{r,echo=FALSE, fig.height=4, fig.width=4.7, out.extra='style="float:right;padding-left: 5px"'}
#library(car)

par(mfrow = c(2,2),mar = c(3,3,3,1))
plot(0, xlim = c(0,1), ylim = c(-.05,1.05), type="n", axes = FALSE, xlab = "",ylab="", main = "Main effect of A")
axis(1,c(.15,.85), c(expression(A[1]), expression(A[2])), tick = FALSE); axis(2,c(-10,10), c("",""))
axis(1,c(-10,10))
segments( x0 = c(.15,.14) ,  y0 = c(.85,.84),  x1 = c(.84,.85), y1 = c(.14,.15) , col = c("#1f78b4","#b2df8a"), lwd = 3)
mtext("Y",side=2, line = 1)
text(x = c(.15,.15), y = c(.98,.72),
     labels = c(expression(B[1]), expression(B[2])),col = c("#1f78b4","#b2df8a"))


plot(0, xlim = c(0,1), ylim = c(-.1,1.05), type="n", axes = FALSE, xlab = "",ylab="", main = "Main effect of B")
axis(1,c(.15,.85), c(expression(A[1]), expression(A[2])), tick = FALSE); axis(2,c(-10,10), c("",""))
axis(1,c(-10,10))
segments( x0 = rep(.15,2) ,  y0 = jitter( c(.85,.15), amount = .1),  x1 = rep(.85,2), y1 = jitter( c(.85,.15), amount = .05) , col = c("#1f78b4","#b2df8a"), lwd = 3)
mtext("Y",side=2, line = 1)
text(x = c(.15,.15), y = c(.95,.05),
     labels = c(expression(B[1]), expression(B[2])),col = c("#1f78b4","#b2df8a"))


plot(0, xlim = c(0,1), ylim = c(-.1,1.05), type="n", axes = FALSE, xlab = "",ylab="", main = "Main effect of B \n Interaction between A & B")
axis(1,c(.15,.85), c(expression(A[1]), expression(A[2])), tick = FALSE); axis(2,c(-10,10), c("",""))
axis(1,c(-10,10))
segments( x0 = rep(.15,2) ,  y0 = jitter( c(.6,.4), amount =  0.025),  x1 = rep(.85,2),   y1 = jitter(c(.99, .01), amount = 0.025) , col = c("#1f78b4","#b2df8a"), lwd = 3)
mtext("Y",side=2, line = 1)
text(x = c(.15,.15), y = c(.75,.25),
     labels = c(expression(B[1]), expression(B[2])),col = c("#1f78b4","#b2df8a"))

plot(0, xlim = c(0,1), ylim = c(-.05,1.05), type="n", axes = FALSE, xlab = "",ylab="", main = "Interaction between A & B")
axis(1,c(.15,.85), c(expression(A[1]), expression(A[2])), tick = FALSE);
axis(1,c(-10,10))
axis(2,c(-10,10), c("",""))
segments( x0 = rep(.15,2) ,  y0 = jitter( c(.85,.15), amount = .1),  x1 = rep(.85,2), y1 = jitter( c(.25,.85), amount = .25) , col = c("#1f78b4","#b2df8a"), lwd = 3)
mtext("Y",side=2, line = 1)
text(x = c(.15,.15), y = c(1,.05),
     labels = c(expression(B[1]), expression(B[2])),col = c("#1f78b4","#b2df8a"))
```




There are many possible outcomes when looking into a model with two predictors and the potential for an interaction. I outline a few extreme possibilities, plotted on the right.

- We can see an effect of only variable **A** on Y. That is, lines can have nonzero slopes ("*Main effect of A*").   
- We can see a effect of only variable **B** on Y. That is, lines can have zero slopes but differing intercepts ("*Main effect of B*").   
- We can see an effect of only variable **B** on Y, but an interaction between that variable and the other. That is, intercepts and slopes can differ (or vice versa) in such a way that the mean of Y only differs by one of the explanatory variables (e.g. "*Main effect of B, Interaction between A & B*").    and/or    
- We can have only an interaction. That is, on their own values of A or B have no predictive power, but together they do.  (different slopes)      






## Interaction case study   


```{r}
#| column: margin
#| echo: false
include_graphics("https://upload.wikimedia.org/wikipedia/commons/thumb/f/fd/Dungfliegen_bei_der_Paarung_-_Scatophaga_sp.jpg/640px-Dungfliegen_bei_der_Paarung_-_Scatophaga_sp.jpg")
```



Females of the yellow dung fly, *Scathophaga atercoraria*, mate with multiple males and the sperm of different males "compete" to fertilize her eggs.  

What determines whether a sperm is competitive? To find out, @hosken2002 tested if/how the origin <font color = "lightgrey">(from UK or Switzerland)</font> of males and females (and their interaction) influence the percentage of offspring sired by the second male.

So our model is:   

$$\text{SIRING } 2^\text{ND}\text{ MALE} = \text{FEMALE} + \text{MALE} + \text{FEMALE} \times \text{MALE}$$   

### Biological hypotheses

There are a few possibilities.    

-   Perhaps females from the UK populations reject (or accept) more sperm from the second male than do females from Sweden (main effect of female population).        
- Perhaps sperm from UK males have more (or less) siring  success than sperm from UK males  second male (main effect of male population).   
- Or maybe, sperm from Swedish males has high siring success with Swedish females, and  sperm from UK males has high siring success with UK females (interaction), suggesting harmonious co-adaptation.  
- Or maybe females have evolved to resist local sperm  so sperm from Swedish males has high siring success with UK females but low siring success with Swedish females, suggesting a conflict between the sexes.   

etc... etc...  



### The data

The raw data are available here if you want to follow along.  

:::aside
<span style="color: LightGrey;">Before we analyze it, I am confessing to changing it for teaching purposes. Specifically I am taking the first data point and saying it came from a mating between a UK female and a swiss male.</span>
:::

```{r}
dung <- tibble(female = c("uk",rep(c("swiss","swiss","uk","uk"), each= 15)[-1]),
               male = rep(c("uk","swiss","uk","swiss"), each= 15),
               sire.2nd.male =  c(74, 77, 72, 81, 75, 76, 78, 68, 81, 74, 71, 58,  
                  67, 63, 83, 74, 63, 51, 63, 69, 61, 34, 51, 62, 61, 69, 59, 72,  
                  51, 56, 58, 73, 88, 80, 67, 65, 95, 72, 75, 69, 70, 56, 66, 60,
                  81, 92, 96, 98, 99, 85, 84, 93, 75, 93, 99, 92, 93, 93, 100, 97))
```


Looking at the means and standard errors below, we see that      

- UK males have similar mating success with both UK and Swedish females.
- Swedish males seem to have remarkably low success with Swedish females and remarkably high success with  UK females. This suggests a history of conflict between the sexes over the success of the second male.      



```{r echo=FALSE, message=FALSE,warning=FALSE}
dung %>%
  mutate(male = paste(male,"male"))%>%
  group_by(female, male) %>%
  summarise(siring = round(mean(sire.2nd.male), digits = 2),
            se     = round(sd(sire.2nd.male)/ sqrt(n()),digits = 1),
            summa  = paste(siring," (",se,")",sep = "")) %>%
  ungroup() %>%
  pivot_wider(id_cols = female, names_from = male, values_from = summa)%>%
  kbl(caption = "Mean (se) male siring success by female population of origin", digits = 2,align = "l") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

### Fitting a linear model  with an interaction in R  

In R we can designate an interaction in addition to main effects with a colon, `:`. Or we can have R do a full model with a `*`. So the two models below are identical

```{r}
lm_dung_fmi <- lm(sire.2nd.male ~ female + male + female:male, dung)
lm_dung_fmi <- lm(sire.2nd.male ~ female*male , dung)
broom::tidy(lm_dung_fmi)
```



As usual, we can look at our estimates and uncertainty in them with the `summary.lm()` <span style="color: LightGrey;">or equivalently, the `tidy()` function</span>. As like most linear model we know that these p and t-values <span style="color: LightGrey;">(which shows up at `statistic` in `tidy()`)</span> do not necessarily describe things we care about, and may be misleading.  

Still, we can use this output to make predictions. The first terms should look familiar. For the final term `femaleuk:maleuk` we multiply `femaleuk` (0 or 1), by `maleuk` (0 or 1) as the thing we multiply this effect by.  That is, when the male and female are from the uk this is 1, and second males in this cross have 34.2% less siring success than we would predict if we added up the effect of male uk and female uk.  Because multiplying `femaleuk` (0 or 1), by `maleuk` (0 or 1) is zero in all other cases, it only shows up in this one cross.  

We can see this in the design matrix (below). Remember for the linear algebra fans that we make predictions as the dot product of our model coefficients and the design matrix.



```{r}
model.matrix(lm_dung_fmi)                                                                                                        %>%  DT::datatable(options = list(autoWidth = TRUE,pageLength = 5, lengthMenu = c(5, 25, 50)))
```

Or, if you don't want to mess with linear algeba, the model predicts siring success of the second male as follows



\begin{equation}
\begin{split}
                                               & \text{intercept}   &\text{female UK?}&\text{male UK?}&\text{both UK?}&\widehat{Y_i}\\
\widehat{Y}_\text{female.swiss x male.swiss} =  &59.733 \times 1+  &32.867 \times  0 + &13.410 \times 0 -&34.197\times 0  =&59.733\\
\widehat{Y}_\text{female.swiss x male.uk} =     &59.733 \times 1+  &32.867 \times  0 + &13.410 \times 1 -&34.197\times 0  =&73.14\\
\widehat{Y}_\text{female.uk    x male.swiss} =  &59.733 \times 1+  &32.867 \times  1 + &13.410 \times 0 -&34.197\times 0  = &92.60\\
\widehat{Y}_\text{female.uk    x male.uk} =     &59.733 \times 1+  &32.867 \times  1 + &13.410 \times 1 -&34.197\times 1  = &71.83\\
\end{split}
\end{equation}


We can see that the model predictions simply describe our four sample means.  

## Hypothesis testing  

### Statistical  hypotheses   

We can translate the biological hypotheses into three pairs of statistical null and alternative hypotheses.    

We examine three null and alternative hypotheses

1. Main effect of Male   
    - <font color = "purple">$H_0:$ Siring success is independent of male origin.</font>  
    - <font color = "orange">$H_A:$ Siring success differs by male origin.</font>  
2. Main effect of Female
    - <font color = "purple">$H_0:$ Siring success is independent of female origin.</font>  
    - <font color = "orange">$H_A:$ Siring success differs by female origin.</font>  
3. Interaction between Male and Female
    - <font color = "purple">$H_0:$ The influence of male origin on siring success does not differ by female origin.</font>     
    - <font color = "orange">$H_0:$ The influence of male origin on siring success is depends on female origin.</font>


We visualize these hypotheses below  

```{r, fig.height = 3, fig.width=7.6, echo=FALSE}
q0 <- ggplot(data = dung , aes(x = male, y = sire.2nd.male, color = female,lty = female, shape = female, group = female)) +
  geom_jitter(width = .1, height = 0, size = 3,show.legend = FALSE, alpha = .3) +
  ylab("Kids from 2nd male") +
  annotate(geom = "text", x = .6, y = c(90,70,80), label = c("female","swiss","UK"), color = c("black",gg_color_hue(2)))+
  xlab("2nd Male")+
  theme_tufte()

qa <-q0 +
  geom_hline(yintercept = mean(dung$sire.2nd.male), lty =2) +
  labs(title = "Null Model",subtitle = "Siring is independent of male, female, and their interaction")

qb <- q0 +
  labs(title = "Alternative Model 1")+
  geom_segment(data = lm(sire.2nd.male ~ factor(male),    data = dung)%>%
              augment()%>%
              dplyr::select(-sire.2nd.male)%>%
              rename(sire.2nd.male = .fitted, male = `factor(male)` ) %>%
              mutate(female = dung$female),
              aes(x = as.numeric(male) - .2, xend = as.numeric(male)+.2,
                  y = sire.2nd.male , yend = sire.2nd.male ),
              lty = 2,show.legend = FALSE, color = "purple")

qc <- q0 +
  labs(title = "Alternative Model 3")+
  geom_segment(data = lm(sire.2nd.male ~ factor(female),    data = dung)%>%
              augment()%>%
              dplyr::select(-sire.2nd.male)%>%
              rename(sire.2nd.male = .fitted, female =`factor(female)` ) %>%
              mutate(male = factor(dung$male)),
              aes(x = as.numeric(male) - .2, xend = as.numeric(male)+.2,
                  y = sire.2nd.male , yend = sire.2nd.male ),
              lty = 2,show.legend = FALSE)

qd <- q0 +
  labs(title = "Alternative Model 4")+
  geom_segment(data = lm(sire.2nd.male ~ female + male,    data = dung %>% mutate(male = factor(male), female = factor(female)))%>%
              augment()%>%
              dplyr::select(-sire.2nd.male)%>%
              rename(sire.2nd.male = .fitted),
              aes(x = as.numeric(male) - .2, xend = as.numeric(male)+.2,
                  y = sire.2nd.male , yend = sire.2nd.male ),
              lty = 2,show.legend = FALSE)+
  geom_line(data = lm(sire.2nd.male ~ female + male,    data = dung %>%
                        mutate(male = factor(male),
                               female = factor(female)))%>%
              augment()%>%
              dplyr::select(-sire.2nd.male)%>%
              rename(sire.2nd.male = .fitted), lty=2,show.legend = FALSE)

qe<- q0 +
  labs(title = "Alternative Model 5")+
  geom_segment(data = lm(sire.2nd.male ~ female:male,    data = dung %>% mutate(male = factor(male), female = factor(female)))%>%
              augment()%>%
              dplyr::select(-sire.2nd.male)%>%
              rename(sire.2nd.male = .fitted),
              aes(x = as.numeric(male) - .2, xend = as.numeric(male)+.2,
                  y = sire.2nd.male , yend = sire.2nd.male ),
              lty = 2,show.legend = FALSE)+
  geom_line(data = lm(sire.2nd.male ~ female : male,    data = dung %>%
                        mutate(male = factor(male),
                               female = factor(female)))%>%
              augment()%>%
              dplyr::select(-sire.2nd.male)%>%
              rename(sire.2nd.male = .fitted), lty=2,show.legend = FALSE)

qf<- q0 +
  labs(title = "Alternative Model 6")+
  geom_segment(data = lm(sire.2nd.male ~ male + female:male,    data = dung %>% mutate(male = factor(male), female = factor(female)))%>%
              augment()%>%
              dplyr::select(-sire.2nd.male)%>%
              rename(sire.2nd.male = .fitted),
              aes(x = as.numeric(male) - .2, xend = as.numeric(male)+.2,
                  y = sire.2nd.male , yend = sire.2nd.male ),
              lty = 2,show.legend = FALSE)+
  geom_line(data = lm(sire.2nd.male ~ male + female : male,    data = dung %>%
                        mutate(male = factor(male),
                               female = factor(female)))%>%
              augment()%>%
              dplyr::select(-sire.2nd.male)%>%
              rename(sire.2nd.male = .fitted), lty=2,show.legend = FALSE)

qg<- q0 +
  labs(title = "Alternative Model 7")+
  geom_segment(data = lm(sire.2nd.male ~ female + female:male,    data = dung %>% mutate(male = factor(male), female = factor(female)))%>%
              augment()%>%
              dplyr::select(-sire.2nd.male)%>%
              rename(sire.2nd.male = .fitted),
              aes(x = as.numeric(male) - .2, xend = as.numeric(male)+.2,
                  y = sire.2nd.male , yend = sire.2nd.male ),
              lty = 2,show.legend = FALSE)+
  geom_line(data = lm(sire.2nd.male ~ female + female : male,    data = dung %>%
                        mutate(male = factor(male),
                               female = factor(female)))%>%
              augment()%>%
              dplyr::select(-sire.2nd.male)%>%
              rename(sire.2nd.male = .fitted), lty=2,show.legend = FALSE)

qh<- q0 +
  labs(title = "Full Alternative Model", subtitle = "Siring depends on male, female, and their interaction")+
  geom_segment(data = lm(sire.2nd.male ~ female * male,    data = dung %>% mutate(male = factor(male), female = factor(female)))%>%
              augment()%>%
              dplyr::select(-sire.2nd.male)%>%
              rename(sire.2nd.male = .fitted),
              aes(x = as.numeric(male) - .2, xend = as.numeric(male)+.2,
                  y = sire.2nd.male , yend = sire.2nd.male ),
              lty = 2,show.legend = FALSE)+
  geom_line(data = lm(sire.2nd.male ~ female * male,    data = dung %>%
                        mutate(male = factor(male),
                               female = factor(female)))%>%
              augment()%>%
              dplyr::select(-sire.2nd.male)%>%
              rename(sire.2nd.male = .fitted), lty=2,show.legend = FALSE)

grid.arrange( qa +theme(axis.line = element_line()),qh+theme(axis.line = element_line()),
  nrow = 1)
```

### Evaluating assumptions   

Remember that linear models assume

- **Linearity:** That observations are appropriately modeled by adding up all predictions in our equation.      
- **Homoscedasticity:** The variance of residuals is independent of the predicted value, $\hat{Y_i}$  is the same for any value of X.    
- **Independence:** Observations are independent of each other (aside from the predictors in the model).  
- **Normality:**  That residual values are normally distributed.     
- **Data are collected without bias**   as usual.   

Taking a look at our diagnostic plots, it looks like data meet test assumptions:

```{r, message=FALSE, warning=FALSE, fig.height=2, fig.width=7}
autoplot(lm_dung_fmi,nrow = 1, which =c(2,3,5), label = FALSE)
```


### Hypothesis testing in an ANOVA framework: Types of Sums of Squares    

**tl/dr**

- We need to come up with fair ways to attribute variance in larger linear models.
    - Type I Sums of Squares (calculated with the `anova()` function) are most appropriate when we want to take a "covariate into account". The order we enter terms into our model can change p and f values with Type I sums of Squares.     
    - Type II Sums of Squares are most appropriate when we are equally interested in all main effects as interesting hypotheses.    
    - Type III Sums of Squares are most appropriate when we are equally interested in all main and interaction effects as interesting hypotheses. want to take a "covariate into account".   
- We can specify which type of sums of squares we want to use with the [`Anova()`](http://finzi.psych.upenn.edu/R/library/car/html/Anova.html) function in the `car` package.    
- If our study is "balanced" type I and type II sums of squares will give identical answers.  
- All of these can go a bit weird in different cases, especially with unbalanced designs.   

For our case, a Type III Sum of Squares is probably most approriate.   

#### **Type I and Type II Sums of Squares**   {-}

In the previous chapter, we calculated *Type I* and *Type II* sums of squares, and saw that Type I Sums of Square -- in which the order of the variables changed our significance claims -- are rarely a good idea. 

**Recal that with Type II sums of squares**, we found $SS_\text{focal variable}$ as $SS_\text{full model}- SS_\text{full model except focal variable}$. If we are also modlling an interaction, we calculate type II sums of sqaures as: 

$$SS_\text{focal variable} = SS_\text{model with all main effects} - SS_\text{model with all main effects except the focal variable}$$

Then, after calculating sums of squares for the main effects, we calculate the sums of squares for the interaction as:

$$SS_\text{interaction} = SS_\text{full model with focal interaction} - SS_\text{full model without focal interaction}$$


We can calculate any type of sums of squares with the [`Anova()`](http://finzi.psych.upenn.edu/R/library/car/html/Anova.html) function in the `car` package.

```{r}
library(car)
Anova(lm_dung_fmi, type = "II")
```

There are two weird things about Type II Sums of squares:  

- They don't sum to total sums of squares.    
- It's a bit weird to talk about the significance of a main effect without considering an interaction.  


####  **Type III Sums of Squares**    {-}  

If we care about an interaction and main effects it makes sense to not leave interactions for last.  In this case, we calculate Type III Sums of Squares for each variable and interaction by comparing the predictions with the full model to predictions with everything but the variable (or interaction) we're interested in. Again we can do this with the [`Anova()`](http://finzi.psych.upenn.edu/R/library/car/html/Anova.html) function in the `car` package. Again, order won't matter.  


```{r}
Anova(lm_dung_fmi, type = "III")
```

####  **Type II or Type III Sums of Squares**    {-}  

I find it difficult to think about which type of sums of squares to use. In general, I prefer to use Type II sums of squares when interactions are subtle, and Type III when interactions are strong enough that main effects become difficult to interpret or explain.

##  Biological coclusions for case study


We conclude that female (F = 102.259, df = 1, p = $3\times 10e^{-14}$) and male (F = 16.4, df = 1, p = 0.00016) population of origin, and their interaction (F = 55.2, df =1, p = $6.7 \times 10^{-10}$) all impact the siring success of the second male. Most notably, the interaction between male and female population of origin seemed to matter substantially -- when females and second males where both from the UK siring success was way lower that expected given each main effect. This suggests that sexual conflict in the UK might modulate the competitiveness of second sperm. Future work should address why the siring success of second males from Sweden was not notably impacted by maternal population of origin.  

## Quiz

```{r, echo=FALSE}
include_app("https://brandvain.shinyapps.io/interactionsandstuff/", height = "800")
```
