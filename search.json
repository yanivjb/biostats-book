[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Biostatistics",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#learning-in-this-era",
    "href": "index.html#learning-in-this-era",
    "title": "Applied Biostatistics",
    "section": "Learning in this era",
    "text": "Learning in this era\nI know you’re dealing with a lot. Every year students are dealing with a lot – from jobs, to supporting family, and all the other challenges of modern college life. Yet, we are all trying to make the most of life in this era. We want to teach, learn, and grow.\nMoreover, I believe this content is increasingly important – statistics is obsessed with the critical evaluation of claims in the face of data, and is therefore particularly useful in uncertain times. Given this focus, and given that you all have different energies, motivations and backgrounds, I am restructuring this course slightly from previous years. The biggest change is a continued de-emphasis on math and programming – that doesn’t mean I’m eliminating these features, but rather that I am streamlining the required math and programming to what I believe are the essentials. For those who want more mathematical and/or computational details (either because you want to push yourself or you need this to make sense of things), I am including a bunch of optional content and support. I am also wrestling with the impact of LLMs in our education (more below).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#i-love-teaching-this-course",
    "href": "index.html#i-love-teaching-this-course",
    "title": "Applied Biostatistics",
    "section": "I love teaching this course",
    "text": "I love teaching this course\nThe content is very important to me. I also care deeply about you. I want to make sure you get all you can / all you need from this course, while recognizing the many challenges we are all facing. One tangible thing I leave you with is this book, which I hope you find useful as you go on in your life. Another thing I leave you with is my concern for your well-being and understanding – please contact me with any suggestions about the pace, content, or structure of this course and/or any life updates which may change how and when you can complete the work.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#course-philosophy-goals",
    "href": "index.html#course-philosophy-goals",
    "title": "Applied Biostatistics",
    "section": "Course philosophy / goals",
    "text": "Course philosophy / goals\nMy motivating goal for this course is to empower you to produce, present, and critically evaluate statistical evidence — especially as applied to biological topics. You should know that statistical models are only models and that models are imperfect abstractions of reality. You should be able to think about how a biological question could be formulated as a statistical question, present graphs which show how data speak to this question, be aware of any shortcomings of that model, and how statistical analysis of a data set can be brought back into our biological discussion.\n\n“By the end of this course…\n\nStudents should be statistical thinkers. \nStudents will recognize that data are comprised of observations that partially reflect chance sampling, & that a major goal of statistics is to incorporate this idea of chance into our interpretation of observations. Thinking this way can be challenging because it is a fundamentally new way to think about the world. Once this is mastered, much of the material follows naturally. Until then, it’s more confusing.\n\n\n Students should think about probability quantitatively.\nThat chance influences observations is CRITICAL to statistics (see above). Quantitatively translating these probabilities into distributions and associated statistical tests allows for mastery of the topic.\n\n\n Students should recognize how bias can influence our results. \nNot only are results influenced by chance, but factors outside of our focus can also drive results. Identifying subtle biases and non-independence is key to conducting and interpreting statistics.\n\n\n Students should become familiar with standard statistical tools / approaches and when to use them. \nRecognize how bias can influence our results. What is the difference between Bayesian and frequentist thinking? How can data be visualized effectively? What is the difference between statistical and real-world significance? How do we responsibly present/ interpret statistical results? We will grapple with & answer these questions over the term.\n\n\n Students should have familiarity with foundational statistical values and concepts. \nStudents will gain an intuitive feel for the meaning of stats words like variance, standard error, p-value, t-statistic, and F-statistic, and will be able to read and interpret graphs, and how to translate linear models into sentences.\n\n\n Students should be able to conduct the entire process of data analysis in R. \nStudents will be able to utilize the statistical language, R, to summarize, analyze, and combine data to make appropriate visualizations and to conduct appropriate statistical tests.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#r-rstudio-and-the-tidyverse",
    "href": "index.html#r-rstudio-and-the-tidyverse",
    "title": "Applied Biostatistics",
    "section": "R, RStudio, and the tidyverse",
    "text": "R, RStudio, and the tidyverse\n\n\n\n\n\n\nThis image comes with permissions from Allison Horst, who makes tremendous aRt. If you appreciate her work, she would appreciate your support for Data for Black Lives\n\n\n\nWe will be using R (version 4.4.0 or above.) in this course, in the RStudio environment. My goal is to have you empowered to make figures, run analyses, and be well positioned for future work in R, with as much fun and as little pain as possible. RStudio is an environment and the tidyverse is a set of R packages that makes R’s powers more accessible without the need to learn a bunch of computer programming.\nSome of you might have experience with R and some may not. Some of this experience might be in tidyverse or not. There will be ups and downs — the frustration of not understanding and/or it not working and the joy of small successes. Remember to be patient, forgiving and kind to yourself, your peers, and me. Ask for help from the internet, your favorite LLM, your friends, your TAs, and your professor.\n\nR Installation\nBefore you can use R you must download and install it.\\(^*\\)  So, to get started, download R from CRAN, and follow the associated installation instructions (see below for detailed instructions for your system).\\(^*\\) This is not strictly true. You can use R online via posit cloud. This is a “freemium” service and the free plan is unlikely to meet your needs.\n\nPC install guideMac install guideLinux install guide\n\n\n\nIf you want a walk through, see Roger Peng’s tutorial on installing R on a PC youtube link here.\n“To install R on Windows, click the Download R for Windows link. Then click the base link. Next, click the first link at the top of the new page. This link should say something like Download R 4.4.2 for Windows except the 4.4.2 will be replaced by the most current version of R. The link downloads an installer program, which installs the most up-to-date version of R for Windows. Run this program and step through the installation wizard that appears. The wizard will install R into your program files folders and place a shortcut in your Start menu. Note that you’ll need to have all of the appropriate administration privileges to install new software on your machine.”\n\nFrom Appendix A of Hands-On Programming With R – Grolemund (2014).\n\n\n\n\n\n\nIf you want a walk through, see Roger Peng’s tutorial on installing R on a mac].\n“To install R on a Mac, click the Download R for macOS link. Next, click on the [newest package link compatible with your computer]. An installer will download to guide you through the installation process, which is very easy. The installer lets you customize your installation, but the defaults will be suitable for most users. I’ve never found a reason to change them. If your computer requires a password before installing new programs, you’ll need it here.”\n\nFrom Appendix A of Hands-On Programming With R – Grolemund (2014).\n\n\n\n\n\n\nR comes pre-installed on many Linux systems, but you’ll want the newest version of R if yours is out of date. The CRAN website provides files to build R from source on [Debian], Redhat, SUSE, and Ubuntu systems under the link “Download R for Linux.” Click the link and then follow the directory trail to the version of Linux you wish to install on. The exact installation procedure will vary depending on the Linux system you use. CRAN guides the process by grouping each set of source files with documentation or README files that explain how to install on your system.\n\nFrom Appendix A of Hands-On Programming With R – Grolemund (2014).\n\n\n\n\n\n\nAfter installing R download/update RStudio from here.\n\nAlternatively you can simply join the course via RStudioCloud. This could be desirable if you do not want to or have trouble doing this.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-is-this-book-and-how-will-we-use-it",
    "href": "index.html#what-is-this-book-and-how-will-we-use-it",
    "title": "Applied Biostatistics",
    "section": "What is this ‘book’ and how will we use it?",
    "text": "What is this ‘book’ and how will we use it?\nA fantastic feature of this book is that it does not stand alone. It is neither the entirety of the course content, nor is it all my idea. In addition from lifting from a few other courses online (with attribution), I also make heavy use of these texts:\n\nThe Analysis of Biological Data Third Edition (Whitlock & Schluter, 2020): I taught with this book for years. It is fantastic and shaped how I think about teaching Biostats. It has many useful resources available online. The writing is great, as are the examples. Most of my material originates here (although I occasionally do things a bit differently). Buy the latest edition.\nCalling Bullshit (Bergstrom & West, 2020): This book is not technical, but points to the big picture concerns of statisticians. It is very practical and well written. I will occasionally assign readings from this book, and/or point you to videos on their website. All readings will be made available for you, but you might want to buy a physical copy.\nFundamentals of Data Visualization (Wilke, 2019): This book is free online, and is very helpful for thinking about graphing data. In my view, graphing is among the most important skills in statistical reasoning, so I reference it regularly.\nR for Data Science (Grolemund & Wickham, 2018): This book is free online, and is very helpful for doing the sorts of things we do in R regularly. This is a great resource.\nThe storytelling with data podcast is a fantastic data viz podcast. Be sure to check out Cole Nussbaumer Knaflic’s books too!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-will-this-term-work-look",
    "href": "index.html#how-will-this-term-work-look",
    "title": "Applied Biostatistics",
    "section": "How will this term work / look?",
    "text": "How will this term work / look?\n\nPrep for ‘class’. This class is flipped with asynchronous content delivery and synchronous meetings.\n\nBe sure to look over the assigned readings and/or videos, and complete the short low-stakes homework BEFORE each course.\n\nDuring class time, I will address questions make announcements, and get you started on in-class work. The TA & I will bounce around your breakout rooms to provide help and check-in. If you cannot make the class, you could do this on your own time without help, but we do not recommend this as a class strategy.\n\nThe help of your classmates and the environment they create is one of the best parts of this class. Help each other.\n\nIn addition to low stakes work before and in class, there will be a few more intense assignments, some collaborative projects, some in class exams, and a summative project as the term ends.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#the-use-of-large-language-models",
    "href": "index.html#the-use-of-large-language-models",
    "title": "Applied Biostatistics",
    "section": "The Use of Large Language Models",
    "text": "The Use of Large Language Models\nWe are in the early days of a truly disruptive technology. Large Language Models (LLMs) like ChatGPT and Claude are transforming how we work and learn. While the impact of these tools on future employment, expertise, and citizenry is yet to be settled, it seems clear that no one will hire you to copy and paste AI-generated output. At the same time, no one will hire you to ignore this technology. Success lies in learning how to critically evaluate and work with LLMs—to validate their output, improve your own understanding, and create high-quality results. Subject-level expertise, in conjunction with strong skills in working with AI, will be essential for the foreseeable future.\n\nYou can use LLMs to learn things or avoid learning things. Choose wisely.\n\nLearning from AI and having it help you solve problems will allow you all to do better and learn more than people have been able to do previously. Using AI to avoid learning – e.g. having it write or code for you without you thinking/learning will always come back to bite you in the ass.\nWhile you are ultimately in charge of your learning, I will provide plenty of opportunities for in-class, computer-free efforts to show your mastery of the subject. I will also provide guidance on individual assignments about the appropriate use of AI to help maximize the impact of the assignment on your learning.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#the-path-through-the-term",
    "href": "index.html#the-path-through-the-term",
    "title": "Applied Biostatistics",
    "section": "The path through the term",
    "text": "The path through the term\nI start by assuming you know nothing about R or statistics to start (I know this assumption is wrong – many of you all know a lot!). From this humble beginning I aim to leave you with the skills to conduct standard statistical analyses, and the understanding of statistics and the ability to go beyond what we have learned. We take the following path in Figure 1, below:\n\n\n\n\n\n\n\n\nFigure 1: Our journey through biostatistics begins with (1) gaining comfort in R, then moves on to (2) describing data and (3) considering sampling and uncertainty. Next, we (4) introduce null hypothesis significance testing, (5) build models, and (6) address more advanced topics (aka “the big lake of statistics”, aka Lake Isabella).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Applied Biostatistics",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\nStudents\nFirst and foremost, I would like to thank the more than 500 students who have taken my Applied Biostatistics course. Students provide the most important feedback on whether a particular pedagogical approach is effective. While not every experiment succeeds, I am incredibly grateful to each student who has helped me learn what works and what doesn’t as they engaged with the material.\n\n\nTeaching Assistants (TAs)\nI have been fortunate to work with outstanding graduate teaching assistants over the past ten years:\n\nDerek Nedveck: Derek played a key role in helping me establish the course during its early years.\nGerman Vargas Gutierrez: A highly skilled statistician, German’s assistance was invaluable in refining the course a few years into its development.\nChaochih Liu: A brilliant programmer, Chaochih contributed greatly to the course’s organization and structure.\nHusain Agha: Husain has remarkable insights into statistics, genetics, and teaching. My work has greatly benefited from bouncing ideas off him.\nBrooke Kern: Brooke was not only an exceptional TA but also a valuable collaborator. Much of the data in this book is drawn from her dissertation research.\n\n\n\n\n\n\n\n\n\n\nFigure 2: My incredible TAs who have all helped shape this material.\n\n\n\n\n\n\n\nCollaborators\nBrooke Kern, Dave Moeller and Shelley Sianta have generated much of the data in this book and have been patient with my delays in turning around our research during teaching times. Dave also provided nearly every picture in this book.\n\n\nTeaching Colleagues\nI have learned a lot about statistics and how to teach it from John Fieberg. His book, Statistics for Ecologists is fantastic! I am also deeply indebted to Fumi Katagiri who began this course and worked through a lot of it before I arrived at UMN, and who thinks deeply about stats and how to teach it.\n\n\nPeople who provided comments\nJohn Rotenberry, and Ruth Shaw have provided helpful comments!\n\n\nUnknowing contributors\nThe online community of statistics and R teaching is an amazing place. I have borrowed heavily from the many amazing free resources. Here are the most critical:\n\nAllison Horst has fantastic illustrations for statistics that she makes freely available.\nPeter D.R. Higgins has created a truly marvelous book – Reproducible Medical Research With R (Higgins (2024)). I have learned a lot and stolen some teaching tricks from this work.\nJenny Bryan has helped me think about getting students able to do things in R well and quickly. Her book, STAT 545: Data wrangling, exploration, and analysis with R (Bryan (2020)), is a classic.\n\n\n\n\n\nBergstrom, C. T., & West, J. D. (2020). Calling bullshit: The art of skepticism in a data-driven world. Random House.\n\n\nBryan, J. J. (2020). STAT 545: Data wrangling, exploration, and analysis with r. Bookdown. https://stat545.com\n\n\nGrolemund, G. (2014). Hands-on programming with r: Write your own functions and simulations. \" O’Reilly Media, Inc.\".\n\n\nGrolemund, G., & Wickham, H. (2018). R for data science.\n\n\nHiggins, P. D. R. (2024). Reproducible medical research with r. Bookdown. https://bookdown.org/pdr_higgins/rmrwr/\n\n\nWhitlock, M. C., & Schluter, D. (2020). The analysis of biological data (Third). Macmillan.\n\n\nWilke, C. O. (2019). Fundamentals of data visualization: A primer on making informative and compelling figures. O’Reilly Media.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "book_sections/clarkia_and_its_data/clarkia_and_its_data.html",
    "href": "book_sections/clarkia_and_its_data/clarkia_and_its_data.html",
    "title": "Motivating biology and datasets",
    "section": "",
    "text": "RILs between sympatric and allopatric parviflora\nWouldn’t it be cool if, at this stage, the populations could evolve a mechanism to preferentially mate with their own kind? The adaptive evolution of avoiding mating with a closely related species—a process known as reinforcement—does just that. However, the evolution of reinforcement is complex and has only been conclusively documented in a handful of cases.\nDave Moeller and colleagues (including me) have been investigating one potential case of reinforcement. Clarkia xantiana subspecies parviflora (hereafter parviflora) is an annual flowering plant native to California. Unlike its outcrossing sister subspecies, Clarkia xantiana subspecies xantiana (hereafter xantiana), parviflora predominantly reproduces through self-pollination.\nNot all populations of parviflora self-fertilize at the same frequency. Dave has observed that populations sympatric with (i.e., occurring in the same area as) xantiana appear more likely to self-fertilize than allopatric populations (Figure 1). Over the past few years, we have conducted numerous studies to evaluate the hypothesis that this increased rate of self-fertilization has evolved via reinforcement as a mechanism to avoid hybridizing with xantiana.\nThroughout this book, I will use data related to the topic of divergence, speciation, and reinforcement between Clarkia subspecies as a path through biostatistics. I hope that this approach allows you to engage with the statistics while not having to keep pace with a bunch of different biological examples. Below, I introduce the major datasets that we will explore.\nFigure 2: Making a RIL population: A cross between individuals from two populations is followed by multiple generations of self-fertilization. As a result, each “line” becomes a mosaic of ancestry blocks inherited from either initial parent of the RIL. The figure above (from Behrouzi & Wit (2017)) illustrates this process, with the original parental chromosome segments depicted in green and red.\nTo investigate which traits, if any, help parviflora populations sympatric with xantiana avoid hybridization, Dave generated Recombinant Inbred Lines (RILs). To do so, he crossed a parviflora plant from “Sawmill Road”—a population sympatric with xantiana—with a parviflora plant from “Long Valley,” far from any xantiana populations. After this initial cross, lines were self-fertilized for eight generations. This process breaks up and shuffles genetic variation from the two parental populations while ensuring each line is genetically stable.\nBy setting these RILs out in the field and observing how many pollinators visited each line, we hope to identify which traits influence pollinator visitation and ultimately hybridization. Because parviflora plants often self-pollinate and because pollinators effectively transfer pollen from the plentiful xantiana plants to parviflora, we assume that greater pollinator visitation corresponds to higher hybrid seed set. However, we will test this assumption!!!",
    "crumbs": [
      "Motivating biology and datasets"
    ]
  },
  {
    "objectID": "book_sections/clarkia_and_its_data/clarkia_and_its_data.html#rils-between-sympatric-and-allopatric-parviflora",
    "href": "book_sections/clarkia_and_its_data/clarkia_and_its_data.html#rils-between-sympatric-and-allopatric-parviflora",
    "title": "Motivating biology and datasets",
    "section": "",
    "text": "RIL Data\nBelow is the RIL dataset. You can learn about the columns (in the Data dictionary tab) and browse the data (in the Data set tab). The full data are available at\nthis link. Aside from pollinator visitation and hybrid seed set, all phenotypes measured come not from the plants in the field, but means from replicates of the genotype grown in the greenhouse.\n\nRIL variationData DictionaryData set\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: An illustration o the variabiltiy in the recombinant inbred lines. Pictures by Taz Mueller and arranged by Brooke Kern.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable_Name\nData_Type\nDescription\n\n\n\n\nril\nCategorical (Factor/String)\nIdentifier for Recombinant Inbred Line (RIL). This is the 'genotype'.\n\n\nlocation\nCategorical (Factor/String)\nField site where the plant was grown.\n\n\nprop_hybrid\nNumeric (discrete)\nProportion of genotyped seeds that were hybrids (see num_hybrid and offspring_genotyped for more information).\n\n\nmean_visits\nNumeric\nAverage number of pollinator visits per plant over a 15-minute observation.\n\n\ngrowth_rate\nNumeric\nGrowth rate of the plant.\n\n\npetal_color\nCategorical (Binary)\nPetal color phenotype (in this case 'pink' or 'white').\n\n\npetal_area_mm\nNumeric\nDate when the first flower opened (in Julian days, i.e., days since New Year's).\n\n\ndate_first_flw\nDate\nNode position of the first flower on the stem.\n\n\nnode_first_flw\nNumeric\nPetal area measured in square millimeters (mm²).\n\n\npetal_perim_mm\nNumeric\nPetal perimeter measured in millimeters (mm).\n\n\nasd_mm\nNumeric\nThe Anther-Stigma Distance (ASD) is the linear distance between the closest anther (the floral part that releases pollen) and the stigma (the floral part that accepts pollen) in a flower, measured in millimeters (mm). The smaller this distance, the more opportunity for self-fertilization.\n\n\nprotandry\nNumeric\nDegree of protandry (e.g., time difference between male and female phase) measured in days. More protandry means more outcrossing.\n\n\nstem_dia_mm\nNumeric\nStem diameter measured in millimeters (mm).\n\n\nlwc\nNumeric\nLeaf water content (LWC).\n\n\ncrossDir\nCategorical (Binary)\nCross direction\n\n\nnum_hybrid\nNumeric (discrete)\nThe number ofseeds that where hybrid.\n\n\noffspring_genotyped\nNumeric (discrete)\nThe number of seeds genotyped.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRIL Hybridization Data\nBelow is the hybridization dataset. For each plant in the field we genotyped eight seeds at species-specific markers to identify if they were the product of hybridization with xantiana. The phenotypes belong to the genotype of the maternal plant (i.e. they are the same as those in the pollinator visitation data set). I include data at both the level of the seed and a summary at the level of the maternal plant.\n\n\nRIL Combined Data\n\n\n\n\nBehrouzi, P., & Wit, E. (2017). Detecting epistatic selection with partially observed genotype data using copula graphical models. Journal of the Royal Statistical Society: Series C (Applied Statistics), 68. https://doi.org/10.1111/rssc.12287",
    "crumbs": [
      "Motivating biology and datasets"
    ]
  },
  {
    "objectID": "book_sections/example_mini/varType.html",
    "href": "book_sections/example_mini/varType.html",
    "title": "Types of Variables",
    "section": "",
    "text": "Explanatory and Response Variables\nAlthough these taxa hybridize in nature, they remain quite distinct, even in areas of sympatry where they exchange genes.\nDave Moeller and his colleagues (including me) have been studying this species for decades. Their research addresses fundamental questions in evolution, such as:\nTo answer these big and exciting questions, Dave and his collaborators must break them down into smaller, direct scientific studies that can be addressed through a combination of experiments and observations. To conduct such studies, we must map these complex ideas onto measurable variables.\nFor example, rather than directly comparing the flower images in Figure 1, we simplify these flowers into variables that summarize them. For instance, we could represent a flower using a set of variables such as flower color, petal length, the distance between stigma and style, etc.\nDave and his team have conducted numerous studies to tackle these big questions. In most cases, they examine how the value of a response variable — the outcome we aim to understand — changes with different values of one or more explanatory variables (also called predictor or independent variables), which are thought to influence or be associated with the biological process of interest.\nUnderstanding the distinction between explanatory and response variables is crucial for framing hypotheses, designing experiments, and interpreting statistical results. However, this distinction often depends on how the research question is framed and can even vary within a single study. For example, in a recent study on predictors of pollinator visitation in parviflora",
    "crumbs": [
      "Types of Variables"
    ]
  },
  {
    "objectID": "book_sections/example_mini/varType.html#explanatory-and-response-variables",
    "href": "book_sections/example_mini/varType.html#explanatory-and-response-variables",
    "title": "Types of Variables",
    "section": "",
    "text": "We first aimed to identify which loci in the genome predicted flower color and petal length in parviflora. Here, genotype was the explanatory variable, while the floral attributes (petal color and petal length) were the response variables.\nWe then asked whether certain floral attributes (petal color and petal length) predicted pollinator visitation to parviflora plants. In this case, the floral attributes became the explanatory variables, and pollinator visitation was the response variable.",
    "crumbs": [
      "Types of Variables"
    ]
  },
  {
    "objectID": "book_sections/example_mini/varType.html#types-of-variables",
    "href": "book_sections/example_mini/varType.html#types-of-variables",
    "title": "Types of Variables",
    "section": "Types of Variables",
    "text": "Types of Variables\n\n\n\n\n\n\n\n\n\nFigure 2: Which type of variable is color? Some variables (like color) can be treated as either categorical or continuous depending on the question. Because most parviflora plants are either pink or white, we treat color as a binary categorical variable. But, as shown above, color can be measured and analyzed quantiatively as well.\n\n\n\n\n\n# INSERT POLINATOR OBSERVATION VIDEO \n\nVariables come in different flavors, and knowing the flavor of a variable is key to choosing appropriate summary statistics, data visualizations, and statistical models. Our parviflora pollinator visitation example above included both major types of variables (Figure 1):\n\nNumeric variables are quantitative and have magnitude. For instance, we measured pollinator visits as the number of times a pollinator visited a flower during a [5-minute observation period], and petal length in centimeters.\n\nCategorical variables are qualitative. In our example, flower color and genotype were treated as categorical variables.\n\nLike much of stats – the line between these types of variables is blurry. For example, we often treat color as a category, but color can be measured quantitatively (Figure 2). So depending on our question we may want to treat color as either a numeric or categorical variable.\n\nNot All Numbers Are Numeric. For example, a gene ID may be represented as a number, but it is an arbitrary label rather than a measurement. Similarly, in our Clarkia studies, some sites were identified by numbers (e.g., Site 22 or Site 100). However, Site 22 is not “less than” Site 100 — these are categorical variables, despite being represented numerically.\n\nWithin these two categories are further sub-flavors which allow us to further refine our statistical approach:\n\nTypes of Numeric Variables\n\n\n\n\n\n\n\n\n\nFigure 3: Types of numeric variables.  A discrete variable: Xantiana flowers with four, five or six petals (photo courtesy of Dave Moeller).   A continuous variable: Parviflora petal whose length is being measured. Image from The University and Jepson Herbaria University of California, Berkeley. Copyright from ©2020 Chris Winchell. (image link)\n\n\n\n\nNumeric variables can be categorized into two main types:\n\nDiscrete variables come in chunks. For instance, flowers receive zero, one, two, three, and so on, pollinators. Pollinators do not come in fractions, making this variable inherently discrete.\nContinuous variables can take any value within a range. Classic examples include height, weight, and temperature. In our example, petal length is a continuous variable because it can be measured to any level of precision within its range.\n\n\nSubtle Distinctions and Blurry Boundaries. The two cases above represent pretty clear distinctions between discrete and continuous variables, but sometimes such distinctions are more subtle.\nConsider the number of pollinators—these cannot be fractional, but the number of pollinators per minute can be fractional. There are other similarly blurry cases. For example, time to first flower is inherently continuous, but we often check on flowers only once per day, so it is measured as discrete. Similarly, in human studies, age is usually reported in whole months or years (discrete), rather than on the more continuous scale of fractional seconds. In such cases, the appropriate question is not “is my data discrete or continuous?” but rather “what process generated my data? and what statistical distribution do the data follow?\n\n\n\nCategorical variables\n\n\n\n\n\n\n\n\n\nFigure 4: Species is a categorical variable: The Clarkia specialist – Clarkia Evening Bee, (Hesperapis regularis) on a Clarkia flower. Shared by © Gene H under CC BY-NC 4.0 copyright on iNaturalist. In our Clarkia research, pollinator is usually nominal (that is which bee species), but is sometimes binary (Clarkia specialist, or non-specialist), and sometimes ordinal (e.g. frequent pollinator, rare pollinator, never pollinator).\n\n\n\n\nCategorical variables are qualitative, and include, nominal, binary, and ordinal variables.\n\nNominal variables cannot be ordered and have names – like sample ID, species, study site etc…\n\nBinary variables are a type of nominal variable with only two options (or for which we only consider two options. Alive/dead, pass/fail, on/off are classic binary variables). In our example of pollinator visitation in parviflora, we considered only two flower colors (pink/white) so flower color in this case is binary.\n\nOrdinal variables can be ordered, but do not correspond to a magnitude. For example, bronze, silver and gold medals in the Olympics are ranked from best to worst, but first is not some reliable distance away from second or third etc… In our pollinator example, we often may wish to distinguish between frequent pollinators (e.g. specialist bees, Figure 4), common but less frequent pollinators (e.g. non-specialist bees), and rare/incidental pollinators (e.g. flies).",
    "crumbs": [
      "Types of Variables"
    ]
  },
  {
    "objectID": "book_sections/example_mini/varType.html#closing-resources",
    "href": "book_sections/example_mini/varType.html#closing-resources",
    "title": "Types of Variables",
    "section": "Closing Resources",
    "text": "Closing Resources\n\nSummary\nUnderstanding the types of variables in our data can help us translate complex biological questions into measurable data that can be evaluated with the statistical tools we develop in this book. Variables can be categorized as numeric or categorical, and further subdivided into types like discrete, continuous, nominal, binary, or ordinal. These classifications influence how we summarize, visualize, analyze, and modelize our data.\n\n\nChatbot tutor\n\nPlease interact with this custom chatbot (link here) I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you’ve got what you needed from it.\n\n\n\nPractice Questions\nTry the questions below! Likert scales look like this:- How do you feel about Clarkia? (1) Love it  (2) Like it  (3) Don’t care  (4) Do not like (5) Hate it\n\nQ1) In a species with pink or white flowers, flower color is a special kind of categorical variable known as a ___ variable NominalBinaryOrdinalBimodal\n\n\nClick here for explanation\n\nThis was a little tricky! The correct answer is Binary, because there are only two possible values. If you picked either “nominal” or “bimodal,” you’re pretty close and definitely thinking along the right path, but not quite there! So let’s walk through these “not quite right” answers:\n\nNominal: Petal color is nominal in the sense that “pink” isn’t greater or lesser than “white”—the categories have no natural order. But because there are only two options here, the more specific (and better) description is Binary.\nBimodal: A bimodal distribution refers to a numeric variable that has two distinct peaks or clusters. If we had measured flower color quantitatively—say, by recording percent reflectance at 550 nm—and the data clustered around two values (say, “mostly pink” vs. “mostly white”), then the distribution would be bimodal. (But even then, we’d probably simplify it to binary for analysis.)\n\nIf you answered “ordinal,” you should probably take another look at the chapter—ordinal variables have a meaningful order, like “small,” “medium,” and “large.”\n\n.\nQ2) Which of these variables is best described as continuous? Flower color (pink, white)Petal lengthNumber of flowers on a plant\nQ3) The number of offspring produced by a single animal in one breeding season is: BinaryContinuousDiscrete\nQ4) TRUE or FALSE: Populations of Clarkia that we named 100 and 22 are numeric TRUEFALSE.\nQ5) TRUE or FALSE: A variable on a “Likert scale” (see margin for details) is clearly numeric TRUEFALSE.\n\n\nClick here for explanation\n\nThe word “clearly” is the key clue here. A Likert scale (like rating agreement from “Strongly disagree” to “Strongly agree”) is based on numbers (e.g., 1, 2, 3, 4, 5), but those numbers represent ordered categories, not truly continuous or clearly numeric values.\nIn other words, while you can treat Likert scale data like numbers sometimes (e.g., calculating averages), the numbers themselves are standing in for categories with an order—not for measured quantities along a true number line. So while you might have seen Likert data analyzed using means, t-tests, or even regressions—treating them like numeric variables—this is a common (and sometimes reasonable) modeling shortcut. Conceptually, Likert data are still clearly ordinal: they are ordered categories, not continuous measurements.\nIf you missed this, don’t worry — Likert scales can be a little tricky because they look numeric. But always pay close attention to what the numbers mean. If they’re just labeling ordered choices (rather than measuring something truly continuous, like height or weight), the variable is ordinal categorical, not clearly numeric.\n\n.\nQ6) The variable, kingdom (corresponding to one of the six kingdoms of life), is a ___ variable NominalBinaryOrdinalDiscrete\nQ7) TRUE of FALSE: A continuous variable can never be modeled as discrete and vice versa TRUEFALSE\n\n\n\n\nGlossary of Terms\n\n\nVariable: A characteristic or attribute that can take on different values or categories in a dataset.\n\nExplanatory Variable: Also known as a predictor or independent variable, this is a variable that is thought to influence or explain the variation in another variable.\nResponse Variable: Also known as a dependent variable, this is the outcome or effect being studied, which changes in response to the explanatory variable.\n\n\n\n\nNumeric Variable: A variable that represents measurable quantities and has magnitude, either as counts (discrete) or as continuous values.\n\nDiscrete Variable: A numeric variable that represents distinct, separate values or counts (e.g., number of pollinators).\nContinuous Variable: A numeric variable that can take any value within a range and is measured with precision (e.g., petal length).\n\n\n\n\nCategorical Variable: A variable that represents categories or groups and is qualitative in nature.\n\nNominal Variable: A categorical variable without an inherent order (e.g., flower species or study site).\nBinary Variable: A nominal variable with only two possible categories (e.g., alive/dead, pink/white).\nOrdinal Variable: A categorical variable with a meaningful order, but without measurable distances between levels (e.g., gold, silver, bronze).",
    "crumbs": [
      "Types of Variables"
    ]
  },
  {
    "objectID": "book_sections/intro2r_index.html",
    "href": "book_sections/intro2r_index.html",
    "title": "SECTION I: Intro to R",
    "section": "",
    "text": "A “tidyverse” Approach\nFigure 2: A collection of Tidyverse hex stickers representing key R packages for data science, including dplyr, ggplot2, tidyr, readr, and more—each with a unique thematic design.\nAs R has evolved over time and its capabilities can be extended with packages (we will discuss this soon), different “dialects” of R have emerged. While many of you have likely seen Base R – built on the standard R program you download, here we will use Tidyverse – a specific and highly standardized set of packages designed for data science workflows (Figure 2). As a broad overgeneralization, Base R allows for much more control of what you are doing but requires more programming skill, while tidyverse allows you to do a lot with less programming skill.\nI focus on tidyverse programming, not because it is better than base R, but because learning tidyverse is a powerful way to do a lot without learning a lot of formal programming. This means that you will be well prepared for a lot of complex data analysis. If you continue to pursue advanced programming in R (or other languages) you will have some programming concepts to catch up on.\nIf you already know how to accomplish certain tasks with base R tools, I encourage you to invest the time in learning the equivalent approaches in tidyverse. While it may feel redundant at first, this foundation knowledge will make you a more versatile and effective R programmer in the long term, and will allow you to make sense of what we do throughout the term.",
    "crumbs": [
      "SECTION I: Intro to R"
    ]
  },
  {
    "objectID": "book_sections/intro2r_index.html#important-hints-for-r-coding",
    "href": "book_sections/intro2r_index.html#important-hints-for-r-coding",
    "title": "SECTION I: Intro to R",
    "section": "Important hints for R coding",
    "text": "Important hints for R coding\nYears of learning and teaching have taught me the following key points about learning R. These amount to the simple observation that a student’s mindset and attitude towards learning and using R is the most important key to their success. I summarize these tips in the video and bullet points, below.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Replace your automatic negative thoughts with balanced alternatives.\n\n\n\n\n\nBe patient with yourself. Every expert R programmer started exactly where you are now. Your understanding will grow naturally as you tackle real problems and challenges. Do not beat yourself up, you are learning. Replace automatic negative thoughts with balanced thoughts (Figure 3).\nR is literally a language. Languages take a while to learn – at first, looking at an unfamiliar alphabet or hearing people speak a foreign language makes no sense. Aluksi vieraan aakkoston katsominen tai vieraan kielen puhumisen kuuleminen ei tunnu lainkaan järkevältä. With time and effort, you can make sense of a bunch of words and sentences but it takes time. You are not dumb for not understanding the sentence I pasted above (and if you do understand it is because you know Finnish, not because you are smart).\nYou don’t need to memorize anything. You have access to dictionaries, translators, LLMs etc etc. That said, these tools or more useful the more you know.\nDo not compare yourself to others. R will come fast to some, and slower to others. This has absolutely nothing to do with either your intelligence or your long-term potential as a competent R user.\nStart small and don’t be afraid to experiment. There is nothing wrong about typing code that is imperfect and/or does not work out. Start with the simplest way of addressing your problem and see how far you get. Start small, maybe with some basic data analysis or creating a simple plot. Each little victory builds your confidence. You can always try new and more complex approaches as you go.",
    "crumbs": [
      "SECTION I: Intro to R"
    ]
  },
  {
    "objectID": "book_sections/intro2r_index.html#whats-ahead",
    "href": "book_sections/intro2r_index.html#whats-ahead",
    "title": "SECTION I: Intro to R",
    "section": "What’s ahead?",
    "text": "What’s ahead?\n\n\n\n\n\n\n\n\nFigure 4: A pretty scene of Clarkia’s home showing the world we get to investigate as we get equipped with R.\n\n\n\n\n\nNow we begin our intro to R. While we will keep practicing what we have learned and learning new R stuff all term, the next four chapters, listed below will get you started:\n\nGetting up and Running. This section introduces RStudio, math in R, vectors, variable assignment, using functions, r packages, loading data (from the internet), and data types. There is a lot here!\nData in R. Here we continue on our introduction to R. We first introduce the concept of tidy data, and introduce the capabilities of the tidyverse package, dplyr.\nIntro to ggplot. The ggplot package allows us to make nice plots quickly. We will get started understanding how ggplot thinks, and introduce the wide variety of figures you can make. Later in this term we will make better figures in ggplot.\nReproducible science. We consider how to collect data, and store it in a folder. We then introduce the concept of R projects and loading data from our computer. Finally, we introduce the idea of saving R scripts.",
    "crumbs": [
      "SECTION I: Intro to R"
    ]
  },
  {
    "objectID": "book_sections/getting_started.html",
    "href": "book_sections/getting_started.html",
    "title": "1. Getting started with R",
    "section": "",
    "text": "What is R? What is RStudio?\nR is a computer program built for data analysis. As opposed to GUIs, like Excel, or click-based stats programs, R is focused on writing and sharing scripts. This enables us to be shared and replicate analyses, ensuring that data manipulation occurs in a script. This practice both preserving the integrity of the original data, while providing tremendous flexibility. R has become the computer language of choice for most statistical work because it’s free, allows for reproducible analyses, makes great figures, and has many “packages” that support the integration of novel statistical approaches. In a recent paper, we used R to analyze hundreds of Clarkia genomes and learn about the (Figure 1 from Sianta et al. (2024)). RStudio is an Integrated Development Environment (IDE)—a nice setup to interact with R and make it easier to use.",
    "crumbs": [
      "1. Getting started with R"
    ]
  },
  {
    "objectID": "book_sections/getting_started.html#what-is-r-what-is-rstudio",
    "href": "book_sections/getting_started.html#what-is-r-what-is-rstudio",
    "title": "1. Getting started with R",
    "section": "",
    "text": "More precisely, R is a programming language that runs computations, while RStudio is an integrated development environment (IDE) that provides an interface by adding many convenient features and tools. So just as the way of having access to a speedometer, rearview mirrors, and a navigation system makes driving much easier, using RStudio’s interface makes using R much easier as well.\n— From Statistical Inference via Data Science: A ModernDive into R and the Tidyverse (Ismay & Kim, 2019)",
    "crumbs": [
      "1. Getting started with R"
    ]
  },
  {
    "objectID": "book_sections/getting_started.html#the-shortest-introduction-to-r",
    "href": "book_sections/getting_started.html#the-shortest-introduction-to-r",
    "title": "1. Getting started with R",
    "section": "The Shortest Introduction to R",
    "text": "The Shortest Introduction to R\nBefore opening RStudio, let’s get familiar with two key ays we use R – (1) Using R as a calculator, and (2) Storing information by assigning values to variables.\nR can perform simple (or complex) calculations. For example, entering 1 + 1 returns 2, and entering 2^3 (two raised to the power of three) returns 8. Try it yourself by running the code below, and then experiment with other simple calculations.Math in R: See posit's recipe for using R as a calculator for more detail.\nCommenting code The hash, #, tells R to stop reading your code. This allows you to “comment” your code – keeping notes to yourself and other readers about what the code is doing. Commenting your code is very valuable and you should do it often!Commenting code The hash, #, tells R to stop reading your code. This allows you to “comment” your code – keeping notes to yourself and other readers about what the code is doing. Commenting your code is very valuable and you should do it often!\nChallengeSolution\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nStoring values in variables allows for efficient (and less error-prone) analyses, while paving the way to more complex calculations. In R, we assign values to variables using the assignment operator, &lt;-. For example, to store the value 1 in a variable named x, type x &lt;- 1. Now, 2 * x will return 2.\n\n\nx &lt;- 1 # Assign 1 to x\n2 *  x # Multiply x by 2\n\n[1] 2\n\nBut R must “know” something before it can “remember” it. The code below aims to set y equal to five, and see what y plus one is (it should be six). However, it returns an error. Run the code to see the error message, then fix it!\n\nChallengeSolution\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nHint\n\nR reads and executes each line of code sequentially, from top to bottom. Think about what y + 1 means to R if it hasn’t seen a definition of y yet.\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nHint\n\nIn R, variables must be defined before they are used. When you try to use y + 1 before assigning a value to y, R throws an error because it doesn’t know what y is yet. When we switch the order—assigning y &lt;- 5 before using y + 1—R understands the command and evaluates it properly.\n\n\n\n\nNow, try assigning different numbers to x and y, or even using them together in a calculation, such as x + y. Understanding this concept of assigning values is critical to understanding how to use R.",
    "crumbs": [
      "1. Getting started with R"
    ]
  },
  {
    "objectID": "book_sections/getting_started.html#lets-get-started-with-r",
    "href": "book_sections/getting_started.html#lets-get-started-with-r",
    "title": "1. Getting started with R",
    "section": "Let’s get started with R",
    "text": "Let’s get started with R\nThe following sections introduce the very basics of R including:\n\nFunctions and vectors in R.\n\nLoading packages and data into R.\n\nData types in R.\n\nAn orientation to RStudio.\n\nThen we summarize the chapter, present a chatbot tutor, practice questions, a glossary, a review of R functions and R packages introduced, and provide links to additional resources.\n\n\n\n\nIsmay, C., & Kim, A. Y. (2019). Statistical inference via data science: A ModernDive into r and the tidyverse. CRC Press.\n\n\nSianta, S. A., Moeller, D. A., & Brandvain, Y. (2024). The extent of introgression between incipient &lt;i&gt;clarkia&lt;/i&gt; species is determined by temporal environmental variation and mating system. Proceedings of the National Academy of Sciences, 121(12), e2316008121. https://doi.org/10.1073/pnas.2316008121",
    "crumbs": [
      "1. Getting started with R"
    ]
  },
  {
    "objectID": "book_sections/getting_started/functions_and_vectors.html",
    "href": "book_sections/getting_started/functions_and_vectors.html",
    "title": "• 1. Functions and vectors",
    "section": "",
    "text": "R Functions\nR comes with tons of built-in functions that do everything from basic math to advanced statistical modeling. So not only can we calculate the mean and variance in Clarkia xantiana petal lengths with the mean() and var() functions, respectively, but we can test the null hypothesis that mean petal size in xantiana is equal to that of parviflora with the t.test() function. Functions are the foundation of how we do things in R – they save time and ensure consistency across your analyses.\nFunctions take arguments, which we put in parentheses. When typing sqrt(25), sqrt() is the function, 25 is the argument, and 5 is the output.\nFunctions can take multiple arguments: If you don’t specify them all, R will either tell you to provide them, or assumes default values. For example, the log function defaults to the natural log (base e), so log(1000) returns 6.908. If you want the logarithm with base 10, you need to specify it explicitly as log(1000, 10), which returns 3. Note that argument order matters:—log(10, 1000) returns 0.3333333, while log(1000, 10) returns 3.",
    "crumbs": [
      "1. Getting started with R",
      "• 1. Functions and vectors"
    ]
  },
  {
    "objectID": "book_sections/getting_started/functions_and_vectors.html#r-functions",
    "href": "book_sections/getting_started/functions_and_vectors.html#r-functions",
    "title": "• 1. Functions and vectors",
    "section": "",
    "text": "R functions: See posit's recipe for R functions for more detail.\n\n\n\n\n# Natural log of 1000\nlog(1000)             \n\n[1] 6.907755\n\n\n\n\n# Log base 1000 of 10\nlog(1000, base = 10)  \n\n[1] 3\n\n\nTips for using functions in R\n\nUse named arguments in functions.\nFor example, typing log(1000, base = 10) makes what each value represents obvious (improving code readability), and allows flexibility in argument order (e.g. log(base = 10, 1000) gives the same value as log(1000, base = 10)). Thus, using named arguments makes your code readable and robust.\n\n\nUse = to assign arguments in functions\nWhen specifying arguments inside a function, always use = (e.g., log(1000, base = 10)). Do not use &lt;-, which is for assigning values to variables. Otherwise, R might mistakenly store the argument as a variable, leading to unexpected results.\n\n\nPipe together functions with |&gt;\nThe pipe, |&gt;, provides a clean way to pass the output of one function into another. For example, we can find the square root of the \\(\\text{log}_{10}\\) of 1000, rounded to two decimal places, as follows:\n\nlog(1000, base = 10)   |&gt;  \n    sqrt()             |&gt;  \n    round(digits = 2)\n\n[1] 1.73\n\n\nNotice that we did not explicitly provide an argument to sqrt() — it simply used the output of log(1000, base = 10). Similarly, the round() function then rounded the square root of 3 to two decimal places.",
    "crumbs": [
      "1. Getting started with R",
      "• 1. Functions and vectors"
    ]
  },
  {
    "objectID": "book_sections/getting_started/functions_and_vectors.html#vectors",
    "href": "book_sections/getting_started/functions_and_vectors.html#vectors",
    "title": "• 1. Functions and vectors",
    "section": "Working with vectors",
    "text": "Working with vectors\nIf we observed one Clarkia plant with one flower, a second with two flowers, a third with three flowers, and a fourth with two flowers, we could find the mean number of flowers as (1 + 2 + 3 + 2)/4 = 2, but this would be tedious and error-prone. It would be easier to store these values in an ordered sequence of values (called a vector) and then use the (mean()) function.\nVectors are the primary way that data is stored in R—even more complex data structures are often built from vectors. We create vectors with the combine function, c(), which takes arguments that are the values in the vector.\n\n# A vector of flower numbers\n  # 1st plant has one flower\n  # 2nd plant has two flowers\n  # 3rd plant has three flowers\n  # 4th plant has two flowers\nnum_flowers &lt;- c(1, 2, 3, 2)  # Create a vector for number of flowers per plant\nmean(num_flowers) # finding the mean flower number\n\n[1] 2\n\n\n\n\n# If each flower produces four petals  \nnum_petals &lt;- 4 * num_flowers\nnum_petals\n\n[1]  4  8 12  8\n\n\n# If we wanted the log_2 of petal number \nlog(num_petals, base = 2) |&gt;\n  round(digits = 3)\n\n[1] 2.000 3.000 3.585 3.000\n\n\n\nVariable assignment can be optional: In the code, I assigned observations to the vector, num_flowers, and then found the mean. But we could have skipped variable assignment—variable assignment — mean(c(1, 2, 3, 2)) also returns 2.\nThere are two good reasons not to skip variable assignment:\n\nVariable assignment makes code easier to understand. If I revisited my code in weeks I would know what the mean of this vector meant.\nVariable assignment allows us to easily reuse the information For example, below I can easily find the mean petal number.",
    "crumbs": [
      "1. Getting started with R",
      "• 1. Functions and vectors"
    ]
  },
  {
    "objectID": "book_sections/getting_started/loading_packages_and_data.html",
    "href": "book_sections/getting_started/loading_packages_and_data.html",
    "title": "• 1. Load packages and data",
    "section": "",
    "text": "R packages\nWhile R has many built-in functions, packages provide even more functions to extend R’s capabilities. Packages can offer alternative (often more efficient and user-friendly) approaches to tasks that can be done with base R functions, or they can enable entirely new functionality that is not included in base R at all. In fact, R packages are a major way that the latest statistical and computational methods in various fields are shared with practitioners.\nBelow I introduce the readr, and dplyr packages. Because these packages are so useful for streamlining data import, manipulation, and cleaning, I use them in nearly every R project. I also introduce the conflicted package, which identifies any functions with shared names across packages, and allows us to tell R which function we mean when more than one function has the same name.\nInstall a package the first time you use it The first time you need a package, install it with the install.packages() function. Here the argument is the package (or vector of packages) you want to install. So, to install the packages above, type:\n# We do this the first time we need a package.\ninstall.packages(c(\"readr\", \"dplyr\", \"conflicted\"))\nLoad installed packages every time you open RStudio You only install a package once, but you must use the library() function, as I demonstrate below, to load installed packages every time you open R.\n# We do this every time we open R and want to use these packages.\nlibrary(conflicted)\nlibrary(readr)\nlibrary(dplyr)",
    "crumbs": [
      "1. Getting started with R",
      "• 1. Load packages and data"
    ]
  },
  {
    "objectID": "book_sections/getting_started/loading_packages_and_data.html#reading-data-into-r",
    "href": "book_sections/getting_started/loading_packages_and_data.html#reading-data-into-r",
    "title": "• 1. Load packages and data",
    "section": "Reading data into R",
    "text": "Reading data into R\nRather than typing large datasets into R, we usually want to read in data that is already stored somewhere. For now, we will load data saved as a csv file from the internet with the read_csv(link) structure from the readr package. Later, we will revisit the challenge of importing data from other file types and locations into R.\n\n\nLoading data: See posit's recipe for importing data for more detail. Note also that read.csv() is a base R function similar to read_csv(), but it behaves a bit differently – for example it reads data in as a dataframe, not a tibble.\nBelow, I show an example of reading pollinator visitation data from a link on my GitHub. After loading a dataset, you can see the first ten lines and all the columns that fit by simply typing its name. Alternatively, the View() function opens up the full spreadsheet for you to peruse.\n\nril_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_rils.csv\"\nril_data &lt;- readr::read_csv(ril_link)\nril_data\n\n# A tibble: 593 × 17\n   ril   location prop_hybrid mean_visits growth_rate petal_color petal_area_mm\n   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;               &lt;dbl&gt;\n 1 A1    GC             0           0     1.272       white                44.0\n 2 A100  GC             0.125       0.188 1.448       pink                 55.8\n 3 A102  GC             0.25        0.25  1.8O        pink                 51.7\n 4 A104  GC             0           0     0.816       white                57.3\n 5 A106  GC             0           0     0.728       white                68.6\n 6 A107  GC             0.125       0     1.764       pink                 66.3\n 7 A108  GC            NA          NA     1.584       &lt;NA&gt;                 51.5\n 8 A109  GC             0           0     1.476       white                48.1\n 9 A111  GC             0          NA     1.144       white                51.6\n10 A112  GC             0.25        0     1           white                89.8\n# ℹ 583 more rows\n# ℹ 10 more variables: date_first_flw &lt;dbl&gt;, node_first_flw &lt;dbl&gt;,\n#   petal_perim_mm &lt;dbl&gt;, asd_mm &lt;dbl&gt;, protandry &lt;dbl&gt;, stem_dia_mm &lt;dbl&gt;,\n#   lwc &lt;dbl&gt;, crossDir &lt;chr&gt;, num_hybrid &lt;dbl&gt;, offspring_genotyped &lt;dbl&gt;\n\n\n\n\n\n\npackage::function() format: I read in the data with the read_csv() function in the readr package by typing: readr::read_csv(), but typing read_csv() gives the same result. The package::function() format comes in handy when two functions in different packages have the same name.",
    "crumbs": [
      "1. Getting started with R",
      "• 1. Load packages and data"
    ]
  },
  {
    "objectID": "book_sections/getting_started/data_types.html",
    "href": "book_sections/getting_started/data_types.html",
    "title": "• 1. Data types in R",
    "section": "",
    "text": "Motivating scenario: You have loaded data into R and are curious about what “types” of data R thinks it is working with.\nLearning goals: By the end of this sub-chapter you should be able to\n\nList the different types of variables that R can keep in a vector.\n\nIdentify which type of variable is in a given column.\n\nAsk logical questions of the data to make a logical vector.\n\n\n\n\nR handles different types of data in specific ways. Understanding these data types is crucial because what you can do with your data depends on how R interprets it. For example, although you know that 1 + \"two\" equals 3, R cannot add a number and a word. So, to use R effectively, you will need to make sure the type of data R has in memory matches the type it needs to have to do what you want. This will also help you understand R’s error messages and confusing results when things don’t work as expected.\nLooking back at the pollinator visitation dataset we loaded above, we see that (if you read it in with read_csv()) R tells you the class of each column before showing you its first few values. In that dataset, columns one, two, five, six, and fifteen (note R provides a peek of the first few variables — in this case, seven — and then provides the names and data type for the rest) — location, ril, growth_rate, and petal_color — are of class &lt;chr&gt;, while all other columns contain numbers (data of class &lt;dbl&gt;). What does this mean? Well it tells you what type of data R thinks is in that column. Here are the most common options:\n\nNumeric: Numbers, including doubles (&lt;dbl&gt;) and integers (&lt;int&gt;). Integers keep track of whole numbers, while doubles keep track of decimals (but R often stores whole numbers as doubles).\nCharacter: Text, such as letters, words, and phrases (&lt;chr&gt;, e.g., \"pink\" or \"Clarkia xantiana\").\n\nLogical: Boolean values—TRUE or FALSE (&lt;logi&gt;), often used for comparisons and conditional statements.\n\nFactors: Categorical variables that store predefined levels, often used in statistical modeling. While they resemble character data, they behave differently in analyses. We will ignore them in this chapter but revisit them later.\n\nWhen you load data into R, you should always check to ensure that the data are in the expected format. Here we are surprised to see that growth_rate is a character, because it should be a number. A close inspection shows that in row three someone accidentally entered the letter O instead of the number zero (0) in what should be 1.80.\n\n\n\n# A tibble: 4 × 2\n  ril   growth_rate\n  &lt;chr&gt; &lt;chr&gt;      \n1 A1    1.272      \n2 A100  1.448      \n3 A102  1.8O       \n4 A104  0.816      \n\n\nAsking logical questions We often generate logical variables by asking logical questions of the data. Here is how you do that in R.\n\n\n\nQuestion\nR Syntax\n\n\n\n\nDoes a equal b?\na == b\n\n\nDoes a not equal b?\na != b\n\n\nIs a greater than b?\na &gt; b\n\n\nIs a less than b?\na &lt; b\n\n\nIs a greater than or equal to b?\na &gt;= b\n\n\nIs a less than or equal to b?\na &lt;= b",
    "crumbs": [
      "1. Getting started with R",
      "• 1. Data types in R"
    ]
  },
  {
    "objectID": "book_sections/getting_started/rstudio_orientation.html",
    "href": "book_sections/getting_started/rstudio_orientation.html",
    "title": "• 1. Orientation to RStudio",
    "section": "",
    "text": "Motivating scenario: You have just downloaded R and RStudio, and want to understand all the stuff that you see when you open RStudio.\nLearning goals: By the end of this sub-chapter you should be able to\n\nIdentify the source pane and what to do there.\n\nIdentify the terminal pane and what to see and do there.\n\nIdentify the environment / history pane, what to see and do there, and how to navigate tabs in this pane.\n\nIdentify the file / plot / help / viewer pane, what to see and do there, and how to navigate tabs in this pane.\n\n\n\n\n\n\nAbove, you ran R in this web browser, but more often you will work with R in RStudio. When you open RStudio for the first time, you will see three primary panes. The one on the left works identically to the basic R console. Navigating to ‘File &gt; New File &gt; R Script’ opens a new script and reveals a fourth pane.\n\n\nR Scripts are ways to keep a record of your code so that you can pick up where you left off, build on previous work, and share your efforts. We will introduce R Scripts more formally soon!\n\n\n\nLike the R console above (and all computer languages) RStudio does not “know” what you wrote until you enter it into memory. There are a few ways to do this, but our preferred way is to highlight the code you intend to run, and then click the Run button in the top right portion of the R script pane. Alternatively, press Ctrl+Return for Windows/Linux or ⌘+Return on OS X.\n\n\n\n\n\n\n\n\nFigure 1: More panes = less pain. A brief tour of RStudio’s panes.\n\n\n\n\n\nFigure 1 shows what your RStudio session might look like after doing just a little bit of work:\n\nThe source pane Pane 1 is used for writing and editing scripts, R Markdown files etc. This is where you write reproducible code that can be saved and reused.\nThe console pane Pane 2 is basically the R command prompt from vanilla R, it is where you directly interact with R. You can type commands here to execute them immediately. It will display output, messages, and error logs.\nThe environment / history pane Pane 3 shows what R has in working memory and what it has done.\n\nThe Environment Tab shows all objects (e.g., data frames, vectors) currently in memory.\n\nThe History Tab shows all the commands you have run in your session. You can even search through your history, which can be easier than scrolling through the console.\n\nThe files / plots / help / viewer pane. Pane 4 is remarkably useful!\n\nThe Plots Tab shows the plots generated during your session. You can delete an individual plot by clicking the red X button, or delete all plots by clicking the broom button.\nThe Help Tab: allows you to access documentation and help files.",
    "crumbs": [
      "1. Getting started with R",
      "• 1. Orientation to RStudio"
    ]
  },
  {
    "objectID": "book_sections/getting_started/getting_started_summary.html",
    "href": "book_sections/getting_started/getting_started_summary.html",
    "title": "• 1. Getting started summary",
    "section": "",
    "text": "Figure 1: Some pretty R from Allison Horst.\n\n\n\n\nLinks to: Summary. Chatbot tutor. Questions. Glossary. R functions. R packages. Additional resources.\n\nChapter summary\nR is (much more than just) a simple calculator – it can keep track of variables, and has functions to make plots, summarize data, and build statistical models. R also has many packages that can extend its capabilities. Now that we are familiar with R, RStudio, vectors, functions, data types and packages, we are ready to build our R skills even further to work with data!\n\n\nChatbot tutor\n\nPlease interact with this custom chatbot (link here) I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you got what you needed from it.\n\n\n\nPractice Questions\n\n\n\n\n\n\n\n\n\nFigure 2: Some encouragement from Allison Horst.\n\n\n\n\nThe interactive R environment below allows you to work without switching tabs.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ1) Entering \"p\"^2 into R produces which error?\n\n What error? It works great? Error: object p not found Error: object of type closure is not subsettable Error in “p”^2 : non-numeric argument to binary operator\n\nQ2) Which logical question provides an unexpected answer?\n\n (2.0 + 1.0) == 3.0 (0.2 + 0.1) == 0.3 2^2 &gt; 8 (1/0) == (10 * 1/0)\n\n\n\nClick here for an explanation\n\nThis is a floating-point precision issue. In R (and most programming languages), some decimal values cannot be represented exactly in the binary code that they use under the hood. To see this, try (0.2 + 0.1) - 0.3:\n\n(0.2 + 0.1) - 0.3\n\n[1] 5.551115e-17\n\n\nIf you are worried about floating point errors, use the all.equal() function instead of ==, or round to 10 decimal places before asking logical questions.\n\nQ3) R has a built-in dataset called iris. You can look at it or give it to functions by typing iris. Which variable type is the Species in the iris dataset?\n\n numeric logical character factor\n\nFor the following questions consider the diabetes dataset available at: https://rb.gy/fan785\nQ4) Which variable in the diabetes dataset is a character but should be a number?:\n\n ratio location age frame none\n\nQ5) True OR False: The numeric variable, bp.1d, is a double, but could be changed to an integer without changing any of our analyses: TRUEFALSE\nQ6) Which categorical variable in the dataset is ordinal?\n\n id location gender frame\n\nQ7) You collected five leaves of the wild grape (Vitis riparia) and measured their length and width. You have a table of lengths and widths of each leaf and a formula for grape leaf area (below).\nThe area of a grape leaf is: \\[\\text{leaf area } = 0.851 \\times \\text{ leaf length } \\times \\text{ leaf width}\\] The data are here, each column is a leaf:\n\n\n\n\n\nlength\n5.0\n6.1\n5.8\n4.9\n6.0\n\n\nwidth\n3.2\n3.0\n4.1\n2.9\n4.5\n\n\n\n\n\nThe mean leaf area is \n\n\nClick here for a hint\n\n\nFirst make vectors for length and width\n\nlength = c(5, 6.1, 5.8, 4.9, 6)\nwidth = c(3.2, 3, 4.1, 2.9, 4.5)\nThen multiply these vectors by each other and 0.851.\nFinally find the mean\n\n\n\n\nClick here for the solution\n\n\n# Create length and width vectors\nlength &lt;- c(5, 6.1, 5.8, 4.9, 6)\nwidth &lt;- c(3.2, 3, 4.1, 2.9, 4.5)\n\n\nleaf_areas &lt;- 0.851 * length * width # find area\nmean(leaf_areas)                     # find mean\n\n[1] 16.89916\n\n# or in one step:\n(0.851 * length * width) |&gt;\n  mean()\n\n[1] 16.89916\n\n\n\n\n\n\n\nGlossary of Terms\n\n\nR: A programming language designed for statistical computing and data analysis.\nRStudio: An Integrated Development Environment (IDE) that makes using R more user-friendly.\nVector: An ordered sequence of values of the same data type in R.\nAssignment Operator (&lt;-): Used to store a value in a variable.\nLogical Operator: A symbol used to compare values and return TRUE or FALSE (e.g., ==, !=, &gt;, &lt;).\nNumeric Variable: A variable that represents numbers, either as whole numbers (integers) or decimals (doubles).\nCharacter Variable: A variable that stores text (e.g., \"Clarkia xantiana\").\nPackage: A collection of R functions and data sets that extend R’s capabilities.\n\n\n\n\n\nNew R functions\n\n\nc(): Combines values into a vector.\ninstall.packages(): Installs an R package.\nlibrary(): Loads an installed R package for use.\nlog(): Computes the logarithm of a number, with an optional base.\nmean(): Calculates the average (mean) of a numeric vector.\nread_csv() (readr): Reads a CSV file into R as a data frame.\nround(): Rounds a number to a specified number of decimal places.\nsqrt(): Finds the square root of a number.\nView(): Opens a data frame in a spreadsheet-style viewer.\n\n\n\n\n\nR Packages Introduced\n\n\nbase: The core R package that provides fundamental functions like c(), log(), sqrt(), and round().\nreadr: A tidyverse package for reading rectangular data files (e.g., read_csv()).\ndplyr: A tidyverse package for data manipulation, including mutate(), glimpse(), and across().\nconflicted: Helps resolve function name conflicts when multiple packages have functions with the same name.\n\n\n\n\nAdditional resources\nThese optional resources reinforce or go beyond what we have learned.\n\nR Recipes:\n\nDoing math in R.\n\nUsing R functions.\n\nImporting data from a .csv.\n\nVideos:\n\nCoding your Data Analysis for Success (From Stat454).\n\nWhy use R? (Yaniv Talking).\n\nAccessing R and RStudio (Yaniv Talking).\n\nRStudio orientation (Yaniv Talking).\n\nR functions (Yaniv Talking).\n\nR packages (Yaniv Talking).\n\nLoading data into R (Yaniv Talking).\n\nData types (Yaniv Talking). Uses compression data as an example.",
    "crumbs": [
      "1. Getting started with R",
      "• 1. Getting started summary"
    ]
  },
  {
    "objectID": "book_sections/data_in_R.html",
    "href": "book_sections/data_in_R.html",
    "title": "2. Data in R",
    "section": "",
    "text": "Tidy data\nFigure 2: A visual explanation of tidy data. Modified from Wickham (2014).\nData can be structured in different ways: in a tidy format, each variable has its own column, and each row represents an observation. In contrast, messy data might combine multiple variables into a single column or store observations in a less structured format. Figure 2 A shows “long” data with one variable per column. Figure 2 B contains boxes (rather than rows or columns) with petals from a given flower laid out neatly, and information about the flower and plant written beneath it. Both formats have their costs and benefits:\nNote that the tidy data format is not necessarily “prettier” or easier to read – in fact, in visual presentation of data for people, we often choose an untidy format. But when analyzing data on our computer, a tidy format simplifies our work. For this reason we will work with tidy data when possible in this book.\nFigure 3: An example of tidy versus untidy data. A) A table where each row is an observation (a petal), and each column is a variable (e.g. pop, plant, image etc…). B) A nicely arranged (but not tidy) sheet of Clarkia xantiana petals - arranged by flower.",
    "crumbs": [
      "2. Data in R"
    ]
  },
  {
    "objectID": "book_sections/data_in_R.html#tidy-data",
    "href": "book_sections/data_in_R.html#tidy-data",
    "title": "2. Data in R",
    "section": "",
    "text": "Like families, tidy datasets are all alike but every messy dataset is messy in its own way. Tidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning).\nHadley Wickham. Tidy data. Wickham (2014).\n\n\n\nFigure 2 A is “tidy”: Each row is an observation (a petal), and each column is a variable related to that observation. Because this style is so predictable, this format simplifies computational analyses.\n\nFigure 2 B is not “tidy”: There are not simple rows and columns, and variables are combined in a long string. This format is useful in many ways—for example, humans can easily identify patterns, and data can be stored compactly.\n\n\n\n\nBecause all untidy data are different, there is no way to uniformly tidy an untidy dataset. However, the tidyr package has many useful functions. Specifically, the pivot_longer() function allows for converting data from wide format to long format.",
    "crumbs": [
      "2. Data in R"
    ]
  },
  {
    "objectID": "book_sections/data_in_R.html#tibbles",
    "href": "book_sections/data_in_R.html#tibbles",
    "title": "2. Data in R",
    "section": "Tibbles",
    "text": "Tibbles\nA tibble is the name for the primary structure that holds data in the tidyverse. A tibble—much like a spreadsheet—does not automatically make data tidy, but encourages a structured, consistent format that works well with tidyverse functions.\n\nIn a tibble, each column is a vector. This means that all entries in a column must be of the same class. If you mix numeric and character values in a column, every entry becomes a character.\nIn a tibble, each row unites observations. A row can have any mix of data types.\n\n\n\nTibbles vs. Data Frames For base R users – A tibble is much like a data frame, but some minor features distinguish them. See Chapter 10 of Grolemund & Wickham (2018) for more info.\n\n\n\n\n\n\n\n\nFeature\nTibble\nData Frame\n\n\n\n\nWhat you see on screen\nFirst ten rows & cols that fit\nEntire dataset\n\n\nData Types Displayed\nYes – &lt;dbl&gt;, &lt;chr&gt;, etc\nNo\n\n\nSubsetting to one column returns\nA tibble\nA vector\n\n\n\nThe read_csv() function that we introduced earlier to load data imports data as a tibble. Looking at the data below, you are probably surprised to see that growth rate is a character &lt;chr&gt;, because it should be a number &lt;dbl&gt;. A little digging reveals that the entry in the third row has a growth rate of 1.8O (with the letter, O, at the end) which should be 1.80 (with the number 0 at the end)\n\nlibrary(readr)\nlibrary(dplyr)\nril_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_rils.csv\"\nril_data &lt;- readr::read_csv(ril_link)\nril_data \n\n# A tibble: 593 × 17\n   ril   location prop_hybrid mean_visits growth_rate petal_color petal_area_mm\n   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;               &lt;dbl&gt;\n 1 A1    GC             0           0     1.272       white                44.0\n 2 A100  GC             0.125       0.188 1.448       pink                 55.8\n 3 A102  GC             0.25        0.25  1.8O        pink                 51.7\n 4 A104  GC             0           0     0.816       white                57.3\n 5 A106  GC             0           0     0.728       white                68.6\n 6 A107  GC             0.125       0     1.764       pink                 66.3\n 7 A108  GC            NA          NA     1.584       &lt;NA&gt;                 51.5\n 8 A109  GC             0           0     1.476       white                48.1\n 9 A111  GC             0          NA     1.144       white                51.6\n10 A112  GC             0.25        0     1           white                89.8\n# ℹ 583 more rows\n# ℹ 10 more variables: date_first_flw &lt;dbl&gt;, node_first_flw &lt;dbl&gt;,\n#   petal_perim_mm &lt;dbl&gt;, asd_mm &lt;dbl&gt;, protandry &lt;dbl&gt;, stem_dia_mm &lt;dbl&gt;,\n#   lwc &lt;dbl&gt;, crossDir &lt;chr&gt;, num_hybrid &lt;dbl&gt;, offspring_genotyped &lt;dbl&gt;",
    "crumbs": [
      "2. Data in R"
    ]
  },
  {
    "objectID": "book_sections/data_in_R.html#lets-get-ready-to-deal-with-data-in-r",
    "href": "book_sections/data_in_R.html#lets-get-ready-to-deal-with-data-in-r",
    "title": "2. Data in R",
    "section": "Let’s get ready to deal with data in R",
    "text": "Let’s get ready to deal with data in R\nThe following sections introduce the very basics of R including:\n\nAdding columns with mutate.\n\nSelecting columns.\n\nSummarizing columns.\n\nChoosing rows.\n\nThen we summarize the chapter, present a chatbot tutor, practice questions, a glossary, a review of R functions and R packages introduced, and provide links to additional resources.\n\n\n\n\nGrolemund, G., & Wickham, H. (2018). R for data science.\n\n\nWickham, H. (2014). Tidy data. Journal of Statistical Software, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10",
    "crumbs": [
      "2. Data in R"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/add_vars.html",
    "href": "book_sections/data_in_R/add_vars.html",
    "title": "• 2. Adding columns w mutate",
    "section": "",
    "text": "Changing or adding variables with mutate()\nOften we want to change the values in a column, or make a new column. For example in our data we may hope to:\nThe mutate() function in the dplyr package can solve this. You can overwrite data in an existing column or make a new column as follows:\nril_data      |&gt;\n  dplyr::mutate(growth_rate = as.numeric(growth_rate),   # make numeric\n                visited = mean_visits &gt; 0)\n\n# A tibble: 593 × 18\n   ril   location prop_hybrid mean_visits growth_rate petal_color petal_area_mm\n   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;\n 1 A1    GC             0           0           1.27  white                44.0\n 2 A100  GC             0.125       0.188       1.45  pink                 55.8\n 3 A102  GC             0.25        0.25       NA     pink                 51.7\n 4 A104  GC             0           0           0.816 white                57.3\n 5 A106  GC             0           0           0.728 white                68.6\n 6 A107  GC             0.125       0           1.76  pink                 66.3\n 7 A108  GC            NA          NA           1.58  &lt;NA&gt;                 51.5\n 8 A109  GC             0           0           1.48  white                48.1\n 9 A111  GC             0          NA           1.14  white                51.6\n10 A112  GC             0.25        0           1     white                89.8\n# ℹ 583 more rows\n# ℹ 11 more variables: date_first_flw &lt;dbl&gt;, node_first_flw &lt;dbl&gt;,\n#   petal_perim_mm &lt;dbl&gt;, asd_mm &lt;dbl&gt;, protandry &lt;dbl&gt;, stem_dia_mm &lt;dbl&gt;,\n#   lwc &lt;dbl&gt;, crossDir &lt;chr&gt;, num_hybrid &lt;dbl&gt;, offspring_genotyped &lt;dbl&gt;,\n#   visited &lt;lgl&gt;\nAfter confirming this worked, we can assign it to R’s memory: In doing so, I even converted 1.8O into 1.80 so we have an observation in that cell rather than missing data.\nril_data       &lt;- ril_data      |&gt;\n  dplyr::mutate(growth_rate = case_when(growth_rate ==\"1.8O\" ~ \"1.80\",\n                                          .default = growth_rate),  \n                growth_rate = as.numeric(growth_rate),\n                visited = mean_visits &gt; 0)\n# A tibble: 593 × 18\n   ril   location prop_hybrid mean_visits growth_rate petal_color petal_area_mm\n   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;\n 1 A1    GC             0           0           1.27  white                44.0\n 2 A100  GC             0.125       0.188       1.45  pink                 55.8\n 3 A102  GC             0.25        0.25        1.8   pink                 51.7\n 4 A104  GC             0           0           0.816 white                57.3\n 5 A106  GC             0           0           0.728 white                68.6\n 6 A107  GC             0.125       0           1.76  pink                 66.3\n 7 A108  GC            NA          NA           1.58  &lt;NA&gt;                 51.5\n 8 A109  GC             0           0           1.48  white                48.1\n 9 A111  GC             0          NA           1.14  white                51.6\n10 A112  GC             0.25        0           1     white                89.8\n# ℹ 583 more rows\n# ℹ 11 more variables: date_first_flw &lt;dbl&gt;, node_first_flw &lt;dbl&gt;,\n#   petal_perim_mm &lt;dbl&gt;, asd_mm &lt;dbl&gt;, protandry &lt;dbl&gt;, stem_dia_mm &lt;dbl&gt;,\n#   lwc &lt;dbl&gt;, crossDir &lt;chr&gt;, num_hybrid &lt;dbl&gt;, offspring_genotyped &lt;dbl&gt;,\n#   visited &lt;lgl&gt;",
    "crumbs": [
      "2. Data in R",
      "• 2. Adding columns w mutate"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/add_vars.html#changing-or-adding-variables-with-mutate",
    "href": "book_sections/data_in_R/add_vars.html#changing-or-adding-variables-with-mutate",
    "title": "• 2. Adding columns w mutate",
    "section": "",
    "text": "Convert growth_rate into a number, with the as.numeric() function.\nAdd the logical variable, visited, which is TRUE if a plant had more than zero pollinators visit them, and is FALSE otherwise.\n\n\n\n\nWarning… ! NAs introduced by coercion:\nYou can see that R gave us a warning. Warnings do not mean that something necessarily went wrong, but they do mean we should look and see what happened. In this case, we see that when trying to change the character string, 1.8O, into a number R did not know what to do and converted it to NA. In the next bit of code I convert it into \"1.80\" with the case_when() function.\n\n\n\n\n\n\n\n\n\n\nBe careful combining classes with case_when() Click the arrow to learn more\n\n\n\n\n\nWhen I was trying to change the character “1.8O” into the 1.80, R kept saying: Error in dplyr::mutate()… Caused by error in case_when(): ! Can’t combine ..1 (right)  and ..2 (right) . Unlike warnings, which tell you to watch out, errors tell you R cannot do what you’re asking of it. It turns out that I could not assign the number 1.80 to the vector held in petal_area_mm because I could not blend characters add numbers. So, as you can see, I replaced \"1.8O\" with \"1.80\", and then I used as.numeric() to convert the vector to numeric.",
    "crumbs": [
      "2. Data in R",
      "• 2. Adding columns w mutate"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/select_vars.html",
    "href": "book_sections/data_in_R/select_vars.html",
    "title": "• 2. Selecting columns",
    "section": "",
    "text": "select()ing columns of interest\nThe dataset above is not tiny – seventeen columns accompany the 593 rows of data. To simplify our lives, let’s use the dplyr function, select(), to limit our data to a few variables of interest:\nril_data |&gt; \n  dplyr::select(location,   prop_hybrid,  mean_visits,  \n                petal_color, petal_area_mm,  asd_mm, \n                growth_rate, visited)\n\n# A tibble: 593 × 8\n   location prop_hybrid mean_visits petal_color petal_area_mm asd_mm growth_rate\n   &lt;chr&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;\n 1 GC             0           0     white                44.0  0.447       1.27 \n 2 GC             0.125       0.188 pink                 55.8  1.07        1.45 \n 3 GC             0.25        0.25  pink                 51.7  0.674       1.8  \n 4 GC             0           0     white                57.3  0.959       0.816\n 5 GC             0           0     white                68.6  1.41        0.728\n 6 GC             0.125       0     pink                 66.3  0.788       1.76 \n 7 GC            NA          NA     &lt;NA&gt;                 51.5  0.6         1.58 \n 8 GC             0           0     white                48.1  0.561       1.48 \n 9 GC             0          NA     white                51.6  1.02        1.14 \n10 GC             0.25        0     white                89.8  0.618       1    \n# ℹ 583 more rows\n# ℹ 1 more variable: visited &lt;lgl&gt;",
    "crumbs": [
      "2. Data in R",
      "• 2. Selecting columns"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/select_vars.html#selecting-columns-of-interest",
    "href": "book_sections/data_in_R/select_vars.html#selecting-columns-of-interest",
    "title": "• 2. Selecting columns",
    "section": "",
    "text": "location: The plant’s location. The pollinator visitation experiment was limited to two locations (either SR or GC), while the hybrid seed formation study was replicated at four locations (SR, GC, LB or US). This should be a &lt;chr&gt; (character), and it is!\nprop_hybrid: The proportion of genotyped seeds that were hybrids.\n\nmean_visits: The mean number of pollinator visits recorded (per fifteen minute pollinator observation) for that RIL genotype at that site. This should be a number &lt;dbl&gt; (double), and it is.\npetal_area_mm: The area of the petals (in mm). This should be a number &lt;dbl&gt; (double), and it is!\n\nasd_mm: The distance between anther (the place where pollen comes from) and stigma (the place that pollen goes to) on a flower. The smaller this number, the easier it is for a plant to pollinated itself. This should be a number &lt;dbl&gt; (double), and it is.\ngrowth_rate: The variable we should have just fixed now it should be a number.\n\nvisited: A logical variable indicating if the plant received any visits at all.\n\n\n\n\n\n\n\n\nWarning: R does not remember this change until you assign it.\n\n\n\nSo, now that we see that our code worked as expected, enter:\n\nril_data &lt;- ril_data |&gt; \n  dplyr::select(location,  prop_hybrid,  mean_visits,  \n                petal_color, petal_area_mm,  asd_mm,  \n                growth_rate, visited)",
    "crumbs": [
      "2. Data in R",
      "• 2. Selecting columns"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/summarize_vars.html",
    "href": "book_sections/data_in_R/summarize_vars.html",
    "title": "• 2. Summarizing columns",
    "section": "",
    "text": "summarize()ing data\nWe rarely want to look at entire datasets, we want to summarize() them (e.g. finding the mean, variance, etc..).\nWe previously used the mean function to find the mean of a vector. When we want to summarize a variable in a tibble we use the function inside of summarize().\nComputing summary statistics with summarize(). The top table contains two columns: prop_hyb (proportion of hybrids) and n_hyb (the number of hybrids). The summarize(mean_hyb = mean(n_hyb)) function is applied to calculate the mean of n_hyb, producing a single-row output where mean_hyb represents the average number of hybrids across the dataset. The final result, shown in the bottom table, contains a single value of 1.\nril_data      |&gt;\n  summarize(avg_visits = mean(mean_visits))\n\n# A tibble: 1 × 1\n  avg_visits\n       &lt;dbl&gt;\n1         NA\nWe notice two things.",
    "crumbs": [
      "2. Data in R",
      "• 2. Summarizing columns"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/summarize_vars.html#summarizeing-data",
    "href": "book_sections/data_in_R/summarize_vars.html#summarizeing-data",
    "title": "• 2. Summarizing columns",
    "section": "",
    "text": "The answer was NA. This is because there are NAs in the data.\nThe results are a tibble. This is sometimes what we want and sometimes not. If you want a vector you can pull() the value.\n\n\nSummarize data in the face of NAs with the na.rm = TRUE argument.\n\nril_data      |&gt;\n  summarize(avg_visits = mean(mean_visits, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  avg_visits\n       &lt;dbl&gt;\n1      0.693\n\n\npull() columns from tibbles to extract vectors.\nMany R functions require vectors rather than tibbles. You can pull() them out as follows:\n\nril_data      |&gt;\n  summarize(avg_visits = mean(mean_visits, na.rm = TRUE))|&gt;\n  pull()\n\n[1] 0.693259\n\n\n\n\n\n\n\n\n\n“A visual explanation of the summarize() function from R for the rest of us.\n\n\n\n\n\n\n\n\n\n\nA visual explanation of the summarize() function from R for the rest of us.",
    "crumbs": [
      "2. Data in R",
      "• 2. Summarizing columns"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/summarize_vars.html#combine-group_by-andsummarize-to-describe-groups",
    "href": "book_sections/data_in_R/summarize_vars.html#combine-group_by-andsummarize-to-describe-groups",
    "title": "• 2. Summarizing columns",
    "section": "Combine group_by() andsummarize() to describe groups",
    "text": "Combine group_by() andsummarize() to describe groups\nSay we were curious about differences in pollinator visitation by location. The code below combines group_by() and summarize() to show that site SR had nearly 10 times the mean pollinator visitation per 15 minute observation than did site GC. We also see a much stronger correlation between petal area and visitation in SR than in GC, but a stronger correlation between proportion hybrid and visitation in GC than in SR. Note that the NA values for LB and US arise because we did not conduct pollinator observations at those locations.\n\nril_data      |&gt;\n  group_by(location) |&gt;\n  summarize(grand_mean = mean(mean_visits, na.rm = TRUE),\n            cor_visits_petal_area =  cor(mean_visits ,petal_area_mm, \n                use = \"pairwise.complete.obs\"),\n            cor_visits_prop_hybrid =  cor(mean_visits , prop_hybrid, \n                use = \"pairwise.complete.obs\")) # Like na.rm = TRUE, but for correlations\n\n# A tibble: 5 × 4\n  location grand_mean cor_visits_petal_area cor_visits_prop_hybrid\n  &lt;chr&gt;         &lt;dbl&gt;                 &lt;dbl&gt;                  &lt;dbl&gt;\n1 GC            0.116                0.0575                  0.458\n2 LB          NaN                   NA                      NA    \n3 SR            1.27                 0.367                   0.281\n4 US          NaN                   NA                      NA    \n5 &lt;NA&gt;        NaN                   NA                      NA    \n\n\nWe can group by more than one variable. Grouping by location and color reveals not only that white flowers are visited less than pink flowers, but also that petal area has a similar correlation with pollinator visitation for pink and white flowers.\n\nril_data      |&gt;\n  group_by(location, petal_color) |&gt;\n  summarize(avg_visits = mean(mean_visits, na.rm = TRUE),\n            cor_visits_petal_area =  cor(mean_visits ,petal_area_mm, \n                use = \"pairwise.complete.obs\")) # Like na.rm = TRUE, but for correlations\n\n# A tibble: 13 × 4\n# Groups:   location [5]\n   location petal_color avg_visits cor_visits_petal_area\n   &lt;chr&gt;    &lt;chr&gt;            &lt;dbl&gt;                 &lt;dbl&gt;\n 1 GC       pink            0.193                 0.0899\n 2 GC       white           0.0104                0.0886\n 3 GC       &lt;NA&gt;            0.2                   0.443 \n 4 LB       pink          NaN                    NA     \n 5 LB       white         NaN                    NA     \n 6 LB       &lt;NA&gt;          NaN                    NA     \n 7 SR       pink            1.76                  0.387 \n 8 SR       white           0.733                 0.458 \n 9 SR       &lt;NA&gt;            1.04                  0.717 \n10 US       pink          NaN                    NA     \n11 US       white         NaN                    NA     \n12 US       &lt;NA&gt;          NaN                    NA     \n13 &lt;NA&gt;     &lt;NA&gt;          NaN                    NA     \n\n\n\n\n\n\n\n\nA visual explanation of group_by() + summarize() from R for the rest of us\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nungroup()\nAfter summarizing, the data above are still grouped by location. You can see this under #A tibble: 6 x 4 where it says # Groups:   location [5]. This tells us that the data are still grouped by location (these groups correspond to GC, SR, US, LB and missing location information NA). It’s good practice to ungroup() next, so that R does not do anything unexpected.\n\n\nPeeling of groups: Above we grouped by location and petal_color in that order. When summarize data, by default, R peels off one group, following a “last one in is the first one out” rule. This is what is meant when R says: “summarise() has grouped output by ‘location’…”.\n\n# re-running code above and then ungrouping it.\n# note that the output no longer says `# Groups:   location [2]`\nril_data      |&gt;\n  group_by(location, petal_color) |&gt;\n  summarize(avg_visits = mean(mean_visits, na.rm = TRUE),\n            cor_visits_petal_area =  cor(mean_visits ,petal_area_mm, \n                use = \"pairwise.complete.obs\"))|&gt;\n  ungroup()\n\n# A tibble: 13 × 4\n   location petal_color avg_visits cor_visits_petal_area\n   &lt;chr&gt;    &lt;chr&gt;            &lt;dbl&gt;                 &lt;dbl&gt;\n 1 GC       pink            0.193                 0.0899\n 2 GC       white           0.0104                0.0886\n 3 GC       &lt;NA&gt;            0.2                   0.443 \n 4 LB       pink          NaN                    NA     \n 5 LB       white         NaN                    NA     \n 6 LB       &lt;NA&gt;          NaN                    NA     \n 7 SR       pink            1.76                  0.387 \n 8 SR       white           0.733                 0.458 \n 9 SR       &lt;NA&gt;            1.04                  0.717 \n10 US       pink          NaN                    NA     \n11 US       white         NaN                    NA     \n12 US       &lt;NA&gt;          NaN                    NA     \n13 &lt;NA&gt;     &lt;NA&gt;          NaN                    NA",
    "crumbs": [
      "2. Data in R",
      "• 2. Summarizing columns"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/choose_rows.html",
    "href": "book_sections/data_in_R/choose_rows.html",
    "title": "• 2. Choose rows",
    "section": "",
    "text": "Remove rows with filter()\nThere are reasons to remove rows by their values. For example, we could remove plants from US and LB locations. We can achieve this with the filter() function as follows:\nFigure 1: Using filter() to subset data based on a condition. The top table contains two columns: prop_hyb (proportion of hybrids) and petal_color (flower color), with values including both “white” and “pink” flowers. The function filter(petal_color == \"pink\") is applied to retain only rows where petal_color is “pink.” The resulting dataset, shown in the bottom table, excludes the “white” flower row and keeps only the observations where petal color is “pink.”\nOR, equivalently\n# rerunning the code summarizing visitation by \n# location and petal color and then ungrouping it\n# but filtering out plants from locations US and LB\n# note that the output no longer says `# Groups:   location [2]`\nril_data      |&gt;\n  filter(location != \"US\" & location != \"LB\")|&gt;\n  group_by(location, petal_color) |&gt;\n  summarize(avg_visits = mean(mean_visits, na.rm = TRUE),\n            cor_visits_petal_area =  cor(mean_visits ,petal_area_mm, \n                use = \"pairwise.complete.obs\"))|&gt;\n  ungroup()\n\n# A tibble: 6 × 4\n  location petal_color avg_visits cor_visits_petal_area\n  &lt;chr&gt;    &lt;chr&gt;            &lt;dbl&gt;                 &lt;dbl&gt;\n1 GC       pink            0.193                 0.0899\n2 GC       white           0.0104                0.0886\n3 GC       &lt;NA&gt;            0.2                   0.443 \n4 SR       pink            1.76                  0.387 \n5 SR       white           0.733                 0.458 \n6 SR       &lt;NA&gt;            1.04                  0.717",
    "crumbs": [
      "2. Data in R",
      "• 2. Choose rows"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/choose_rows.html#remove-rows-with-filter",
    "href": "book_sections/data_in_R/choose_rows.html#remove-rows-with-filter",
    "title": "• 2. Choose rows",
    "section": "",
    "text": "ril_data |&gt; filter(location == \"GC\" | location == \"SR\"): To only retain samples from GC or (noted by |) SR. Recall that == asks the logical question, “Does the location equal SR?” So combined, the code reads “Retain only samples with location equal to SR or location equal to GC.”\n\n\n\nril_data |&gt; filter(location != \"US\" & location != \"LB\"): To remove samples from US and (noted by &) LB. Recall that != asks the logical question, “Does the location not equal US?” Combined the code reads “Retain only samples with location not equal to US and with location not equal to LB.”\n\n\n\nWarning! Remove one thing can change another:\nThink hard about removing things (e.g. missing data), and if you decide to remove things, consider where in the pipeline you are doing so. Removing one thing can change another. For example, compare:\n\nril_data |&gt;\n  filter(!is.na(petal_color))|&gt;\n  summarise(avg_visits = mean(mean_visits, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  avg_visits\n       &lt;dbl&gt;\n1      0.705\n\n\nand\n\nril_data |&gt;\n  summarise(avg_visits = mean(mean_visits, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  avg_visits\n       &lt;dbl&gt;\n1      0.693\n\n\nThese answers differ because when we removed plants with no petal color information we also removed their pollinator visitation values.",
    "crumbs": [
      "2. Data in R",
      "• 2. Choose rows"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/data_in_R_summary.html",
    "href": "book_sections/data_in_R/data_in_R_summary.html",
    "title": "• 2. Data in R summary",
    "section": "",
    "text": "Chapter summary\nLinks to: Summary, Chatbot Tutor, Practice Questions, Glossary, R functions, R packages introduced, and Additional resources.\nA beautiful Clarkia xantiana flower.\nKeeping data in the tidy format—where each column represents a variable and each row represents an observation—allows you to fully leverage the powerful tools of the tidyverse. In the tidyverse, data are stored in tibbles, a modern update to data frames that enhances readability and maintains consistent data types. The dplyr package offers a suite of intuitive functions for transforming and analyzing data. For example, mutate() lets you create or modify variables, while summarize() computes summary statistics. When paired with group_by(), you can easily generate summaries across groups. Other essential functions include select() for choosing columns, filter() for subsetting rows, rename(), and arrange() for ordering data. Together—and especially when used with the pipe operator (group_by(...) |&gt; summarize(...))—these tools enable clear, reproducible workflows. In the next chapter, you’ll see how tidy data also powers beautiful and flexible plots using ggplot2.",
    "crumbs": [
      "2. Data in R",
      "• 2. Data in R summary"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/data_in_R_summary.html#data_in_R_summarySummary",
    "href": "book_sections/data_in_R/data_in_R_summary.html#data_in_R_summarySummary",
    "title": "• 2. Data in R summary",
    "section": "",
    "text": "Chatbot tutor\n\nPlease interact with this custom chatbot (link here) I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you got what you needed from it.\n\n\n\nPractice Questions\nTry these questions! Use the R environment below to work without changing tabs.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ1) The code above returns the error: \"Error: could not find function \"summarise\"\". How can you solve this?\n\n Change “summarise” to “summarize” load the dplyr library\n\n\nQ2) Revisit the pollinator visitation dataset we explored. Which location has a greater anther stigma distance (asd_mm)? They are the sameSMGCS6\n\n\nClick here for explanation\n\n\nril_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_rils.csv\"\nril_data &lt;- readr::read_csv(ril_link)\nril_data      |&gt;\n  group_by(location) |&gt;\n  summarise(avg_asd = mean(asd_mm, na.rm = TRUE))\n\n# A tibble: 5 × 2\n  location avg_asd\n  &lt;chr&gt;      &lt;dbl&gt;\n1 GC         0.920\n2 LB         0.861\n3 SR         0.921\n4 US         0.866\n5 &lt;NA&gt;     NaN    \n\n\n\n\nQ3) Consider the table below. The data are tidynot tidy\n\n\n\n\n\nlocation\nGC\nGC\nGC\nGC\nGC\nGC\n\n\nril\nA1\nA100\nA102\nA104\nA106\nA107\n\n\nmean_visits\n0.0000\n0.1875\n0.2500\n0.0000\n0.0000\n0.0000\n\n\n\n\n\n\n\nClick here for explanation\n\nHere the data are transposed, so the data are not tidy. Remember in tidy data each variable is a column, not a row. This is particularly hard for R because there are numerous types of data in a column.\n\n.\n\nQ4 Consider the table below. The data are tidynot tidy \n\n\n\n\n\nlocation-ril\nmean_visits\n\n\n\n\nGC-A1\n0.0000\n\n\nGC-A100\n0.1875\n\n\nGC-A102\n0.2500\n\n\nGC-A104\n0.0000\n\n\nGC-A106\n0.0000\n\n\nGC-A107\n0.0000\n\n\n\n\n\n\n\nClick here for explanation\n\nHere location and ril are combined in a single column, so the data are not tidy. Remember in tidy data each variable is its own column. It would be hard to get e.g. means for RILs of locations in this format.\n\n\nQ5 Consider the table below. The data are tidynot tidy\n\n\n\n\n\nril\nGC\nSR\n\n\n\n\nA1\n0.0000\n0.6667\n\n\nA100\n0.1875\n0.5833\n\n\nA102\n0.2500\n0.6667\n\n\nA104\n0.0000\n1.7500\n\n\nA106\n0.0000\n0.5000\n\n\nA107\n0.0000\n1.5000\n\n\n\n\n\n\n\nClick here for explanation\n\nThis is known as “wide format” and is not tidy. Here the variable, location, is used as a column heading. This can be a fine way to present data to people, but it’s not how we are analyzing data.\n\n\nQ6 You should always make sure data are tidy when (pick best answer)\n\n collecting data presenting data analyzing data with dplyr all of the above\n\n\nQ7 What is wrong with the code below (pick the most egregious issue).\n\n I overwrote iris and lost the raw data I did not show the output I used summarise() rather than summarize() I did not tell R to remove missing data when calculating the mean.\n\n\niris &lt;- iris |&gt; \n  summarise(mean_sepal_length =  mean(Sepal.Length))\n\n\nQ8 After running the code below, how many rows and columns will the output tibble have? NOTE The original data has 593 rows, 7 columns and 186 unique RILs* 1 row, 1 column1 row, 2 columns2 rows, 2 columns186 rows, 2 columns186 rows, 7 columns593 rows, 2 columns593 rows, 7 columns\n\nril_data   |&gt;\n    group_by(ril) |&gt;\n    summarize(avg_visits = mean(mean_visits, na.rm = TRUE))\n\n\n\n\n\nGlossary of Terms\n\n\nTidy Data A structured format where:\n\nEach row represents an observation.\n\nEach column represents a variable.\n\nEach cell contains a single measurement.\n\nTibbles: A modern form of a data frame in R with:\n\nCleaner printing (only first 10 rows, fits columns to screen).\n\nExplicit display of data types (e.g., , ).\n\nStrict subsetting (prevents automatic type conversion).\n\nCharacter data is not automatically converted to factors.\n\nPiping (|&gt;) functions: A way to chain operations together, making code more readable and modular.\n\nGrouping in Data Analysis: Grouped operations allow calculations within subsets of data (e.g., mean visits per location).\nMissing Data (NA): R uses NA to represent missing values. Operations with NA return NA unless handled explicitly (e.g., na.rm = TRUE to ignore missing values, use = \"pairwise.complete.obs\", etc).\n\nWarnings: Indicate a possible issue but allow code to run (e.g., NAs introduced by coercion).\n\nErrors: Stop execution completely when something is invalid.\n\n\n\n\n\nKey R functions\n\n\nread_csv() (readr): Reads a CSV file into R as a tibble, automatically guessing column types.\nselect() (dplyr): Selects specific columns from a dataset.\nmutate() (dplyr): Creates or modifies columns in a dataset.\ncase_when() (dplyr): Replaces values conditionally within a column.\nas.numeric(): Converts a vector to a numeric data type.\nsummarize() (dplyr): Computes summary statistics on a dataset (e.g., mean, sum).\nmean(): Computes the mean (average) of a numeric vector.\n\nArgument: na.rm = TRUE: An argument used in functions like mean() and sd() to remove missing values (NA) before computation.\n\npull() (dplyr): Extracts a single column from a tibble as a vector.\ngroup_by() (dplyr): Groups data by one or more variables for summary operations.\n|&gt; (Base R Pipe Operator): Passes the result of one function into another, making code more readable.\n\n\n\n\n\nR Packages Introduced\n\n\nreadr: A tidyverse package for reading rectangular data files (e.g., read_csv()).\ndplyr: A tidyverse package for data manipulation, including mutate(), glimpse(), and across().\n\n\n\n\nAdditional resources\n\nR Recipes:\n\nSelecting columns.\n\nAdd a new column (or modify an existing one).\n\nSummarize data.\nSummarize data by group.\n\nOther web resources:\n\nChapter 10: Tidy data from R for data science (Grolemund & Wickham (2018)).\n\nAnimated dplyr functions from R or the rest of us.\n\nVideos:\n\nBasic Data Manipulation (From Stat454).\nCalculations on tibble (From Stat454).\n\n\n\n\n\n\nGrolemund, G., & Wickham, H. (2018). R for data science.",
    "crumbs": [
      "2. Data in R",
      "• 2. Data in R summary"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot.html",
    "href": "book_sections/intro_to_ggplot.html",
    "title": "3. Introduction to ggplot",
    "section": "",
    "text": "Always visualize your data.\nAnscombe’s quartet famously displays four data sets with identical summary statistics but very different interpretations (Figure 1, Table 1). The key lesson from Anscombe’s quartet is that statistical summaries can miss the story in our data, and therefore must be accompanied by clear visualization of patterns in the data.\ndataset\nsd.x\nsd.y\ncor\nmean.x\nmean.y\n\n\n\n\n1\n3.32\n2.03\n0.82\n9\n7.5\n\n\n2\n3.32\n2.03\n0.82\n9\n7.5\n\n\n3\n3.32\n2.03\n0.82\n9\n7.5\n\n\n4\n3.32\n2.03\n0.82\n9\n7.5\n\n\n\n\n\n\nTable 1: Summary statistics for Anscombe’s quartet.\nFigure 1: Four datasets with identical summary statistics but distinct visual patterns. Each panel represents a dataset with nearly identical means (shown in red), variances, correlations, and regression lines (shown in blue), yet their scatterplots reveal strikingly different structures.\nTherefore before diving into formal analysis, we always generate exploratory plots to uncover key patterns, detect data quality issues, and reveal the underlying structure of the data. One quick exploratory plot is rarely enough though, as you get to know and model your data, you will develop additional visualizations to dig deeper into the story.\nUltimately, after a combination of exploratory plots, summary statistics, and statistical modelling has helped you understand the data, you will generate well-crafted explanatory plots to communicate your insight elegantly to your audience. We will focus on the process of making explanatory plots in a later chapter.",
    "crumbs": [
      "3. Introduction to ggplot"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot.html#always-visualize-your-data.",
    "href": "book_sections/intro_to_ggplot.html#always-visualize-your-data.",
    "title": "3. Introduction to ggplot",
    "section": "",
    "text": "Detect data quality issues early It is common to split up data collect to a team and maybe someone enters data in centimeters, and someone else in inches (etc). Or maybe for a data point or two a decimal point was lost, a value is way bigger than it should be. Everytime you collect and enter data you should make some plots to help identify any such data issues.",
    "crumbs": [
      "3. Introduction to ggplot"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot.html#focus-on-biological-questions.",
    "href": "book_sections/intro_to_ggplot.html#focus-on-biological-questions.",
    "title": "3. Introduction to ggplot",
    "section": "Focus on biological questions.",
    "text": "Focus on biological questions.\nBefore starting your R coding, consider the plots you want to make and why. It is all too easy to get stuck in the R vortex—making plots simply because they’re easy to create, visually appealing, fun, or challenging, and before you know it you’ve wasted an hour doing something that did not move your analysis or understanding forward. So before make a (set of) plot(s) always consider\n\nThe thing you want to know, and how it relates to the motivating biology. This includes\n\nIdentifying explanatory and response variables, and\nDistinguishing between key explanatory variables from covariates that you may not care about but need to include.\n\n\nThat the visualization of data reflects your biological motivation.This is particularly tricky for categorical variables which R loves to put in alphabetical order but may likely have a reasonable order to highlight patterns or biology.\n\n\nFor the Clarkia RIL datasets:.\nWe primarily want to know which (if any) phenotypes influence the amount of pollinator visitation and hybrid seed formation.\n\nOur response variables are hybrid seed production and pollinator visitation (and perhaps we would like to know the extent to which pollinator visitation predicts the proportion of hybrid seed).\n\nOur explanatory variables that we care a lot about include: petal area, petal color, herkogamy, protandry etc… We also want to account for differences in the location of the experiment, even though this is not motivating our study.\n\nWe also may want to evaluate the extent to which correlation between traits were broken up as we made the RILs. If trait correlations persists in the RILs it means that we cannot fully dissect the contribution of each trait to the outcome we care about, and that the genetic and/or physiological linkage between traits may have prevented evolution from acting independently on these traits.\n\nIn this case there is not a natural order to our categorical variables, so we do not need to think too hard about that.\n\n\n\n\n\n\n\nVideo\n\n\nFigure 2: Tweet from Shasta E. Webb (@webbshasta) about how she makes a plot. “My approach to figure-making in #ggplot ALWAYS begins with sketching out what I want the final product to look like. It feels a bit analog but helps me determine which #geom or #theme I need, what arrangement will look best, & what illustrations/images will spice it up. #rstats”\n\n\n\n\nLet’s sketch some potential plots to address these questions.\nI recommend sketching out what you want your plot to look like and what alternative results would look like (Figure 2). This helps to ensure that you are making the plot you want, not the one R gives you. In this case, some potentially important questions are:\n\nWhat is the distribution of the number of pollinator visits?\nDo we see different visitation by location?\n\nAre pink flowers more likely to be visited than white flowers?\n\nHow does the number of visits change with petal area, and does this depend on petal color?\n\nDoes pollinator visitation predict hybridization rate?\n\nSee Figure 3 for examples of how these may look.\n\n\n\n\n\n\n\n\nFigure 3: Brainstorming potential figures.",
    "crumbs": [
      "3. Introduction to ggplot"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot.html#the-idea-of-ggplot",
    "href": "book_sections/intro_to_ggplot.html#the-idea-of-ggplot",
    "title": "3. Introduction to ggplot",
    "section": "The idea of ggplot",
    "text": "The idea of ggplot\n\n\n\n\nAs described in the video above ggplot is based on the grammar of graphics, a framework for constructing plots by mapping data to visual aesthetics.. A major idea here is that plots are made up of data that we map onto aesthetic attributes, and that we build up plots layer by layer.\nLet’s unpack this sentence, because there’s a lot there. Say we wanted to make a very simple plot e.g. observations for categorical data, or a simple histogram for a single continuous variable. Here we are mapping this variable onto a single aesthetic attribute – the x-axis.\n\nWe are using the ggplot2 package to make plots.\nIf you do not have ggplot2 installed, type:\n\ninstall.packages(\"ggplot2\") # Do this the 1st time!\n\nIf you have ggplot2 installed or you just installed it, every time you start a new R session you still need to enter\n\nlibrary(ggplot2)\n\n\n\n\nShow code to load and format the data set, so we are where we left off.\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nril_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_rils.csv\"\nril_data &lt;- readr::read_csv(ril_link)|&gt;\n  dplyr::select(location,  ril, prop_hybrid,  mean_visits,  \n                petal_color, petal_area_mm,  asd_mm)|&gt;\n  dplyr::mutate(visited = mean_visits &gt; 0)",
    "crumbs": [
      "3. Introduction to ggplot"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot.html#making-ggplots",
    "href": "book_sections/intro_to_ggplot.html#making-ggplots",
    "title": "3. Introduction to ggplot",
    "section": "Making ggplots:",
    "text": "Making ggplots:\nThe following sections show how to make plots that:\n\nVisualize the distribution of a single continuous variable.\n\nSaving a ggplot.\n\nCompare a continuous variable for different categorical variables.\n\nVisualize associations between two categorical variables.\n\nVisualize associations between two continuous variables.\n\nVisualize multiple explanatory variables.\n\nThen we summarize the chapter, present a chatbot tutor, practice questions, a glossary, a review of R functions and R packages introduced, and provide links to additional resources.\nFinally, for the true aficionados, I introduce ggplotly as a great way to get to know your data.",
    "crumbs": [
      "3. Introduction to ggplot"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot/one_continuous.html",
    "href": "book_sections/intro_to_ggplot/one_continuous.html",
    "title": "• 3. A continuous variable",
    "section": "",
    "text": "Motivating scenario: You want to see the distribution of a single continuous variable.\nLearning goals: By the end of this sub-chapter you should be able to\n\nFamiliarize yourself with making plots using the ggplot2 package.\nUse geom_histogram() to make a histogram.\n\nSpecify the width (with binwidth) or number (with bins) of bins in a histogram.\n\n\nUse geom_density() to make a density plot.\nChange the outline color (with color) and the fill color (with fill).\n\n\n\n\nVisualizing Distributions\nLet’s first consider the distribution of a single continuous variable—pollinator visitation. There are some natural questions we would want answered early on in our analysis:\n\nAre most flowers visited frequently, or do visits tend to be rare?\n\nIs the distribution symmetric, or is it skewed, with many flowers receiving few or no visits and a small number receiving many?\n\nAs we will see throughout the term, understanding the shape of our data helps guide our analysis. Let’s start with two common visualizations for distributions:\n\n\n\n\n\n\n\n\n\n\nA histogram bins the x-axis variable (in this case, visitation) into intervals and shows the number of observations in each bin on the y-axis. This allows us to see how frequently different levels of visitation occur.\n\nA density plot fits a smooth function to the histogram, providing a continuous representation of the distribution. This smoothing can sometimes make patterns easier (or harder) to see. Later, we’ll see that density plots can also help compare distributions.\n\nMaking histograms & density plots: A Step-by-Step Guide. To create these visualizations in ggplot2, we need to (minimally) specify three key layers:\n\nThe data layer: This is the dataset we’re plotting—in this case, ril_data. We pass this as an argument inside the ggplot() function, e.g., ggplot(data = ril_data).\n\nThe aesthetic mapping: This tells R how to map each variable onto the plot. In a histogram, we map the variable whose distribution we want to visualize (in this case, mean_visits) onto the x-axis. We define this inside the aes() argument within ggplot(), e.g., ggplot(data = ril_data, aes(x = mean_visits)).\n\nThe geometric object (geom) that displays the data: In this case, we use geom_histogram() for a histogram or geom_density() for a density plot. These are added to the plot using a +, as shown in the examples below.\n\n\nSet upAdding a geomElaborationsDensity plot\n\n\n\nInside the ggplot() function, we tell R that we are working with the ril_data, and we map mean_visits onto x inside aes(). The result is a boring canvas, but it is the canvas we build on.\n\nggplot(data = ril_data, aes(x = mean_visits))\n\n\n\n\n\n\n\n\n\n\n\n\nBut now we want to show the data. To do so we need to add our geom. In this case we show the data as a histogram.\nThe resulting plot shows that most plants get zero visits, but some get up to five visits.\n\nggplot(ril_data,aes(x = mean_visits))+\n  geom_histogram()\n\n\n\n\n\n\n\n\n\n\n\n\nTo take a bit more control of my histogram, I like to\n\nSpecify the number or width of bins (with the binwidth and bins arguments, respectively).\n\nAdd white lines between bins to make them easier to see (with color = \"white\").\n\nSpruce up the bins by specifying the color that fills them (with e.g. fill = \"pink\").\n\n\nggplot(ril_data, aes(x = mean_visits))+\n  geom_histogram(binwidth = .2, color = \"white\", fill = \"pink\")\n\n\n\n\n\n\n\n\n\n\n\n\nNow you can easily make a density plot if you prefer!\n\nggplot(ril_data, aes(x = mean_visits))+\n  geom_density(color = \"white\", fill = \"pink\")\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: Returning to our motivating questions, we see that most plants receive no visits, but this distribution is skewed with some plants getting lots of visits.",
    "crumbs": [
      "3. Introduction to ggplot",
      "• 3. A continuous variable"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot/saving_ggplots.html",
    "href": "book_sections/intro_to_ggplot/saving_ggplots.html",
    "title": "• 3. Saving a ggplot",
    "section": "",
    "text": "Motivating scenario: You have made a plot and want to save it!\nLearning goals: By the end of this sub-chapter you should be able to\n\nSave a ggplot using either a screen grab, RStudio’s point-and-click options, or the ggsave() function.\n\nKnow the costs and benefits of each approach and when to use which.\n\n\n\n\nSaving Your ggplot.\nYou probably want to save your plot once you’ve made one you like. There are several ways to do this, each with its own pros and cons.\n\n\n\n\n\n\n\n\n\nThe quickest approach is to simply take a screenshot – I do this quite often, and it is great because the plot you save looks exactly like the one on your screen. However, these plots are not vectorized and can lose quality in other ways, so I usually use these as a quick first pass for exploratory data analysis, rather than a refined solution.\nThe next fastest way is to use RStudio’s built-in tools – simply click on the plot in the Plots panel, then use the Export button to copy or save the image. This allows you to choose a file format (like PNG or PDF), set the dimensions, and adjust the resolution. This method is convenient and good for quick outputs, but it’s manual, which means it doesn’t lend itself to reproducible workflows — if you make changes and regenerate your plot later, you’ll have to go through the same export process again.\nFor more control and reproducibility, I suggest using the ggsave() function. This function saves the most recently displayed plot by default, or you can specify a plot object manually. You can choose the file type simply by specifying the file extension (e.g., .png, .pdf, .svg) and control the size and resolution of the output. For example, the code below will save a high-resolution pdf file, called ril_visit_hist.pdf:\n\nggplot(ril_data, aes(x = mean_visits))+\n  geom_histogram(binwidth = .2, color = \"white\", fill = \"pink\")\n\nggsave(\"ril_visit_hist.pdf\", width = 6, height = 4)\n\n\n\n\n\n\n\nWarning\n\n\n\nWhen using either ggsave() or RStudio’s built-in tools always check that your saved plot looks as expected, as it is often slightly different than what you saw in your R session.",
    "crumbs": [
      "3. Introduction to ggplot",
      "• 3. Saving a ggplot"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot/cont_cat.html",
    "href": "book_sections/intro_to_ggplot/cont_cat.html",
    "title": "• 3. Continuous y/categorical x",
    "section": "",
    "text": "Motivating scenario: You want to compare a continuous variable across different levels of a categorical explanatory variable.\nLearning goals: By the end of this sub-chapter you should be able to\n\nUnderstand the challenge of overplotting.\n\nUse geom_point, geom_jitter() and/or geom_boxplot() to visualize the distribution.\n\nUse the size and alpha arguments to adjust the size and transparency of points.\n\n\nCombine geom_jitter() and/or geom_boxplot(), noting that\n\nOrder matters - always plot jittered points on top of (i.e. after) boxplots.\n\nWhen showing boxplots and jittered points, make sure the boxplot does not show outliers, otherwise those points will be shown twice.\n\n\n\n\n\nVisualizing associations between a continuous response and a categorical explanatory variable.\nWe often want to know more than just the distribution of variables, we want to know which, if any, explanatory variables are associated with this variation. So, for example, we may want to know if pollinator visitation differs by location. In this case we map the categorical variable (location) onto the x-axis and the continuous response (visitation) onto the y-axis.\n\n\n\n\n\n\n\n\n\nTo compare visitation by site, we start with this setup:\n\nFilter our data for samples from \"GC\" and \"SR\" (because we did not conduct pollinator observations at the other sites).\n\nPipe this directly into our ggplot() function.\n\nNote: We do not need to specify the data argument when piping data. That is because ggplot inherits data from the pipe.\n\nril_data |&gt; \n  filter(location == \"GC\" | location == \"SR\") |&gt; # Pipe data into ggplot\n  ggplot(aes(x = location, y = mean_visits))\n\nContinuous response to a categorical explanatory variable: A Step-by-Step Guide: In the tabs below I show some options for our geom.\n\npoint()jitter()boxplot() + jitter()\n\n\n\nThe Simplest Plot\n\nUses the geom_point() function to display the data.\n\nWe can add a mean using stat_summary().\n\nNote: I plot the mean in red to make it stand out.\n\nril_data |&gt; \n  filter(location == \"GC\" | location == \"SR\") |&gt;  # Pipe data into ggplot\n  ggplot(aes(x = location, y = mean_visits)) +\n  geom_point(size =2, alpha = 0.5) +\n  stat_summary(size = 1.2, color = \"red\")\n\n\n\n\n\n\n\n\nHowever, these figures can be difficult to interpret when many points overlap, making it hard to distinguish individual data points—this issue is known as over-plotting.\nOne way to address over-plotting is to adjust the transparency of points using the alpha argument. There are several other techniques to handle this, which we’ll explore in the next section.\n\n\n\n\nJitter plots spread out data along the x-axis. This works well with categorical predictors but can be misleading when used with continuous predictors. To improve clarity, I make a few adjustments in the geom_jitter() function:\n\nI set height = 0 to keep the y-values unchanged. I always do this.\n\nI set width = 0.3 to prevent points from overlapping too much between categories. You may experiment with this value to find the best fit for your plot.\n\nI make the points larger (size = 3) and partially transparent (alpha = 0.5) to help visualize overlap more effectively.\n\n\nril_data |&gt; \n  filter(location == \"GC\" | location == \"SR\") |&gt; # Pipe data into ggplot\n  ggplot(aes(x = location, y = mean_visits))+\n  geom_jitter(height = 0, width = .3, size = 3, alpha = .5)+\n  stat_summary(size = 1.2,color = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n\nWe can add multiple geom layers to a plot. For example, we can combine a boxplot (geom_boxplot()) with a jitter plot. However, there are a few important things to keep in mind:\n\nOrder matters. geom_boxplot() + geom_jitter() places points over the boxplot, making the data visible, while geom_jitter() + geom_boxplot() places the boxplot on top, potentially obscuring the points.\n\nHandling outliers. geom_boxplot() automatically displays outliers as individual points, which is useful when we’re not showing the raw data. However, if we add jittered points with geom_jitter(), these outliers will appear twice, potentially misleading us. To avoid this, I set outlier.shape = NA in the boxplot.\n\n\nril_data |&gt; \n  filter(location == \"GC\" | location == \"SR\") |&gt; # Pipe data into ggplot\n  ggplot(aes(x = location, y = mean_visits))+\n  geom_boxplot(outlier.shape = NA) + \n  geom_jitter(height = 0, width = .3, size = 3, alpha = .5)\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: Returning to our motivating question, we see that plants at the SR population receive way more visits by pollinators than do plants at GC.",
    "crumbs": [
      "3. Introduction to ggplot",
      "• 3. Continuous y/categorical x"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot/two_categorical.html",
    "href": "book_sections/intro_to_ggplot/two_categorical.html",
    "title": "• 3. Two categorical variables",
    "section": "",
    "text": "Motivating scenario: You want to explore how two categorical variables are associated.\nLearning goals: By the end of this sub-chapter you should be able to\n\nMake barplots with geom_bar() and geom_col().\n\nMake stacked and grouped barplots.\n\nKnow when to use geom_bar() and when to use geom_col().\n\n\n\n\nCategorical explanatory and response variables\nAbove, we saw that most plants received no visits, so we might prefer to compare the proportion of plants that did and did not receive a visit from a pollinator by some explanatory variable (e.g. petal color or location). Recall that we have added the logical variable, visited, by typing mutate(visited = mean_visits &gt; 0).\n\n\n\n\n\n\n\n\n\nMaking bar plots: A Step-by-Step guide. There are two main geoms for making bar plots, depending on the structure of our data:\n\nIf we have raw data (i.e. a huge dataset with values for each observation) use geom_bar().\n\nIf we have aggregated data (i.e. a summary of a huge dataset with counts for each combination of variables) use geom_col()\n\nNote: Here we map petal color onto the x-axis, and visited (TRUE / FALSE) onto the fill aesthetic.\n\nStacked barplotGrouped barplotAggregated data\n\n\n\n\nril_data |&gt; \n    filter(!is.na(petal_color), !is.na(mean_visits))|&gt;\n    mutate(visited = mean_visits &gt;0)|&gt;\n  ggplot(aes(x = petal_color, fill = visited))+\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n\n\nril_data |&gt; \n    filter(!is.na(petal_color), !is.na(mean_visits))|&gt;\n    mutate(visited = mean_visits &gt;0)|&gt;\n  ggplot(aes(x = petal_color, fill = visited))+\n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\n\n\nIf you had aggregated data, like that below. We need to plot these data somewhat differently. There are two key differences:\n\nWe map our count (in this case n) onto the y aesthetic.\n\nWe use geom_col() instead of geom_bar().\n\n\n\n\n\n\nlocation\npetal_color\nvisited\nn\n\n\n\n\nGC\npink\nFALSE\n32\n\n\nGC\npink\nTRUE\n23\n\n\nGC\nwhite\nFALSE\n46\n\n\nGC\nwhite\nTRUE\n2\n\n\nSR\npink\nFALSE\n1\n\n\nSR\npink\nTRUE\n56\n\n\nSR\nwhite\nFALSE\n11\n\n\nSR\nwhite\nTRUE\n39\n\n\n\n\n\n\nggplot(data = aggregated_pollinator_obs, \n       aes(x = petal_color, y = n, fill = visited))+\n  geom_col()\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: We see that a greater proportion of pink-flowered plants receive visits compared to white-flowered plants.",
    "crumbs": [
      "3. Introduction to ggplot",
      "• 3. Two categorical variables"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot/two_continuous.html",
    "href": "book_sections/intro_to_ggplot/two_continuous.html",
    "title": "• 3. Two continuous variables",
    "section": "",
    "text": "Motivating scenario: You want to visualize the relationship between two continuous variables.\nLearning goals: By the end of this sub-chapter you should be able to\n\nMake a scatterplot with geom_point().\n\nShow trends with geom_smooth().\n\nShow a linear regression with method = \"lm\"\n\nShow a moving average with method = \"loess\"\nOptionally suppress the grey ribbon by setting se = FALSE.\n\nTransform the scale of x or y axis with e.g. scale_x_continuous(trans = \"log1p\").\n\n\n\n\nVisualizing associations between continuous variables.\nWhen Brooke watched pollinators visit parviflora recombinant inbred lines (RILs), she was hoping that these observations also informed the probability of hybrid seed set. The first step in evaluating this hypothesis is to generate a scatterplot – a visualization that shows the association between continuous variables.\n\n\n\n\n\n\n\n\n\nWe use geom_point() to make such a plot and add trends with geom_smooth(). There are numerous types of trendlines we could add with the method argument:\n\nTo add the best fit linear model type method = lm.\n\nTo add a smoothed moving average type method = loess.\n\nBy default ggplot adds uncertainty about its guess of the line with a grey background. This is sometimes helpful but can get in the way. To remove it type se = FALSE.\nAt times transforming the data makes patterns easier to see. We can transform our presentation of the data with the trans argument in the scale_x_continuous() (or scale_y_continuous()) functions.\n\npoint()transformlinear trendsmoothed average\n\n\n\nWe present the data with geom_point(). Because some data points overlapped\n\nI increased the point size (size = 3) and\n\nI made the points partially transparent (alpha = 0.4).\n\nA small jitter could have been ok, but jittering continuous values gives me the ick, because I want my data presented faithfully.\n\nggplot(ril_data, aes(x = mean_visits, y  = prop_hybrid))+\n  geom_point(size = 3, alpha = .4)\n\n\n\n\n\n\n\n\n\n\n\n\nSometimes nonlinear scales better reveal trends. Transforming the scale on which the data are presented (rather than transforming the data itself) is nice because we retain original values.\n\nggplot(ril_data, aes(x = mean_visits, y  = prop_hybrid))+\n  geom_point(size = 3, alpha = .4)+\n  scale_x_continuous(trans = \"log1p\")\n\n\n\n\n\n\n\n\n\nTake care not to lose data when transforming: The log of any number less than or equal to zero is undefined. To avoid losing these data points, I transformed the data as log(x+1) with the \"log1p\" transformation. If the data were all greater than one, I could have used the log log10 or sqrt transform.\n\n\n\n\n\nThe geom_smooth() allows us to highlight patterns in our data. There are lots of ways to draw trends through data, and we can specify how we want to do so with the method argument.\nHere I present the standard “best-fit” line with method = \"lm\". The grey area around that line represents plausible lines that would also have been statistically acceptable (more on that later).\n\nggplot(ril_data, aes(x = mean_visits, y  = prop_hybrid))+\n  geom_point(size = 3, alpha = .4)+\n  geom_smooth(method = \"lm\")+\n  scale_x_continuous(trans = \"log1p\")\n\n\n\n\n\n\n\n\n\n\n\n\nSometimes a simple line does not do a great job of highlighting patterns. Specifying method = loess presents a smoothed moving average.\nIn addition to showing that moving average, I show you how to suppress that grey area with se = FALSE. I tend to like to include the uncertainty in our estimated trend so I usually don’t do this, but sometimes showing the uncertainty hides other features of the data, so I wanted to empower you.\n\nggplot(ril_data, aes(x = mean_visits, y  = prop_hybrid))+\n  geom_point(size = 3, alpha = .4)+\n  geom_smooth(method = \"loess\")+\n  scale_x_continuous(trans = \"log1p\")\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: Returning to our motivating question, we see that the proportion of seeds that are hybrids appears to increase with pollinator visitation. Later in the term we will address this question more rigorously.",
    "crumbs": [
      "3. Introduction to ggplot",
      "• 3. Two continuous variables"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot/many_explanatory.html",
    "href": "book_sections/intro_to_ggplot/many_explanatory.html",
    "title": "• 3. Many explanatory vars",
    "section": "",
    "text": "Motivating scenario: You want to visualize associations between a response variable and numerous explanatory variables.\nLearning goals: By the end of this sub-chapter you should be able to\n\nUse the color or fill aesthetics to map categorical variables.\n\nUse “small multiples” with facet_wrap() and facet_grid()\n\n\n\n\nThe challenge of many explanatory variables\nThere is usually more than one thing going on in a scientific study, and we may want to see how different combinations of explanatory variables are associated with a response. This can be a challenge over the term, you will encounter many tips and tricks to help in this task. For now we will look at two useful approaches:\n\nMapping different explanatory variables to different aesthetics.\n\nThe use of “small multiples”.\n\n\n\nMultiple aesthetic mappings\nWe saw that the number of pollinators increased with petal size and was greater for pink than white flowers. However, visualizing a response variable as a function of its multiple explanatory variables together can help us home in on the patterns.\n\nTo do so we can make a scatterplot and map petal color onto the color aesthetic.\n\nWe can even use size and color as extra aesthetics to map onto. I show you how to do this, but use it sparingly, because too many extra aesthetics may provide more distraction than insight.\n\n\n\n\n\n\n\n\n\n\n\nTwo explanatory variablesThree explanatory variables\n\n\n\n\nril_data |&gt;\n  filter(!is.na(petal_color))|&gt;\n  ggplot(aes(x = petal_area_mm, \n             y = mean_visits, \n             color  = petal_color))+\n  geom_point(size = 3, alpha = .4)+\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\nHere I added anther stigma distance as another explanatory variable by mapping it onto size. Note that:\n\nI removed the size = 3 argument from geom_point() otherwise it would not map asd onto size.\n\nIn this case, it did not reveal much and was probably not worth doing.\n\n\nril_data |&gt;\n  filter(!is.na(petal_color))|&gt;\n  ggplot(aes(x = petal_area_mm, \n             y = mean_visits, \n             color  = petal_color))+\n  geom_point(aes(size = asd_mm), alpha = .4)+\n  geom_smooth(method = \"lm\",show.legend = FALSE, se = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: Returning to the motivating question, we see that visitation increases with both petal area (positive slope) and pink petals. The proportion of seeds that are hybrids appears to increase with pollinator visitation. Later in the term we will address this question more rigorously.\n\n\nSmall multiples\n\n\n\n\n\n\n\n\nFigure 1: Using small multiples to show the lunar phase moon over a month. From this link\n\n\n\n\n\n\n\nAt the heart of quantitative reasoning is a single question: Compared to what? Small multiple designs, multivariate and data bountiful, answer directly by visually enforcing comparisons of changes, of the differences among objects, of the scope of alternatives. For a wide range of problems in data presentation, small multiples are the best design solution. \n\nEdward Tufte (Tufte, 1990)\n\nEdward Tufte, a major figure in the field of data visualization - popularized the concept of “small multiples” – showing data with the same structure across various comparisons. He argued that such visualizations can help our eyes make powerful comparisons (See quote above). For example, the lunar phase can be well visualized by using small multiples (Figure 1).\nFor our analyses, we may find that adding location as a variable helps us better see and understand patterns. Additionally, small multiples are sometimes preferable to mapping different categorical variables onto different aesthetics. I show these examples below:\n\nOne explanatory “facet”Two explanatory “facets”\n\n\n\nWe can add a small multiple with a “facet”. For a single categorical variable, use the facet_wrap() function. Here are some notes and options:\n\nThe first argument is ~ &lt;Thing to facet by&gt;, e.g. ~ location.\n\nYou can specify the number of rows or columns with nrow or ncol arguments.\n\nThe labeller = \"label_both\" shows the variable name in addition to its value.\n\nOccasionally, you may want different facets on different scales. You can use the scales argument with options, free_x, free_y, and free.\n\n\nril_data |&gt;\n  filter(!is.na(petal_color))|&gt;\n  ggplot(aes(x = petal_area_mm, \n             y = prop_hybrid, \n             color  = petal_color))+\n  geom_point(size = 3, alpha = .4)+\n  facet_wrap(~ location, nrow = 1, labeller = \"label_both\")+\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\nFor two categorical variables, use the facet_grid() function. Here are some additional notes and options:\n\nThe first argument is &lt;Row to facet by&gt; ~ &lt;Column to facet by&gt;, e.g. petal_color ~ location.\n\nYou can no longer specify the number of rows or columns.\n\nI have shown the same data as the previous plot in a different way. But you can see that this adds room for mapping another variable.\n\n\nril_data |&gt;\n  filter(!is.na(petal_color))|&gt;\n  ggplot(aes(x = petal_area_mm, \n             y = prop_hybrid))+ \n  geom_point(size = 3, alpha = .4)+\n  facet_grid(petal_color ~ location, labeller = \"label_both\")+\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTufte, E. R. (1990). Envisioning information. Graphics Press.",
    "crumbs": [
      "3. Introduction to ggplot",
      "• 3. Many explanatory vars"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot/ggplot_summary.html",
    "href": "book_sections/intro_to_ggplot/ggplot_summary.html",
    "title": "• 3. ggplot summary",
    "section": "",
    "text": "Chapter Summary\nLinks to: Summary, Chatbot Tutor, Practice Questions, Glossary, R functions and R packages, and Additional resources + BONUS CONTENT: ggplotly.\nEffective data visualization begins with curiosity and clear biological questions. Before writing a single line of code, it’s worth thinking about what you want to learn from your data and how best to visualize that information. Once we have developed some starting ideas and sketches, we are ready to use ggplot2’s flexible framework to bring these ideas to life. Data visualization with ggplot2 is built around the idea that plots are constructed by layering components: you begin by mapping variables to aesthetic properties like position, color, or size, and then choose how to display those mapped variables using geometric elements like histograms, barplots, or points. With this approach you can iteratively build visualizations that reflect your questions and highlight meaningful patterns.",
    "crumbs": [
      "3. Introduction to ggplot",
      "• 3. ggplot summary"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot/ggplot_summary.html#ggplot_summaryGgploty",
    "href": "book_sections/intro_to_ggplot/ggplot_summary.html#ggplot_summaryGgploty",
    "title": "• 3. ggplot summary",
    "section": "Bonus content: Interactive plots with ggplotly",
    "text": "Bonus content: Interactive plots with ggplotly\nOften data have strange outliers, or patterns you think you see but aren’t sure about, or simply interesting data points. When I run into these issues during exploratory data analysis I often want to know more about individual data points. To do so, I make interactive graphs with the ggplotly() function in the plotly package.\nThe example below shows how to do this. Note that you can make up random aesthetics that you never use and they show up when you hover over points – this helps with understanding outliers. You can also zoom in!\n\nlibrary(plotly)\nbig_plot &lt;- ril_data |&gt;\n  filter(!is.na(petal_color))|&gt;\n  ggplot(aes(x = petal_area_mm, \n             y = prop_hybrid,\n             ril = ril,\n             mean_visits =  mean_visits))+ \n  geom_point(size = 3, alpha = .4)+\n  facet_grid(petal_color ~ location, labeller = \"label_both\")+\n  geom_smooth(method = \"lm\", se = FALSE)\n\nggplotly(big_plot)\n\n\n\n\n\n\n\n\n\nGrolemund, G., & Wickham, H. (2018). R for data science.",
    "crumbs": [
      "3. Introduction to ggplot",
      "• 3. ggplot summary"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science.html",
    "href": "book_sections/reproducible_science.html",
    "title": "4. Reproducible Science",
    "section": "",
    "text": "Making science reproducible\nThis section includes background on:\nIn my roles as a biostatistics professor and Data Editor at The American Naturalist, I have found that the greatest beneficiary of reproducible research is often the lead author themselves. In this chapter, we will work through the process of creating reproducible research—from collecting data in the field to writing and sharing R scripts that document your analyses.\nThis chapter walks you through the key steps for making your science reproducible—from field notes to final scripts. You will learn how to:\nThen we summarize the chapter, present a chatbot tutor, practice questions, a glossary, a review of R functions and R packages introduced, and provide links to additional resources.",
    "crumbs": [
      "4. Reproducible Science"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science.html#making-science-reproducible",
    "href": "book_sections/reproducible_science.html#making-science-reproducible",
    "title": "4. Reproducible Science",
    "section": "",
    "text": "Appropriately collect and store data, including Making rules for data collection, Making a spreadsheet for data entry, Making data (field) sheets, A checklist for data collection, Making a README and/or data dictionary, and Long term public data storage.\n\nDevelop reproducible computational strategies, including: Making an R project, Loading data into R, Writing and saving R scripts (with comments), Cleaning and tidying data, and finally a reproducible code checklist (modified from The American Naturalist).",
    "crumbs": [
      "4. Reproducible Science"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/collecting_data.html",
    "href": "book_sections/reproducible_science/collecting_data.html",
    "title": "• 4. Collecting data",
    "section": "",
    "text": "Making rules for data collection\nThis section includes background on: Making rules for data collection, Making a spreadsheet for data entry, Making data (field) sheets, A checklist for data collection, Making a README and/or data dictionary, and Long term public data storage.\nSometimes, data come from running a sample through a machine, resulting in a computer readout of our data (e.g., genotypes, gene expression, mass spectrometer, spectrophotometer, etc.). Other times, we collect data through counting, observing, and similar methods, and we enter our data by hand. Either way, there is likely some key data or metadata for which we are responsible (e.g., information about our sample). Below,we focus on how we collect and store our data, assuming that our study and sampling scheme have already been designed. Issues in study design and sampling will be discussed in later chapters.\nBefore we collect data, we need to decide what data to collect. Even in a well-designed study where we want to observe flower color at a field site, we must consider, “What are my options?”, for example:\nSome of these questions have answers that are more correct, while for others, it’s crucial to agree on a consistent approach and ensure each team member uses the same method.\nSimilarly, it’s important to have consistent and unique names – for example, my collaborators and I study Clarkia plants at numerous sites – include Squirrel Mountain and SawMill – which one should we call SM (there is a similar issue in state abbreviation – we live in MiNnesota (MN), not MInnesota, to differentiate our state from MIchigan (MI)). These questions highlight the need for thoughtful data planning, ensuring that every variable is measured consistently.\nOnce you have figured out what you want to measure, how you will measure and report it, and other useful info (e.g., date, time, collector), you are ready to make a data sheet for field collection, a database for analysis, and a README or data dictionary.",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Collecting data"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/collecting_data.html#making-rules-for-data-collection",
    "href": "book_sections/reproducible_science/collecting_data.html#making-rules-for-data-collection",
    "title": "• 4. Collecting data",
    "section": "",
    "text": "Should the options be “white” and “pink,” or should we distinguish between darker and lighter shades?\n\nIf we’re measuring tree diameter, at what height on the tree should we measure the diameter?\n\nWhen measuring something quantitative, what units are we reporting?",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Collecting data"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/collecting_data.html#making-a-spreadsheet-for-data-entry",
    "href": "book_sections/reproducible_science/collecting_data.html#making-a-spreadsheet-for-data-entry",
    "title": "• 4. Collecting data",
    "section": "Making a spreadsheet for data entry",
    "text": "Making a spreadsheet for data entry\nAfter deciding on consistent rules for data collection, we need a standardized format to enter the data. This is especially important when numerous people and/or teams are collecting data at different sites or years. Spreadsheets for data entry should be structured similarly to field sheets (described below) so that it is easy to enter data without needing too much thought.\nSome information (e.g., year, person, field site) might be the same for an entire field sheet, so it can either be written once at the top or explicitly added to each observation in the spreadsheet (copy-pasting can help here).",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Collecting data"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/collecting_data.html#making-data-field-sheets",
    "href": "book_sections/reproducible_science/collecting_data.html#making-data-field-sheets",
    "title": "• 4. Collecting data",
    "section": "Making Data (field) sheets",
    "text": "Making Data (field) sheets\nWhen collecting data or samples, you need a well-designed data (field) sheet. The field sheet should closely resemble your spreadsheet but with a few extra considerations for readability and usability. Ask yourself: How easy is this to read? How easy is it to fill out? Each column and row should be clearly defined so it’s obvious what goes where. Consider the physical layout too—does the sheet fit on one piece of paper? Should it be printed in landscape or portrait orientation? Print a draft to see if there’s enough space for writing. A “notes” column can be useful but should remain empty for most entries, used only for unusual or exceptional cases that might need attention during analysis.\nIt’s also smart to include metadata at the top—things like the date, location, and who collected the data. Whether this metadata gets its own column or is written at the top depends on practical needs—if one person is responsible for an entire sheet, maybe it belongs at the top, not repeated for every sample (contrary to the example in Figure 2).\n\n\n\n\n\n\n\n\nFigure 2: Example field data sheet designed for recording ecological and biological observations. The form includes sections for site information, environmental conditions, personnel details, and a structured table for recording species, individual IDs, sex, location descriptions, time, GPS coordinates, photos, collector name, and additional notes.\n\n\n\n\n\n\nData collection and data entry checklist:\n\nBe consistent and deliberate: You should refer to a thing in the same thoughtful way throughout a column. Take, for example, gender as a nominal variable. Data validation approaches, above, can help.\n\n\n\nA bad organization would be: male, female, Male, M, Non-binary, Female.\nA good organization would be: male, female, male, male, non-binary, female.\nA just as good organization would be: Male, Female, Male, Male, Non-binary, Female.\n\nUse good names for things: Names should be concise and descriptive. They need not tell the entire story. For example, units are better kept in a data dictionary (Figure 4) than column name. This makes downstream analyses easier. Avoiding spaces and special characters makes column names easier to work with in R.\n\n\n\n\n\n\n\n\n\n\nFigure 3: A comparison of bad and good variable naming conventions in a dataset. The top section (labeled “Bad” in red) displays poor naming choices, including spaces, special characters, unclear abbreviations, and inconsistent formatting. The bottom section (labeled “Good” in blue) demonstrates improved naming conventions with consistent formatting, clear descriptions, and no special characters.\n\n\n\n\n\nSave in a good place. Make a folder for your project. Keep your data sheet (and data dictionary) there. Try to keep all that you need for the project in this folder, but try not to let it get too complex. Some people like to add more order with additional subfolders for larger projects.  \nSave early and often: You are welcome.  \nBackup your data, do not touch the original data file and do not perform calculations on it. In addition to scans of your data (if collected on paper) also save your data on both your computer and a locked, un-editable location on the cloud (e.g. google docs, dropbox, etc.). When you want to do something to your data do it in R, and keep the code that did it. This will ensure that you know every step of data manipulation, QC etc.  \nDo Not Use Font Color or Highlighting as Data. You may be tempted to encode information with bolded text, highlighting, or text color. Don’t do this! Be sure that all information is in a readable column. These extra markings will either be lost or add an extra degree of difficulty to your analysis. Reserve such markings for the presentation of data. \nNo values should be implied Never leave an entry blank as “shorthand for same as above”. Similarly, never denote the first replicate (or treatment, or whatever) by its order in a spreadsheet, but rather make this explicit with a value in a column. Data order could get jumbled and besides it would take quite a bit of effort to go from implied order to a statistical analysis. It is ok to skip this while taking data in the field, but make sure no values are implied when entering data into a computer. \nData should be tidy (aka rectangular): Ideally data should be entered as tidy (Each variable must have its own column. Each observation must have its own row. Each value must have its own cell). However, you must balance two practice considerations – “What is the best way to collect and enter data?” vs “What is the easiest way to analyze data?” So consider the future computational pain of tidying untidy data when ultimately deciding on the best way to format your spreadsheet (above).  \nUse data validation approaches: When making a spreadsheet, think about future-you (or your collaborator) who will analyze the data. Typos and inconsistencies in values (e.g., “Male,” “male,” and “M” for “male”) create unnecessary headaches. Accidentally inputting the wrong measure (e.g., putting height where weight should be, or reporting values in kg rather than lb) sucks.  Both Excel and Google Sheets (and likely most spreadsheet programs) have a simple solution: use the “Data validation” feature in the “Data” drop-down menu to limit the potential categorical values or set a range for continuous values that users can enter into your spreadsheet. This helps ensure the data are correct.\n\n\n\nMaking a README and/or “data dictionary”\nA data dictionary is a separate file (or sheet within a file, if you prefer) that explains all the variables in your dataset. It should include the exact variable names from the data file, a version of the variable name that you might use in visualizations, and a longer explanation of what each variable means. Additionally, it is important to list the measurement units and the expected minimum and maximum values for each variable, so anyone using the data knows what to expect.\nAlongside your data dictionary, you should also create a README file that provides an overview of the project and dataset, explaining the purpose of the study and how the data were collected. The README should include a description of each row and column in the dataset. While there may be some overlap with the data dictionary, this is fine. The data dictionary can serve as a quick reference for plotting and performing quality control, whereas the README provides a higher-level summary, designed for those who may not have been directly involved in the project but are seriously interested in the data.\n\n\n\n\n\n\n\n\nFigure 4: Example data dictionary from (Broman & Woo, 2018). This table provides structured metadata for a dataset, including variable names, descriptions, and group classifications. It ensures clarity in data documentation by defining each column’s purpose and expected values. See the Clarkia RIL data dictionary here for another example.\n\n\n\n\n\n\n\nLong-term public data storage\nA key to the scientific method is reproducibility. This is why scientific papers have a methods section. Nowadays - the internet allows for even more detailed sharing of methods. Additionally it is the expectation in most fields that data is made available after publication on repositories like data DRYAD or DRUM.\n\n\n\n\nBabbage, C. (1864). Passages from the life of a philosopher. Longman; Co.\n\n\nBroman, K. W., & Woo, K. H. (2018). Data organization in spreadsheets. The American Statistician, 72(1), 2–10. https://doi.org/10.1080/00031305.2017.1375989",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Collecting data"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/reproducible_analyses.html",
    "href": "book_sections/reproducible_science/reproducible_analyses.html",
    "title": "• 4. Reproducible analyses",
    "section": "",
    "text": "R Projects, Storing, and Loading data\nThis section includes background on: Making an R project, Loading data into R, Writing and saving R scripts (with comments), Cleaning and tidying data, and finally a reproducible code checklist (modified from The American Naturalist).\nIn addition to the high-minded values of transparency and sharing scientific progress, there is a pragmatic and selfish reason to make you work reproducible.\nThis humorous observation highlights a very real challenge. Research is complex and can stretch over months or even years. Well-documented data and code, allows you to retrace your steps, understand your past analyses, and explain your results. By keeping a good record of your workflows, you make your future self’s life much easier.\nFigure 1: A screenshot of the files pane of RStudio. This displayes the folder containing my RProject, my data, and data dictionary. If you do this, there is no need to use the setwd() function or to use a long, “absolute path” to the data.\nI have previously introduced an easy way to load data from the internet into R, but often data is on our computer, not the internet. Loading data from your computer to R can be a difficult challenge, but if you follow the guide below you will see that it can be very easy.",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Reproducible analyses"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/reproducible_analyses.html#r-projects-storing-and-loading-data",
    "href": "book_sections/reproducible_science/reproducible_analyses.html#r-projects-storing-and-loading-data",
    "title": "• 4. Reproducible analyses",
    "section": "",
    "text": "Making an R project\nThe first step towards reproducible analyses is making an R project. I do this by clicking “File” &gt; “New Project” and I usually use an existing directory (Navigating to the folder with my data and data dictionary that we just made).\n\nNow every time you use R to work on these data, open R by double clicking on this project (or if R is already open, navigate to “Open Project”).\n\n\n\nLoading data\nNow if your project and data are in the same folder, loading the data is super easy. Again just used the read_csv() function in the readr package with an argument of the filename (in quotes) to read in the data!\n\n\nSome people like a different structure of files in their R project – they like the project in the main folder and then a bunch of sub-folders like data and scripts and figures. If you are using this organization, just add the subfolder in the path (e.g. read_csv(\"data/my_data.csv\") ).\n\nlibrary(readr)\nril_data &lt;- read_csv(\"clarkia_rils.csv\")\n\n\nRead excel files into R with the read_xlsx() from the readxl package). You can even specify the sheet as an argument!",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Reproducible analyses"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/reproducible_analyses.html#r-scripts-commenting-code-cleaning-tidying-data",
    "href": "book_sections/reproducible_science/reproducible_analyses.html#r-scripts-commenting-code-cleaning-tidying-data",
    "title": "• 4. Reproducible analyses",
    "section": "R Scripts, Commenting Code, + Cleaning & Tidying data",
    "text": "R Scripts, Commenting Code, + Cleaning & Tidying data\nWe have just run a few lines of code – loading the readr library and assigning the data in clarkia_rils.csv to the variable, ril_data.\nWe will likely want to remember that we have done this, and build off it in the future. To do so let’s open up an R Script – by navigating to File, New File, R Script.\nA new file, likely called Untitled 1 will show up in the R Scripts pane of your R Studio session. After giving it a more descriptive name, be sure save all of the code you are using for these analyses in this file.\nHere I introduce the best practices for writing an R script (Part I). I revisit these in greater depth later in the chapter.\n\nFirst include your name, and the goal of the script as comments on the opening lines.\n\nNext load all packages you will use. Don’t worry, you can go back and add more later.\n\nDo not include install.packages() in your script: You only want to do this once, so it should not be part of a reproducible pipeline. People can see they need to install the package from you loading it.\n\n\nNext load all the data you will use.\n\nThen get started coding.\n\nBe sure to comment your code with the #.\n\n\nCleaning data\nWe previously said not to make column names too long, yet in our dataset, we have the variable petal_area_mm. We can use the rename() function in the dplyr package to give columns better names.\nI do this below in my complete and well-commented R script, which I conclude with a small plot. Notice that anyone could use this script and get exactly what I got.\n\n\n\n\n\n\n\n\n\n\n# Yaniv Brandvain. March 07 2025\n# Code to load RIL data, and make a histogram of petal area facetted by petal color\n\nlibrary(readr)                                      # Load the readr package to load our data\nlibrary(ggplot2)                                    # Load the ggplot2 package to make graphs\n\nril_data &lt;- read_csv(\"clarkia_rils.csv\") |&gt;         # load the RIL data\n    filter(!is.na(petal_color))          |&gt;         # remove missing petal color data\n  rename(petal_area = petal_area_mm)                # rename petal_area_mm as petal_area\n\nggplot(ril_data, aes(x = petal_area))+              # set up a plot with petal_area on x\n  geom_histogram(binwidth = 10, color = \"white\")+   # make it a histogram, with a bins of width 10 and separated by white lines \n  facet_wrap(~petal_color, ncol = 1)                # wrap by petal color, with one column\n\n\nIf your data set has many column names that are difficult to use in R, the clean_names() function in the janitor package fixes a bunch of them at once.",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Reproducible analyses"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/reproducible_analyses.html#tidying-data",
    "href": "book_sections/reproducible_science/reproducible_analyses.html#tidying-data",
    "title": "• 4. Reproducible analyses",
    "section": "Tidying data",
    "text": "Tidying data\nWe have previously seen that there are many ways for data to be untidy. One common untidy data format is the “wide format”, below. Here, the columns SR and GC describe the proportion of hybrid seed on each ril (row) observed in each of these two locations. You can use the pivot_longer() function in the tidyr package, to tidy such data:\n\n\nCode to create untidy_ril\nlibrary(tidyr)\nlibrary(dplyr)\nuntidy_ril &lt;- ril_data |&gt; \n  select(ril, location,prop_hybrid)|&gt;\n  filter(location %in% c(\"GC\",\"SR\"))|&gt; \n  pivot_wider(id_cols = \"ril\",\n              names_from = location,\n              values_from = prop_hybrid)\n\nuntidy_ril|&gt;head(n = 5)|&gt; kable()\n\n\n\n\n\nril\nGC\nSR\n\n\n\n\nA1\n0.000\n0.00\n\n\nA100\n0.125\n0.25\n\n\nA102\n0.250\n0.00\n\n\nA104\n0.000\n0.00\n\n\nA106\n0.000\n0.00\n\n\n\n\n\n\nlibrary(tidyr)\npivot_longer(untidy_ril, \n             cols = c(\"GC\",\"SR\"),\n             names_to = \"location\", \n             values_to = \"prop_hybrid\")|&gt;\n  head(n = 10)|&gt;\n  kable()\n\n\n\n\nril\nlocation\nprop_hybrid\n\n\n\n\nA1\nGC\n0.000\n\n\nA1\nSR\n0.000\n\n\nA100\nGC\n0.125\n\n\nA100\nSR\n0.250\n\n\nA102\nGC\n0.250\n\n\nA102\nSR\n0.000\n\n\nA104\nGC\n0.000\n\n\nA104\nSR\n0.000\n\n\nA106\nGC\n0.000\n\n\nA106\nSR\n0.000",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Reproducible analyses"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/reproducible_analyses.html#reproducible-code-a-checklist-from-amnat",
    "href": "book_sections/reproducible_science/reproducible_analyses.html#reproducible-code-a-checklist-from-amnat",
    "title": "• 4. Reproducible analyses",
    "section": "Reproducible Code: A Checklist From AmNat",
    "text": "Reproducible Code: A Checklist From AmNat\nWhat you do to data and how you analyze it is as much a part of science as how you collect it. As such, it is essential to make sure your code:\n\nReliably works – even on other computers\n\nAnd can be understood.\n\nThe principles from The American Naturalist’s policy about this are pasted in the box below:\n\nREQUIRED:\n\nScripts should start by loading required packages, then importing raw data from files archived in your data repository.\nUse relative paths to files and folders (e.g. avoid setwd() with an absolute path in R), so other users can replicate your data input steps on their own computers.\nMake sure your code works. Shut down your R. (or type rm(list=ls()) into the console and run the code again. You should get the same results. If not, go back and fix your mistakes.\nAnnotate your code with comments indicating what the purpose of each set of commands is (i.e., “why?”). If the functioning of the code (i.e., “how”) is unclear, strongly consider re-writing it to be clearer/simpler. In-line comments can provide specific details about a particular command.\n\nNote that ChatGPT is very good at commenting your code.\n\nAnnotate code to indicate how commands correspond to figure numbers, table numbers, or subheadings of results within the manuscript.\nIf you are adapting other researcher’s published code for your own purposes, acknowledge and cite the sources you are using. Likewise, cite the authors of packages that you use in your published article.\n\nRECOMMENDED:\n\nTest code ideally on a pristine machine without any packages installed, but at least using a new session.\nUse informative names for input files, variables, and functions (and describe them in the README file).\nAny data manipulations (merging, sorting, transforming, filtering) should be done in your script, for fully transparent documentation of any changes to the data.\nOrganize your code by splitting it into logical sections, such as importing and cleaning data, transformations, analysis and graphics and tables. Sections can be separate script files run in order (as explained in your README) or blocks of code within one script that are separated by clear breaks (e.g., comment lines, #————–), or a series of function calls (which can facilitate reuse of code).\nLabel code sections with headers that match the figure number, table number, or text subheading of the paper.\nOmit extraneous code not used for generating the results of your publication, or place any such code in a Coda at the end of your script.",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Reproducible analyses"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/reproducible_summary.html",
    "href": "book_sections/reproducible_science/reproducible_summary.html",
    "title": "• 4. Reproducibility summary",
    "section": "",
    "text": "Chapter Summary\nLinks to: Summary, Chatbot Tutor, Practice Questions, Glossary, R functions and R packages, and Additional resources.\nReproducibility is a cornerstone of good science, ensuring that research is transparent, reliable, and easy to build upon. This chapter covered best practices for collecting, organizing, and analyzing data in a reproducible manner.\nBefore collecting data, establish clear rules for measurement, naming conventions, and data entry to maintain consistency. Field sheets should be well-structured, and tidy. Additionally, creating a data dictionary and README document ensures that variables and project details are well-documented. Finally, storing data and scripts in public repositories supports transparency and open science.\nIn analysis, using an R Project helps keep files organized, and loading data with relative paths avoids location issues. Writing well-structured R scripts with clear comments makes workflows understandable and repeatable. By prioritizing reproducibility you not only strengthen the integrity of your work, but also make future analyses smoother for yourself and others.",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Reproducibility summary"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/reproducible_summary.html#reproducible_summarySummary",
    "href": "book_sections/reproducible_science/reproducible_summary.html#reproducible_summarySummary",
    "title": "• 4. Reproducibility summary",
    "section": "",
    "text": "Chatbot tutor\n\nPlease interact with this custom chatbot (link here) I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you got what you needed from it.\n\n\n\nPractice Questions\nTry these questions!\n\nQ1) What is the biggest mistake in the table below?\n\n ID should be lower case Its perfect, change nothing the column name, weight is not sufficiently descriptive, it should include the units. date_colleted_empty_means_same_as_above is too wordy, replace with date Values for date_collected_empty_means_same_as_above are implied. date is in Year-Month-Day format, while Month-Day-Year format is preffered.\n\n\n\n\n\n\nID\nweight\ndate_collected_empty_means_same_as_above\n\n\n\n\n1-A1\n104\n2024-03-01\n\n\n1-1B\n210\n\n\n\n3-7\n150\n\n\n\n2-B\n176\n2024-03-15\n\n\n1-A5\n110\n\n\n\n\n\n\n\n\nClick here for explanation\n\nWhile some of these (like the long name for date) are clearly shortcomings, spreadsheets should never leave values implied.\n\n.\n\nQ2) What would you expect in a data dictionary accompanying the table above? (select all correct)\n\n The units for weight. A statement that date is in Year-Month-Day format A statement explaining that in the date colleted column, empty means same as above.\n\n\nQ3) How do you read data from a Excel sheet, called raw_data in an Excel filed named bird_data.xlsx located inside the R project you are working in?\n\n You cannot load excel files into R. You must save it as a csv, and read it in with read_csv(). Assuming the readxl package is installed and loaded, type read_xlsx(file = “bird_data.xlsx”, sheet = “raw_data”). While you can read excel into R, you cannot specify the sheet.\n\n\nQ4) What should you do to make code reproducible? (pick the best answer)\n\n Specify the working directory with setwd() Show the packages installed with install.packages() Restart R once your done, and rerun your script to see if it works\n\n\n\n\n\nGlossary of Terms\n\nAbsolute Path – A file location specified from the root directory (e.g., /Users/username/Documents/data.csv), which can cause issues when sharing code across different computers. Using relative paths instead is recommended.\nData Dictionary – A structured document that defines each variable in a dataset, including its name, description, units, and expected values. It helps ensure data clarity and consistency.\nData Validation – A method for reducing errors in data entry by restricting input values (e.g., dropdown lists for categorical variables, ranges for numerical values).\nField Sheet – A structured data collection form used in the field or lab, designed for clarity and ease of data entry.\nMetadata – Additional information describing a dataset, such as when, where, and how data were collected, the units of measurement, and details about the variables.\nR Project – A self-contained environment in RStudio that organizes files, code, and data in a structured way, making analysis more reproducible.\nRaw Data – The original, unmodified data collected from an experiment or survey. It should always be preserved in its original form, with any modifications performed in separate scripts.\nREADME File – A text file that provides an overview of a dataset, including project details, data sources, file descriptions, and instructions for use.\nReproducibility – The ability to re-run an analysis and obtain the same results using the same data and code. This requires careful documentation, structured data storage, and clear coding practices.\nRelative Path – A file path that specifies a location relative to the current working directory (e.g., data/my_file.csv), making it easier to share and reproduce analyses.\nTidy Data – A dataset format where each variable has its own column, each observation has its own row, and each value is in its own cell.\n\n\n\n\nKey R functions\n\n\nclean_names(data) – Standardizes column names (from the janitor package).\ndrop_na(data) – Removes rows with missing values (from the tidyr package)).\nread_csv(\"file.csv\") – Reads a CSV file into R as a tibble (from the readr package).\nread_xlsx(\"file.xlsx\", sheet = \"sheetname\") – Reads an excel sheet into R as a tibble (from the readxl package).\nrename(data, new_name = old_name) – Renames columns in a dataset (from the dplyr package).\npivot_longer(data, cols, names_to, values_to) – Converts wide-format data to long format (from the tidyr package).\nsessionInfo() – Displays session details, including loaded packages (useful for reproducibility).\n\n\n\n\n\nR Packages Introduced\n\n\nreadr – Provides fast and flexible functions for reading tabular data (here we revisited read_csv() for CSV files).\ndplyr – A grammar for data manipulation. Here we introduced the rename(data, new_name = old_name) function to give columns better names.\ntidyr – Helps tidy messy data. Here we introduced pivot_longer() to make wide data long.\njanitor – Cleans and standardizes data, including clean_names()](https://sfirke.github.io/janitor/reference/clean_names.html) for formatting column names.\n\n\n\n\nAdditional resources\n\nR Recipes:\n\nRead a .csv: Learn how to read a csv into R as a tibble.\n\nRead an Excel file: Learn how to read an excel file into R as a tibble.\n\nObey R’s naming rules: You want to give a valid name to an object in R.\n\nRename columns in a table: You want to rename one or more columns in a data frame.\n\nOther web resources:\n\nData Organization in Spreadsheets (Broman & Woo, 2018).\nTidy Data: (Wickham, 2014).\nTen Simple Rules for Reproducible Computational Research: (Sandve, 2013).\nNYT article: For big data scientists hurdle to insights is janitor work.\nStyle guide: Chapter 9 of Data management in large-scale education research by Lewis (2024). Includes sections on general good practices, file naming, and variable naming.\nData Storage and security: Chapter 13 of Data management in large-scale education research by Lewis (2024).\n\nVideos:\n\nData integrity: (By Kate Laskowski who was the victim of data fabrication by her collaborator (and my former roommate) Jonathan Pruitt).\nTidying data with pivor_longer (From Stat454)\n\n\n\n\n\n\nBroman, K. W., & Woo, K. H. (2018). Data organization in spreadsheets. The American Statistician, 72(1), 2–10. https://doi.org/10.1080/00031305.2017.1375989\n\n\nLewis, C. (2024). Data management in large-scale education research. CRC Press.\n\n\nSandve, A. A. T., Geir Kjetil AND Nekrutenko. (2013). Ten simple rules for reproducible computational research. PLOS Computational Biology, 9(10), 1–4. https://doi.org/10.1371/journal.pcbi.1003285\n\n\nWickham, H. (2014). Tidy data. Journal of Statistical Software, Articles, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Reproducibility summary"
    ]
  },
  {
    "objectID": "book_sections/summarizing_data_index.html",
    "href": "book_sections/summarizing_data_index.html",
    "title": "Section II: Summarizing data",
    "section": "",
    "text": "The major goals of statistics are to: (1) Summarize data, (2) Estimate uncertainty, (3) Test hypotheses, (4) Infer cause, and (5) Build models. Now that we can do things in R, we are ready to begin our journey through these goals. In this section, we focus on the first goal—Summarizing data.\n\n\nIt is somewhat weird to start with summarizing data without also describing uncertainty because, in the real world, data summaries should always be accompanied by some estimate of uncertainty. However, biting off both of these challenges at once is too much, so for now, as we move forward in summarizing data, remember that this is just the beginning and is inadequate on its own.\nEven though we aren’t tackling uncertainty yet, summarizing data on its own is already incredibly useful. Understanding and interpreting summaries helps us find patterns, spot errors, and build towards deeper statistical analysis.\nWhy summarize data?\nSummarizing data serves several purposes:\n\nCondensing large datasets: Raw data often contains thousands of observations—summaries make the patterns manageable.\n\nIdentifying trends and variability: Are the values clustered? Is there a lot of spread?\n\nDetecting errors: Unexpected values often signal data entry issues or outliers.\n\nComparing groups: Does one group tend to have larger values than another?\n\nLaying the foundation for modeling: Before building models, we need a clear picture of the data’s structure.\n\n\n\n\n\n\n\n\n\nFigure 1: A pretty scene of Clarkia’s home showing the world we get to summarize.\n\n\n\n\n\nIn this section, we’ll not only learn how to compute summaries but also how to think about them in a meaningful way. That means:\n\nGet everyone up to speed with standard data summaries:\n\nChapter Five introduces univariate summaries of Shape, and how to change the shape of our data! Central tendency (e.g. mean, median, mode), and Variability (e.g. sum of squares, standard deviation, and variance), and more!\nChapter 6 summarizes associations between variables. This includes: conditional means, conditional proportions, covariance and correlation. Students will also get a sense of the difference between parametric and non-parametric summaries. For each summary, we aim to (1) build intuition, (2) provide the mathematical nuts and bolts, and (3) show you how to find it in R.\nIn Chapter 7, we build from these simple summaries to linear models that allow us to predict the value of a response variable from a (combination) of explanatory variable(s). Later in the book we will dig much more deeply into linear models.\n\nIntroduce multivariate summaries and dimension reduction techniques: Summarizing data in one or two dimensions is relatively straightforward. However, nowadays, it is pretty standard to collect high-dimensional data, and we want to be able to work with such complex data. Chapter 8 introduces a few techniques to tackle this real-world challenge.\nFurther your data visualization and interpretation skills: Visual summaries of data are perhaps the most valuable summaries because, when done well, they tell a clear and honest story of the data. We introduced ggplot previously, but that was the minimum to get up and running. Chapter 9 shows what goes into making a good plot, and Chapter 10 shows how to make good plots with ggplot!",
    "crumbs": [
      "Section II: Summarizing data"
    ]
  },
  {
    "objectID": "book_sections/univariate_summaries.html",
    "href": "book_sections/univariate_summaries.html",
    "title": "5. Simple Summaries",
    "section": "",
    "text": "Histograms\nWe can similarly display discrete numerical variables as a bar plot. So, for example, the x-axis of Figure 2 (in the Num hybrid bar plot tab) shows the number of a mother’s seeds that we found to be hybrids, and the y-axis shows the number of mothers with x seeds shown to be hybrid.\nUnfortunately, Figure 2 is somewhat misleading – we aimed to genotype eight seeds per plant, but we sometimes missed this goal. Figure 3 (in the Prop hybrid bar plot tab) is more honest, as it shows the proportion, but it is a bit distracting and confusing – there are occasional weird dips which represent – not a biological fact of parviflora hybridization, but experimental weirdness of a non-standard sample size across mothers.\nFigure 4 (in the Prop hybrid hist tab) is a histogram that displays the distribution of the proportion of hybrid seeds per RIL. A histogram is much like a bar plot, but rather than referring to a single value of the x-variable, in a histogram, values of x are binned.\nIt helps to remember that in contrast to a bar plot, where the value on the x axis is the value of all the observations in that bar, in a histogram this value is the center of the range of values in the bin. So in Figure 4 the first bin corresponds to values between -0.0625 and 0.0625, and the second bin (centered on 1/8th) corresponds to proportions between 0.0625 and 0.1875, etc… . The x-axis of Figure 5 shows this more explicitly, but it is too busy, and complex for standard presentation.\nFigure 5: A histogram showing the proportion of genotyped plants shown to be hybrid. Most of the time, we genotyped eight seeds per mom. This is identical to Figure Figure 4, except, the x-label shows the range of x values rather than the center of thebins.",
    "crumbs": [
      "5. Simple Summaries"
    ]
  },
  {
    "objectID": "book_sections/univariate_summaries.html#histograms",
    "href": "book_sections/univariate_summaries.html#histograms",
    "title": "5. Simple Summaries",
    "section": "",
    "text": "Num hybrid bar plotProp hybrid bar plotProp hybrid hist\n\n\n\n\ngc_rils |&gt; \n  ggplot(aes(x = num_hybrid))+\n  geom_bar()+\n  scale_x_continuous(breaks = 0:8)\n\n\n\n\n\n\n\nFigure 2: A bar plot showing the number of genotyped seeds of each mom shown to be hybrid.\n\n\n\n\n\n\n\n\n\n\ngc_rils |&gt; \n  ggplot(aes(x = prop_hybrid))+\n  geom_bar()\n\n\n\n\n\n\n\nFigure 3: A bar plot showing the proportion of genotyped plants shown to be hybrid. Most of the time, we genotyped eight seeds per mom.\n\n\n\n\n\n\n\n\n\n\ngc_rils |&gt; \n  ggplot(aes(x = prop_hybrid))+\n  geom_histogram(binwidth = 1/8, color = \"white\")+\n  scale_x_continuous(breaks= seq(0,1,1/8))\n\n\n\n\n\n\n\nFigure 4: A histogram showing the proportion of genotyped plants shown to be hybrid. Most of the time, we genotyped eight seeds per mom.\n\n\n\n\n\n\n\n\n\n\n\n\nWhen making a histogram, you are in charge of the bin size. In ggplot2 you can specify this with one of two arguments:\n\nbinwidth: Which tells R how big to make the the bins.\n\nbins: Which tells R how many equally sized bins to split the data into.\n\nThere is not a universally correct answer for the appropriate bin size – it will vary by data set. It takes thought and expertise. It is best to experiment some until you find the binning that honestly reflects the variability in your data. This reflects a trade-off between capturing the variability, without distracting the reader with every bump or dip. I usually start with about thirty bins and then dial that number up or down until I feel like the variability in the data is well-communicated by the plot.",
    "crumbs": [
      "5. Simple Summaries"
    ]
  },
  {
    "objectID": "book_sections/univariate_summaries.html#lets-get-started-with-univariate-summaries",
    "href": "book_sections/univariate_summaries.html#lets-get-started-with-univariate-summaries",
    "title": "5. Simple Summaries",
    "section": "Let’s get started with univariate summaries!",
    "text": "Let’s get started with univariate summaries!\nThe following sections introduce how to summarize a single variable by:\n\nDescribing its shape including its skew, and number of modes.\n\nChanging its shape (if necessary), including when to transform, rules of transformation, how to transform variables in R, and a table of common transformations.\n\nDescribing its center, including standard non-parametric and parametric summaries, when to use which, and some useful but used-less summaries.\n\nDescribing its width including nonparametric and parametric summaries, with a worked example with mathematical calculations, R functions, and an illustrative plot or visualization.\n\nThen we summarize the chapter, present practice questions, a glossary, a review of R functions and R packages introduced, and present additional resources.",
    "crumbs": [
      "5. Simple Summaries"
    ]
  },
  {
    "objectID": "book_sections/univariate_summaries/summarizing_shape.html",
    "href": "book_sections/univariate_summaries/summarizing_shape.html",
    "title": "• 5. Summarizing shape",
    "section": "",
    "text": "Code for selecting data from a few columns from RILs planted at GC\nril_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_rils.csv\"\nril_data &lt;- readr::read_csv(ril_link) |&gt;\n  dplyr::mutate(growth_rate = case_when(growth_rate ==\"1.8O\" ~ \"1.80\",\n                                          .default = growth_rate),  \n                growth_rate = as.numeric(growth_rate),\n                visited = mean_visits &gt; 0)\ngc_rils &lt;- ril_data |&gt;\n  filter(location == \"GC\", !is.na(prop_hybrid), ! is.na(mean_visits))|&gt;\n  select(petal_color, petal_area_mm, num_hybrid, offspring_genotyped, prop_hybrid, mean_visits , asd_mm )\n\n\n\n\nMotivating Scenario:\nYou have a fresh new dataset and want to check it out. Before providing numeric summaries you want to poke around the data so you are prepared to responsibly present appropriate summaries.\nLearning Goals: By the end of this subchapter, you should be able to:\n\nIdentify the “skew” of data: You should be able to distinguish between:\n\nRight skewed data – Most values small, some are very large.\nLeft skewed data – Most values large, some are very small.\nSymmetric data – There are roughly as many small as large values.\n\nIdentify the number of modes in a data set. Differentiate between one, two and more modes.\nExplain why we must consider the shape of data when summarizing it.\n\n\n\n\nWe start this chapter with bar plots and histograms instead of numerical summaries like the mean or variance because we must understand the shape of our data to properly present and interpret classic numeric summaries.\n\n\n\n\n\n\n\n\n\nFigure 1: Measuring Clarkia xantiana flowers. Image from CalPhotos shared by Chris Winchell with a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 (CC BY-NC-SA 3.0) license.\n\n\n\n\n\n\n\nSkew\nOne key aspect of a dataset’s shape is skewness—whether the data is symmetrical or leans heavily toward smaller or larger values.\n\nThe proportion of hybrid seeds per RIL mom (Figure 4, and shown again in Figure 2 A) is strongly right-skewed—most values are small, but some are very large. Right-skewed data is common in biology and everyday life. For example, income is right-skewed: most people earn relatively little money, but some make loads of ca$h.  \nThe number of offspring we genotyped (Figure 2 B) is strongly left-skewed—most values are large, but some are very small. Left-skewed data is also common in real-world settings. For example, age at death follows a left-skewed distribution: most people live long lives (often between 70 and 90), but some individuals die at much younger ages due to childhood mortality, diseases, accidents, or suicide.  \nThe distributions in Figure 2 A and B represent extreme cases of skewness, where most values are either very small or very large. In contrast, Figure 2 C shows that on a \\(\\text{log}_{10}\\) scale, petal area is nearly symmetric, with an even distribution of large and small values and most observations concentrated in the middle. \n\n\n\n\n\n\n\n\n\nFigure 2: A bar plot showing the number of genotyped seeds of each mom shown to be hybrid.\n\n\n\n\n\n\nData transformations and skew: Figure 2 showed that \\(\\text{log}_{10}\\) petal area was roughly symmetric. The careful might be suspicious and wonder why I transformed the data. The answer is that area is usually right skewed and log transforming often removes such skew.\nWe discuss changing the shape of distributions by transformation in the next section. Later we will see that symmetric data are often easier for statistical models than skewed data so such transformations are common.\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of modes\nOne of the first things to notice when visualizing a dataset is whether the values cluster around a single peak or multiple peaks—this number of modes can reveal important patterns, such as distinct subgroups or natural variation in biological traits.\n\n\n\n\n\n\n\n\n\nFigure 3: The distribution of petal area across four xantiana / parviflora hybrid zones. Data are available here.\n\n\n\n\n\nIn unimodal distributions, there is a single, clear peak, with the number of observations in other bins decreasing as we move away from this central point. All distributions in Figure 2 are unimodal.\nIn bimodal distributions, there are two distinct peaks separated by a trough. Bimodal distributions are particularly interesting because they suggest that the dataset is composed of two distinct groups or categories.\nOf course, distributions can have more than two modes. Trimodal distributions have three peaks, and so on. However, be careful not to over-interpret small fluctuations—small dips can create false peaks from random variation. It’s worth experimenting with bin sze to avoid overinterpreting such small blips.\n\nThe number of modes is particularly important in the study of speciation, especially in populations that may be hybridizing.\n\nA unimodal hybrid zone suggests that two species merge when they come back into contact, implying they may not be distinct, stable species.\n\nBimodal or trimodal hybrid zones suggest that the two species largely maintain their distinctiveness when they have the opportunity to hybridize.\n\nIn a bimodal hybrid zone, the “trough” between peaks may include some hybrids.\nIn a trimodal hybrid zone, the middle peak might represent F1 hybrids.\n\n\nWe were particularly interested in examining the distribution of phenotypes in seeds collected from parviflora / xantiana hybrid zones. Figure 4 shows that—unlike petal area in our RILs-most phenotypes from natural hybrid zones are largely bimodal. However, Figure 3 suggests that these distributions may themselves be a blend of different underlying distributions. While petal area appears bimodal in most populations, it may be unimodal at site S6. Figure 4 and Figure 3 highlight the benefit of digging into the data visually. We must visualize the distribution o values of a variable before we can provide a meaningful and interpretable summary statistic. .\n\n\n\n\n\n\n\n\nFigure 4: Distributions of three floral traits in Clarkia xantiana hybrid zones: log10-transformed average petal area (sq. cm) (left), average protandry (middle), and average herkogamy (right). The petal area distribution is bimodal, with two distinct peaks. In contrast, both protandry and herkogamy are bimodal and strongly right-skewed, with most values clustered near zero and a secondary peak at higher values. These distributions suggest potential underlying biological structure, such as genetic variation or environmental influences shaping floral trait expression. Data are available here.",
    "crumbs": [
      "5. Simple Summaries",
      "• 5. Summarizing shape"
    ]
  },
  {
    "objectID": "book_sections/univariate_summaries/changing_shape.html",
    "href": "book_sections/univariate_summaries/changing_shape.html",
    "title": "• 5. Changing shape",
    "section": "",
    "text": "Code for selecting data from a few columns from RILs planted at GC\nlibrary(tweetrmd)\nlibrary(knitr)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(stringr)\nlibrary(DT)\nlibrary(webexercises)\nlibrary(ggplot2)\nlibrary(tidyr)\nsource(\"../../_common.R\") \nril_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_rils.csv\"\nril_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_rils.csv\"\nril_data &lt;- readr::read_csv(ril_link) |&gt;\n  dplyr::mutate(growth_rate = case_when(growth_rate ==\"1.8O\" ~ \"1.80\",\n                                          .default = growth_rate),  \n                growth_rate = as.numeric(growth_rate),\n                visited = mean_visits &gt; 0)\ngc_rils &lt;- ril_data |&gt;\n  filter(location == \"GC\", !is.na(prop_hybrid), ! is.na(mean_visits))|&gt;\n  select(petal_color, petal_area_mm, num_hybrid, offspring_genotyped, prop_hybrid, mean_visits , asd_mm )\n\n\n\n\nMotivating Scenario:\nYour data are not symmetric, you are wondering about potentially changing the shape of your data to make it easier to deal with. Here I introduce some thoughts and guidance about common data transformations and when to use them.\nLearning Goals: By the end of this subchapter, you should be able to:\n\nUnderstand what a transformation is and know when they are (or are not) a good idea, and how they connect to the process that generated your data.\nKnow the rules for a legit transformation and be aware of common “gotchas” which accidentally break these rules.\nRecognize which transformations are appropriate for different skews of data.\n\nRight skewed data – Try log, square root, or reciprocal transformations.\nLeft skewed data – Try exponential or square transformation.\n\n\n\n\n\nStatistical approaches should follow biology\n\n\n\n\n\n\n\n\n\nFigure 1: Different biological traits follow different natural distributions. Panel A shows the distribution of stem diameter (mm) in a recombinant inbred line (Clarkia RIL) dataset, which is approximately symmetric and normally distributed. In contrast, panel B displays the distribution of petal area (mm²), which exhibits a right-skewed, exponential-like distribution.\n\n\n\n\nThere is nothing inherently “natural” about the linear scale — in fact, some research (review by Asmuth et al. (2018)) suggests that humans naturally think in log scale, and only begin thinking in linear scale with more formal schooling. That is, kids tend to think that the difference between seventy-five and eighty is smaller than the difference between five and ten. In a sense, they’re right — ten is double five, while eighty is only \\(\\frac{16}{15}\\) of seventy-five. Of course, in another sense, they’re wrong — the difference is five in both cases.\nIt turns out that different variables naturally vary on different scales. For example, growth rate, area, volume, and other such processes often generate right-skewed data (growth is exponential — at least initially — area scales with the square of length, volume scales with the cube, etc.). It’s therefore not surprising that Clarkia stem diameter in the RILs has a fairly symmetric distribution, while petal area is right-skewed (Figure 1).\nBy contrast, variables that exhibit diminishing returns or are constrained by hard limits often result in left-skewed distributions (as we saw in our genotyping efforts, where we aimed to genotype eight seeds per mom — and no more — but sometimes ended up with fewer).\n\n\n\nTo transform or not to transform? That is the question\nWhen the underlying biological process results in nonlinear data with asymmetric distributions, transforming the data is often appropriate — even if you don’t know the specific biological mechanism. So: transforming your data is often OK, but is it a good idea?\n\nWhy transform? Because linear and symmetric data are often easier to interpret and typically better suited for statistical modeling.\n\nWhy not transform? Truth and clear understanding of data is more important than having symmetric distributions. So while transforming is perfectly legitimate, it can make your results harder to communicate (I, for one, find thinking on the log scale difficult — even if four-year-olds do not), since most people are accustomed to a linear world. You may also worry that knee-jerk transformation might hide the very biological processes that we are aiming to understand. Finally, some data can’t be made symmetric or unimodal no matter what transformation you apply. For example, it’s rare for transformations to make multimodal data unimodal. So take this all into consideration before jumping to transform your data to make it have an appealing shape.\n\n\n\n\nRules for Transforming Data\nIn addition to letting biology guide your decisions, there are some actual rules that determine when a transformation is legitimate:\n\nTransformed values must have a one-to-one correspondence with the original values. For example, don’t square values if some are negative and others positive — you’ll lose information.\n\nTransformed values must maintain a monotonic relationship with the original values. This means that the order of values should be preserved — larger values in the original data should remain larger after transformation.\n\nTransformations must not bias results by inadvertently dropping data. For example, this can happen when a log transformation fails on zero or negative values, or if extreme values are removed during transformation.\n\n\n\nTransformation in R\n\n\n\n\n\n\n\n\n\nFigure 2: Applying a log10 transformation can change the shape of a dataset. Panel A shows the distribution of Clarkia petal area (mm²) on a linear scale, where the data are right-skewed. Panel B shows the same data after a log10 transformation, which reduces skewness and results in a more symmetric distribution.\n\n\n\n\nTransformation in R is remarkably straightforward. Simply use dplyr's mutate() function to add a transformed variable. Below, I show a log base 10 transformation — one of the most common and useful approaches for right-skewed data (see the next section for a more thorough description of this and other common transformations). As discussed in the previous chapter, Figure 2 shows that this transformation changes the shape of the petal area distribution from right-skewed to roughly symmetric.\n\ngc_rils  &lt;- gc_rils |&gt;\n  mutate(log10_petal_area = log10(petal_area_mm))\n\n\n\n\n\n\n\n\n\nCommon Transformations\nApplying the right transformation can improve interpretability, meet statistical assumptions, and connect patterns to process. The table below introduces some of the more common transformations you will run into.\n\n\n\nTransform\nFormula\nWhat it does\nData it’s good for\nExample datasets\nR function (use with mutate())\nLimitations\n\n\n\n\nLog\nlog(x)\nCompresses large values, spreads small values\nRight-skewed data\nBody mass, gene expression, reaction times\nlog()\nValues ≤ 0 cause errors\n\n\nLog + 1\nlog(x + 1)\nSimilar to log but works for zero values\nRight-skewed data with zeros\nPopulation counts, RNA-seq read counts\nlog1p()\nValues ≤ -1 cause errors\n\n\nLog Base 10\nlog10(x)\nSimilar to log but base 10, this can be more interpretable.\nRight-skewed data\npH, sound intensity, scientific measurements\nlog10()\nValues ≤ 0 cause errors\n\n\nSquare Root\nsqrt(x)\nReduces range while preserving order\nRight-skewed data\nEnzyme activity\nsqrt()\nValues &lt; 0 cause errors\n\n\nSquare\n\\(x^2\\)\nIncreases spread, emphasizes large values\nLeft-skewed data\n\nx^2\nMakes values more extreme\n\n\nCube\n\\(^3\\)\nFurther increases spread\nLeft-skewed data\nTree volume, growth rates\nx^3\nStrongly affects scale\n\n\nInverse\n\\(1/x\\)\nEmphasizes small values, compresses large ones\nRight-skewed data\nReaction times, waiting times\n1/x\nValues = 0 cause errors\n\n\nArcsin Square Root\n\\(arcsin(\\sqrt{x})\\)\nNormalizes proportions\nProportions (e.g., survival, germination rates)\nAllele frequencies, % cover\nasin(sqrt())\nWorks poorly near 0 and 1\n\n\n\n\n\n\n\nAsmuth, J., Morson, E. M., & Rips, L. J. (2018). Children’s understanding of the natural numbers’ structure. Cognitive Science, 42(6), 1945–1973. https://doi.org/https://doi.org/10.1111/cogs.12615",
    "crumbs": [
      "5. Simple Summaries",
      "• 5. Changing shape"
    ]
  },
  {
    "objectID": "book_sections/univariate_summaries/summarizing_center.html",
    "href": "book_sections/univariate_summaries/summarizing_center.html",
    "title": "• 5. Summarizing the center",
    "section": "",
    "text": "Code for selecting data from a few columns from RILs planted at GC\nril_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_rils.csv\"\nril_data &lt;- readr::read_csv(ril_link) |&gt;\n  dplyr::mutate(growth_rate = case_when(growth_rate ==\"1.8O\" ~ \"1.80\",\n                                          .default = growth_rate),  \n                growth_rate = as.numeric(growth_rate),\n                visited = mean_visits &gt; 0)\ngc_rils &lt;- ril_data |&gt;\n  filter(location == \"GC\", !is.na(prop_hybrid), ! is.na(mean_visits))|&gt;\n  select(petal_color, petal_area_mm, num_hybrid, offspring_genotyped, prop_hybrid, mean_visits , asd_mm )\n\n\n\nMotivating Scenario:\nYou are continuing your exploration of a fresh new dataset. You have figured out the shape and made the transformations you thought appropriate. You now want to get some numerical summaries of the center of the data.\nLearning Goals: By the end of this subchapter, you should be able to:\n\nDifferentiate between parametric and nonparametric summaries: and know what shapes of data make one more appropriate than the other.\nCalculate and interpret standard summaries of center in R. These include:\n\nMedian: The middle.\nMean: The center of gravity.\n\nMode(s): The common observation(s).\n\nLook up / use less common summaries of the center. These include:\n\nThe trimmed mean: The average after removing a fixed percentage of the smallest and largest values (i.e., trimming the “tails”).\n\nThe harmonic mean: The reciprocal of the arithmetic mean of reciprocals, useful for averaging rates.\n\nThe geometric mean: The \\(n^{th}\\) root of the product of all values, often used for multiplicative data.\n\n\n\n\n\nWe hear and say the word, “Average”, often. What do we mean when we say it? “Average” is an imprecise term for a middle or typical value.\n\n\n\n\n\n\n\n\n\n\nFigure 1: Step-by-step process of finding the median petal area in parviflora RILs. The animation begins with unordered petal area measurements plotted against their dataset order. The values are then sorted in increasing order, and a vertical dashed line appears at the middle value, marking the median. The median is highlighted, illustrating how it divides the dataset into two equal halves.\n\n\n\n\nThere are many ways to describe the center of a dataset, but we can broadly divide them into two categories – “nonparametric” or “parametric”. We will first show these summaries for petal area in our parviflora RILS, then compare them for numerous traits in these RILs.\n\nNonparametric summaries\nNonparametric summaries describe the data as it is, without assuming an underlying probability model that generated it. The most common non-parametric summaries of center are:\n\nMedian: The middle observation, which is found by sorting data from smallest to biggest (Shown visually in Figure 1).\n\nSelecting the value of the \\(\\frac{n+1}{2}^{th}\\) value if there are an odd number of observations,\nSelecting the average of the \\(\\frac{n}{2}^{th}\\) and \\(\\frac{(n+2)}{2}^{th}\\) observations if there are an even number of observations.\n\nOr just use the median() function in R – usually inside summarize() (revisit the chapter on summarizing columns in dplyr for a refresher). Remember to specify na.rm = TRUE.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Illustration of the mode in petal areas of parviflora RILs. The histogram displays the distribution of petal area (mm), with the mode marked by a blue vertical line and labeled in blue text. The mode represents the most frequently occurring value in the dataset, corresponding to the tallest bar in the histogram.\n\n\n\n\n\nMode(s): The most common observation(s) or observation bin (Figure 2).\n\nWhen reporting the mode, make sure your bin size is appropriate so as to make this a meaningful summary.\n\nCommunicating the modes is particularly important bimodal and multimodal data.\n\n\n\n\nParametric summaries\nParametric summaries describe the data in a way that aligns with a probability model (often the normal distribution), allowing us to generalize beyond the observed data.\n\nMean: The mean is the most common description of central tendency, and is known as the expected value or the weight of the data.\n\nWe find this by adding up all values and dividing by the sample size. In math notation the mean, \\(\\overline{X} = \\frac{\\Sigma x_i}{n}\\), where \\(\\Sigma\\) means that we sum over the first \\(i = 1\\), second \\(i = 2\\) … up until the \\(n^{th}\\) observation of \\(x\\), \\(x_n\\). and divide by \\(n\\), where \\(n\\) is the size of our sample. Remember this size does not count missing values.\nOr just use the mean() function in R – usually inside summarize() (revisit the chapter on summarizing columns in dplyr for a refresher). Remember to specify na.rm = TRUE.\n\n\nRevisiting our examples above, we get the following simple summaries of mean and median. To do so, I type something like the code below (with elaborations for prettier formatting etc).\n\n\nBut remember mean and/or median may not be the best ways to summarize the center of either data set.\n\ngc_rils|&gt;\n  mutate(log10_petal_area_mm = log10(petal_area_mm))|&gt;\n  summarise(mean_log10_petal_area_mm = mean(log10_petal_area_mm, na.rm=TRUE),\n            median_log10_petal_area_mm = median(log10_petal_area_mm, na.rm = TRUE))\n\n# and\n\ngc_rils|&gt;\n  mutate(mean_mean_visits = mean(mean_visits, na.rm=TRUE),\n         median_mean_visits = median(mean_visits, na.rm = TRUE))\n\n\n\n\n\n\nsummary\nlog10 petal area in hybrid zones (cm^2)\nlog10 petal area in RILs (mm^2)\nPollinator visitation in GC\n\n\n\n\nmean\n-0.099\n1.781\n0.12\n\n\nmedian\n0.064\n1.789\n0.00\n\n\n\n\n\n\n\nWhich Summaries to Use When?\n\nMeans are best when data are roughly symmetric and plausibly generated by a well-understood distribution. Parametric summaries like the mean integrate easily with most statistical methods, and in many cases, the mean, median, and mode are roughly equivalent.\nMedians are most appropriate when data are skewed. A classic example is income data—if Bill Gates walks into a room, the mean wealth increases dramatically, but the typical person in the room does not become wealthier. The median, which is less affected by extreme values, provides a more representative summary in such cases.\nModal peaks are most appropriate when data have multiple peaks (modes) or a large, dominant peak, the mode is often the most relevant measure of central tendency. For example, in our investigation of petal area in a Clarkia hybrid zone, the mean and median of log₁₀ petal area (cm²) were both close to zero (which corresponds to 1 cm²). However, this value falls in the trough between two peaks in the histogram—one corresponding to Clarkia xantiana xantiana and another to Clarkia xantiana parviflora. This means that neither the mean nor the median represents an actual plant particularly well, and the modal peaks give a clearer picture of what values are most typical.\n\n\n\n\nCool down\n\n\n\n\n\n\n\n\nFigure 3: Distributions of select traits in Clarkia datasets. This figure shows histograms of three different variables from two datasets: Recombinant Inbred Line (RIL) populations and a hybrid zone dataset. The left panel displays the distribution of petal area (log10 mm^2) in the RIL dataset, showing a unimodal distribution. The middle panel presents the log10-transformed petal area (log10 cm^2) in the hybrid zone dataset, which appears bimodal. The right panel illustrates the number of pollinator visits at GC in the RIL dataset, showing a highly right-skewed distribution with many zero observations.\n\n\n\n\n\n\n\n\n\nUse-full but used-less summaries\nBelow are a few additional, useful but less commonly used, summaries of central tendency. It is good to know these exist. If this material is too slow / easy. for you, I recommend using your study time to familiarize yourself with these useful summaries, but otherwise don’t worry about them.\nThese assume that you are modelling these non-linear processes on a linear scale. You can decide if transformation or a more relevant summary statistics on a linear scale is more effective for your specific goal.\n\n\nHarmonic mean – Is the reciprocal of the mean of reciprocals. Useful when averaging rates, ratios, or speeds. Unlike the arithmetic mean, which sums values, the harmonic mean gives more weight to smaller values and is particularly useful when values are reciprocals of meaningful quantities. For example in my field population genetics, the harmonic mean is used to calculate effective population size (\\(N_e\\)), as small population sizes have a disproportionate effect on genetic drift.\n\nMathematical calculation of the harmonic mean: The harmonic mean of a vector x is = \\(\\frac{1}{\\text{mean}(\\frac{1}{x})}\\) = \\(\\frac{n}{\\sum_{i=1}^{n} \\frac{1}{x_i}}\\).\nHarmonic mean in R: You can find the harmonic mean as: 1/(mean(1/x)), or use the Hmean() function in the DescTools package. Watch out for zeros!!\n\nGeometric mean - Is the \\(n^{th}\\) root of the product of \\(n\\) observations. The geometric mean is a useful summary of multiplicative or exponential processes For example: (1) Bacterial growth: If a bacterial population doubles in size daily, the geometric mean correctly summarizes growth trends, and (2) pH values in chemistry: Since pH is logarithmic, the geometric mean is a better measure than the arithmetic mean.\n\nMathematical calculation of the geometric mean: The geometric mean of a vector x is \\(\\left( \\prod_{i=1}^{n} x_i \\right)^{\\frac{1}{n}}\\), where \\(\\prod\\) is the the “cumulative”product operator” i.e. the cumulative product of all observations.\nGeometric mean in R: You can find the geometric mean as: prod(x)^(1/sum(!is.na(x))), or use the Gmean() function in the DescTools package. Watch out for negative values as they make this kind of meaningless.\n\nTrimmed mean – A robust version of the mean that reduces the influence of extreme values by removing a fixed percentage of the smallest and largest observations before calculating the average. A 10% trimmed mean, for example, removes the lowest 10% and highest 10% of values before computing the mean. This is useful when extreme values may distort the mean but full exclusion of outliers isn’t justified (e.g., summarizing body weights where a few exceptionally large or small individuals exist).\n\nTrimmed mean in R: You can find the trimmed mean yourself or by using the trimmed_mean() function in the in the r2spss package.",
    "crumbs": [
      "5. Simple Summaries",
      "• 5. Summarizing the center"
    ]
  },
  {
    "objectID": "book_sections/univariate_summaries/summarizing_variability.html",
    "href": "book_sections/univariate_summaries/summarizing_variability.html",
    "title": "• 5. Summarizing variability",
    "section": "",
    "text": "Code for selecting data from a few columns from RILs planted at GC\nril_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_rils.csv\"\nril_data &lt;- readr::read_csv(ril_link) |&gt;\n  dplyr::mutate(growth_rate = case_when(growth_rate ==\"1.8O\" ~ \"1.80\",\n                                          .default = growth_rate),  \n                growth_rate = as.numeric(growth_rate),\n                visited = mean_visits &gt; 0)\ngc_rils &lt;- ril_data |&gt;\n  filter(location == \"GC\", !is.na(prop_hybrid), ! is.na(mean_visits))|&gt;\n  select(petal_color, petal_area_mm, num_hybrid, offspring_genotyped, prop_hybrid, mean_visits , asd_mm )\nlibrary(ggthemes)\n\n\n\nMotivating Scenario: You are continuing your exploration of a fresh dataset. You have examined its shape and applied any necessary transformations. Now, you want to obtain numerical summaries that describe the variability in your data.\nLearning Goals: By the end of this subchapter, you should be able to:\n\nExplain why variability in a dataset is biologically important.\nDifferentiate between parametric and nonparametric summaries and understand which data shapes make one more appropriate than the other.\nVisualize variability and connect numerical summaries to plots. You should be able to read the interquartile range off of a boxplot.\nDistinguish between biased and unbiased estimators.\nCalculate and interpret standard summaries of variability in R, including:\n\nInterquartile range (IQR): The difference between the 25th and 75th percentiles, summarizing the middle 50% of the data.\n\nStandard deviation and variance: The standard deviation quantifies how far, on average, data points are from the mean. Variance is the square of the standard deviation.\n\nCoefficient of variation (CV): A standardized measure of variability that allows for fair comparisons across datasets with different means.\n\n\n\n\n\n\n\n\nIn a world where everything was the same every day (See video above), describing the center would be enough — luckily our world is more exciting than that. In the real world, the extent to which a measure of center is sufficient to understand a population depends on the extent and nature of variability. Not only is understanding variability essential to interpreting measures of central tendency, but in many cases, describing variability is as or even more important than describing the center. For example, the amount of genetic variance in a population determines how effectively it can respond to natural selection. Similarly, in ecological studies, two populations of the same species may have similar average survival rates, but greater variability in survival in one population might indicate environmental instability, predation pressure, or developmental noise.\n\nNonparametric measures of variability\nPerhaps the most intuitive summary of variation in a dataset is the range—the difference between the largest and smallest values. While the range is often worth reporting, it is a pretty poor summary of variability because it is highly sensitive to outliers (a single unexpectedly extreme observation can strongly influence the range) and is biased with respect to sample size (the more samples you collect, the greater the expected difference between the smallest and largest values).\nAs such, the interquartile range (IQR)—the difference between the 75th and 25th percentiles (i.e., the third and first quartiles)—is a more robust, traditional nonparametric summary of variability. We can read off the interquartile range from a boxplot. A boxplot shows a box around the first and third quartiles, a line at the median, and “whiskers” that extend to the minimum and maximum values (excluding outliers, which are shown as black dots).\n\n\ngc_rils |&gt;\n  ggplot(aes(x = 1,\n      y = petal_area_mm))+\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nFigure 1: Anatomy of a boxplot. This boxplot displays the distribution of petal area in the Clarkia RIL dataset. The box spans from the first quartile (25th percentile) to the third quartile (75th percentile), highlighting the interquartile range (IQR). The line inside the box marks the median (50th percentile). The “whiskers” extend to the smallest and largest non-outlier values, and individual outliers are shown as separate points.\n\n\n\n\nFigure 1 shows that the third quartile for petal area is a bit above seventy, and the first quartile is a bit above fifty, so the interquartile range is approximately twenty. Or with the IQR() function:\n\ngc_rils |&gt;\n  summarise(Q3 = quantile(petal_area_mm,.75, na.rm = TRUE),\n            Q1 = quantile(petal_area_mm,.25, na.rm = TRUE),\n            iqr_petal_area = IQR(petal_area_mm, na.rm = TRUE))\n\n# A tibble: 1 × 3\n     Q3    Q1 iqr_petal_area\n  &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;\n1  71.6  51.4           20.2\n\n\n\n\nParametric summaries of variability\nMathematical summaries of variability aim to describe how far we expect an observation to deviate from the mean. Such summaries start by finding the sum of squared deviations, \\(SS_X\\) (i.e. the sum of squared differences between each observation and the mean). We square deviations rather than taking their absolute value because squared deviations (1) are mathematically tractable, (2) emphasize large deviations, and (3) allow the mean to be the value that minimizes them — which ties directly into least squares methods we’ll use later in regression. We find \\(SS_X\\), know as “the sum of squares” as:\n\\[\\text{Sum of Squares} = SS_X = \\Sigma{(X_i-\\overline{X})^2}\\]\nFrom the sum of squares we can easily find three common summaries of variability:\n\n\nWhy divide by \\(n-1\\)? When we calculate how far values are from the mean, we might think to average the squared deviations by dividing by the number of values, \\(n\\). But, by calculating the mean, we’ve used up a little bit of information. Because the mean pulls the values toward itself, numbers aren’t totally free to vary anymore. Because of that, we divide the sum of squares by \\((n-1)\\) rather than \\(n\\). This gives us a more accurate sense of how spread out the values really are, based on how much they can still vary around the mean.\n\nThe variance, \\(s^2\\) is roughly the average squared deviation, but we divide the sum of squares by our sample size minus 1. That is the \\(\\text{variance} = s^2 = \\frac{SS_x}{n-1} = \\frac{\\Sigma{(X_i-\\overline{X})^2}}{n-1}\\). The variance is mathematically quite handy, and is in squared units relative to the initial observations.\nThe standard deviation, \\(s\\) is simply the square root of the variance. The standard deviation is often easier to think about because it lies on the same (linear) scale as the initial data (as opposed to the squared scale of the variance).\nThe coefficient of variation, CV allows us to compare variance between variables with different means. In general variability increases with the mean, so you cannot meaningful compare the variance in petal area (which equals 203.399) with the variance in anther stigma distance (which equals 0.134). But it’s still biologically meaningful to ask: “Which trait is more variable, relative to its mean?” We answer this question by finding the coefficient of variation which equals the standard deviations divided by the mean: CV = \\(\\frac{s_x}{\\overline{X}}\\). Doing so, we find that anther–stigma distance is nearly twice as variable as petal area.\n\n\nIt’s ok to be MAD. The mean absolute difference (aka MAD, which equals \\(\\frac{\\Sigma{|x_i-\\bar{x}|}}{n}\\)) is a valid, and robust, but non-standard summary of variation. The MAD is most relevant when presenting the median as the median minimizes the sum of absolute deviations, while the mean minimizes the sum of squared deviations.\n\n\n\nParametric summaries of variability: Example\n\n\n\n\n\n\n\n\nFigure 2: Understanding the squared deviation. The “Sum of Squared Deviations” is critical to understanding standard summaries of variability. The animation above aims to explain a “squared deviation.” Left panel: Each individual’s deviation from the mean petal area is visualized as a vertical line, with the iᵗʰ value highlighted in pink. Middle panel: The same individual is plotted in a 2D space – the squared deviation is shown as the area of a pink rectangle. Right panel: The squared deviation is shown for all individuals, with the same focal individual highlighted.\n\n\n\n\n\nTo make these ideas clear, let’s revisit the distribution of petal area in parviflora RILs planted in GC. Figure 1 shows how we calculate the Squared Deviation for each data point.\n\nThe left panel shows the difference between each observed value and its mean.\n\nThe middle panel shows this as a box (or square) away from the overall, “grand” mean.\n\nThe right panel shows the squared deviation.\n\nWe find the sum of squares by summing these values, and then use this to find the variance, standard deviation and coefficient of variation, following the formulae above:\n\ngc_rils |&gt;\n  filter(!is.na(petal_area_mm))|&gt;\n  summarise(mean_petal_area = mean(petal_area_mm),\n            ss_petal_area = sum((petal_area_mm - mean_petal_area)^2),\n            var_petal_area = ss_petal_area / (n()-1),\n            sd_petal_area = sqrt(var_petal_area),\n            CV_petal_area = sd_petal_area / mean_petal_area )\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean_petal_area\nss_petal_area\nvar_petal_area\nsd_petal_area\nCV_petal_area\n\n\n\n\n62.05\n19933\n203.4\n14.26\n0.23\n\n\n\n\n\nOr we can skip the formulae and just use standard R functions, var() and sd(). We can even find the mean absolute difference with the mad() function:\n\ngc_rils |&gt;\n  filter(!is.na(petal_area_mm))|&gt;\n  summarise(mean_petal_area = mean(petal_area_mm),\n            var_petal_area = var(petal_area_mm),\n            sd_petal_area  = sd(petal_area_mm),\n            CV_petal_area = sd_petal_area / mean_petal_area,\n            mad_petal_area = mad(petal_area_mm ))\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean_petal_area\nmad_petal_area\nvar_petal_area\nsd_petal_area\nCV_petal_area\n\n\n\n\n62.05\n15.01\n203.4\n14.26\n0.23",
    "crumbs": [
      "5. Simple Summaries",
      "• 5. Summarizing variability"
    ]
  },
  {
    "objectID": "book_sections/univariate_summaries/summarizing_summaries.html",
    "href": "book_sections/univariate_summaries/summarizing_summaries.html",
    "title": "• 5. Summarizing summary",
    "section": "",
    "text": "Chapter Summary\nLinks to: Summary. Chatbot tutor Questions. Glossary. R functions. R packages. More resources.\nA beautiful Clarkia xantiana flower.\nBecause they can be used to parameterize an entire distribution, the mean and variance (or its square root, the standard deviation) are the most common summaries of a variable’s center and spread. However, these summaries are most meaningful when the data resemble a bell curve. To make informed choices about how to summarize a variable, we must first consider its shape, typically visualized with a histogram. When data are skewed or uneven, we can either transform the variable to make its distribution more balanced, or use alternative summaries like the median and interquartile range, which better capture the center and spread in such cases.",
    "crumbs": [
      "5. Simple Summaries",
      "• 5. Summarizing summary"
    ]
  },
  {
    "objectID": "book_sections/univariate_summaries/summarizing_summaries.html#chapter-summary",
    "href": "book_sections/univariate_summaries/summarizing_summaries.html#chapter-summary",
    "title": "• 5. Summarizing summary",
    "section": "",
    "text": "Chatbot tutor\n\nPlease interact with this custom chatbot (link here) I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you got what you needed from it.",
    "crumbs": [
      "5. Simple Summaries",
      "• 5. Summarizing summary"
    ]
  },
  {
    "objectID": "book_sections/univariate_summaries/summarizing_summaries.html#summarizing_summaries_practice-questions",
    "href": "book_sections/univariate_summaries/summarizing_summaries.html#summarizing_summaries_practice-questions",
    "title": "• 5. Summarizing summary",
    "section": "Practice Questions",
    "text": "Practice Questions\nTry these questions! By using the R environment you can work without leaving this “book”.\n\nIrisFaithfulRivers\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nThe tabs above – Iris, Faithful, and Rivers – all attempt to make histograms, but include errors, and may have improper bin sizes. (Click the Iris tab if they are initially empty).\nQ1) Iris, Faithful, and Rivers – all attempt to make histograms, but include errors. Which code snippet makes the best version of the Iris plot (ignoring bin size)?\n\n ggplot(iris,aes(x = Sepal.Width, fill = 'white'))+ geom_histogram() ggplot(iris,aes(x = Sepal.Width, color = 'white'))+ geom_histogram() ggplot(iris,aes(x = Sepal.Width))+ geom_histogram(color = 'white') ggplot(iris,aes(x = Sepal.Width))+ geom_histogram(color = white) ggplot(iris,aes(x = Sepal.Width))+ geom_histogram(fill = 'white')\n\n\nBefore addressing this set of questions fix the errors in the histograms of Iris, Faithful, and Rivers, and adjust the bin size of each plot until you think it is appropriate. (Click any of the tabs if they are initially empty).\nQ2a) Which variable is best described as bimodal? Iris sepal widthWaiting time between eruptions of old faithful.log-10 river length\nQ2b) Which variable is best described as unimodal and symmetric? Iris sepal widthWaiting time between eruptions of old faithful.log-10 river length\nQ2c) Which variable is best described as unimodal and right skewed? Iris sepal widthWaiting time between eruptions of old faithful.log-10 river length\n\n\n\nPenguins\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ3) I calculate means in two different ways above and get different answers. Which is correct? mean_mass_1mean_mass_2it depends\nQ4) What went wrong in calculating these means?\n\n n() counts the number of entries, but we need the number of non-NA entries. na.rm should be set to TRUE, not T The denominator for the mean is (n - 1), not n. Nothing – they are both potentially correct depedning on your goals.\n\n\nQ5) Accounting for species differences in mean body mass, which penguin species shows the greatest variability in body mass? AdelieChinstrapGentoo\n\n\nClick here for code\n\n\nlibrary(palmerpenguins)\nlibrary(ggplot2)\nlibrary(dplyr)\n\npenguins |&gt;\n  group_by(species)|&gt;\n  summarize(mean_mass = mean(body_mass_g, na.rm = T),\n            sd_mass   = sd(body_mass_g, na.rm = T),\n            coef_var_mass =   sd_mass / mean_mass \n  )\n\n# A tibble: 3 × 4\n  species   mean_mass sd_mass coef_var_mass\n  &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;\n1 Adelie        3701.    459.        0.124 \n2 Chinstrap     3733.    384.        0.103 \n3 Gentoo        5076.    504.        0.0993\n\n\n\n.\n\nFor the next set of questions consider the boxplot below, which summarizes the level of Lake Huron in feet every year from 1875 to 1972.\n\n\n\n\n\n\n\n\n\nQ6a) The mean is roughly\n\n Six One and three quarters Five hundred and seventy nine We cannot estimate the mean from this plot.\n\nQ6b) The median is roughly\n\n Six One and three quarters Five hundred and seventy nine We cannot estimate the median from this plot.\n\nQ6c) The mode is roughly\n\n Six One and three quarters Five hundred and seventy nine We cannot estimate the mode from this plot.\n\nQ6d) The interquartile range is roughly\n\n Six One and three quarters The median Five hundred and seventy nine We cannot estimate the IQR from this plot.\n\nQ6e) The range is roughly\n\n Six One and three quarters The median Five hundred and seventy nine We cannot estimate the range from this plot.\n\nQ6f) The variance is roughly\n\n Six One and three quarters The median Five hundred and seventy nine We cannot estimate the variance from this plot.\n\n\nBrooke planted RILs at four different locations, and found tremendous variation in the proportion of hybrid seed across locations. The first step in quantifying this variation is to calculate the sum of squares, so let’s do it. Use the image below (Figure 1) to calculates sum of squares for proportion hybrid seeds across plants planted at four different locations.\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ7a) The sum of squares for differences between proportion hybrids at each location and the grand mean equals: \nQ7b) So the variance is \nQ7c) The standard deviation is \nQ7d) Accounting for differences in their means, how does the variability in the proportion of hybrid seed across locations compare to the variability in petal area among RILs? (refer to this section for reference)\n\n They are very similar Petal area among RILs is roughly thirty times as variable The proportion of hybrid seed among sites is roughly two times as variable You cannot compare variability for different traits measured on such different scales\n\n\n\nClick here for hint\n\nWe can compare the variability of traits measured on different scales by dividing the standard deviation by the mean. This gives us the coefficient of variation (CV), a standardized measure of spread.\n\n.\nQ8) Why is it important to standardize by the mean when comparing variability between variables?",
    "crumbs": [
      "5. Simple Summaries",
      "• 5. Summarizing summary"
    ]
  },
  {
    "objectID": "book_sections/univariate_summaries/summarizing_summaries.html#summarizing_summaries_glossary-of-terms",
    "href": "book_sections/univariate_summaries/summarizing_summaries.html#summarizing_summaries_glossary-of-terms",
    "title": "• 5. Summarizing summary",
    "section": "Glossary of Terms",
    "text": "Glossary of Terms\n\n\n📐 1. Shape and Distribution\n\nSkewness: A measure of asymmetry in a distribution.\n\nRight-skewed: Most values are small, with a long tail of large values.\n\nLeft-skewed: Most values are large, with a long tail of small values.\n\nMode: The most frequently occurring value (or values) in a dataset.\nUnimodal / Bimodal / Multimodal: Describes the number of peaks (modes) in a distribution.\n\nUnimodal: One peak\n\nBimodal: Two peaks, possibly indicating two subgroups\n\nMultimodal: More than two peaks\n\n\n\n\n\n🔁 2. Transformations and Data Shape\n\nTransformation: A mathematical function applied to data to change its shape or scale. Often used to reduce skew or satisfy model assumptions.\nMonotonic Transformation: A transformation that preserves the order of values (e.g., if \\(x_1 &gt; x_2\\), then \\(f(x_1) &gt; f(x_2)\\)). Required for valid shape-changing operations.\nLog Transformation (log(), log10()): Reduces right skew by compressing large values.\n\n✅ Use for right-skewed data (e.g., area, income, growth).\n\n⚠️ Don’t use with zero or negative values — log is undefined in those cases. A workaround is log(x + 1) for count data.\n\nSquare Root Transformation (sqrt()): Less aggressive than log. Preserves order while compressing large values.\n\n✅ Use for right-skewed data like enzyme activity or count data.\n\n⚠️ Not defined for negative values.\n\nReciprocal / Inverse (1/x): Emphasizes small values and compresses large ones.\n\n✅ Use for rates or time-based data (e.g., reaction time).\n\n⚠️ Undefined for zero values; extremely sensitive to small values.\n\nSquare / Cube (x^2, x^3): Spreads data out, emphasizing large values.\n\n✅ Can reduce left skew.\n\n⚠️ Squaring loses sign if data contains negatives; avoid if data include both positive and negative values.\n\n\n\n\n\n🎯 3. Summarizing the Center (Central Tendency)\n\nMean (mean()): The arithmetic average. Sensitive to outliers.\n\n\\(\\overline{X} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\\)\n\nMedian (median()): The middle value of a sorted dataset. Robust to outliers.\nMode: Most frequent value or value bin.\nTrimmed Mean: Mean after removing fixed percentages of extreme values. Balances robustness and efficiency.\nGeometric Mean: The nth root of the product of values.\n\n✅ Appropriate for multiplicative data (e.g., growth rates, ratios, log-normal data).\n\n⚠️ Don’t use with zeros or negative values — the geometric mean is undefined.\n\n🧠 Tip: Especially useful for right-skewed, strictly positive data that spans multiple orders of magnitude.\n\nHarmonic Mean: The reciprocal of the mean of reciprocals.\n\n✅ Useful when averaging ratios or rates (e.g., speed, population size in genetics).\n\n⚠️ Very sensitive to small values and undefined for zero or negative numbers.\n\n🧠 Tip: Use when the quantity being averaged is in the denominator (e.g., “miles per hour”).\n\n\n\n\n\n📉 4. Summarizing Variability\n\nRange: Difference between maximum and minimum. Sensitive to outliers.\nInterquartile Range (IQR) (IQR()): Middle 50% of data. Robust and often paired with the median.\nMean Absolute Deviation (MAD) (mad()): The average absolute deviation from the mean or median. Robust and intuitive.\n\n\\(\\text{MAD} = \\frac{1}{n} \\sum |x_i - \\bar{x}|\\)\n\nSum of Squares (SS): Total squared deviation from the mean.\n\n\\(SS = \\sum (x_i - \\bar{x})^2\\)\n\nVariance (var()): The average squared deviation from the mean.\n\n\\(s^2 = \\frac{SS}{n - 1}\\)\n\nStandard Deviation (sd()): Square root of variance. Easier to interpret due to linear units.\n\n\\(s = \\sqrt{s^2}\\)\n\nCoefficient of Variation (CV): Standard deviation divided by the mean. Unitless and good for comparing across traits or units.\n\n\\(CV = \\frac{s}{\\bar{x}}\\)\n\n\n\n\n\n📊 5. Visualizing Distributions\n\nHistogram: Shows frequency of values within bins. Useful for assessing shape, skewness, and modes.\nBoxplot: Summarizes median, quartiles, range, and outliers in a compact visual form.",
    "crumbs": [
      "5. Simple Summaries",
      "• 5. Summarizing summary"
    ]
  },
  {
    "objectID": "book_sections/univariate_summaries/summarizing_summaries.html#summarizing_summaries_key-r-functions",
    "href": "book_sections/univariate_summaries/summarizing_summaries.html#summarizing_summaries_key-r-functions",
    "title": "• 5. Summarizing summary",
    "section": "Key R Functions",
    "text": "Key R Functions\n\n\n📊 Visualizing Univariate Data\n\ngeom_histogram() ([ggplot2]): Makes histograms for visualizing distributions.\n\ngeom_boxplot() ([ggplot2]): Visualizes the distribution using a box-and-whisker plot.\n\ngeom_col() ([ggplot2]): Creates bar plots from summarized data.\n\ngeom_bar() ([ggplot2]): Bar plot for raw count data.\n\n\n\n\n📈 Summarizing Center\n\nmean() ([base R]): Computes the arithmetic mean.\n\nmedian() ([base R]): Computes the median.\n\nmutate() ([dplyr]): Adds new variables or transforms existing ones.\n\nsummarise() ([dplyr]): Reduces multiple rows to a summary value per group.\n\n\n\n\n📏 Summarizing Variability\n\nvar() ([base R]): Computes variance.\n\nsd() ([base R]): Computes standard deviation.\n\nmad() ([base R]): Computes the median absolute deviation — a robust summary of variability.\n\nIQR() ([base R]): Computes the interquartile range.\nquantile() ([base R]): Returns sample quantiles. Useful for computing percentiles and quartiles.\n\nsum() ([base R]): Used in calculating the sum of squared deviations (sum((x - mean(x))^2)).\n\n\n\n\n🔁 Transformations\n\nlog() ([base R]): Natural log (base e) transformation.\n\nlog10() ([base R]): Base 10 log transformation.\n\nsqrt() ([base R]): Computes square roots.\n\n^ ([base R]): Exponentiation (x^2, x^3, etc.).\n\n[1/x]: Reciprocal transformation. Beware of dividing by zero!",
    "crumbs": [
      "5. Simple Summaries",
      "• 5. Summarizing summary"
    ]
  },
  {
    "objectID": "book_sections/univariate_summaries/summarizing_summaries.html#summarizing_summaries_r-packages-introduced",
    "href": "book_sections/univariate_summaries/summarizing_summaries.html#summarizing_summaries_r-packages-introduced",
    "title": "• 5. Summarizing summary",
    "section": "R Packages Introduced",
    "text": "R Packages Introduced\n\n\nggforce: Provides advanced geoms for ggplot2. This chapter uses geom_sina() to reduce overplotting by jittering points while preserving density.",
    "crumbs": [
      "5. Simple Summaries",
      "• 5. Summarizing summary"
    ]
  },
  {
    "objectID": "book_sections/univariate_summaries/summarizing_summaries.html#additional-resources-summarizing_summaries_additional-resources",
    "href": "book_sections/univariate_summaries/summarizing_summaries.html#additional-resources-summarizing_summaries_additional-resources",
    "title": "• 5. Summarizing summary",
    "section": "Additional resources #summarizing_summaries_additional-resources}",
    "text": "Additional resources #summarizing_summaries_additional-resources}\n\nR Recipes:\n\nCompute summary statistics for a table: Learn how to find summary stats within a summarise() call.\n\nCompute summary statistics for groups of rows within a table Discover how to calculate summary stats by group.\n\nVisualize a Distribution with a Histogram: Learn to plot histograms to visualize the distribution of a continuous variable.\n\nVisualize a Boxplot: Find out how to create boxplots to summarize the distribution of a continuous variable and identify potential outliers.\n\nVideos:\n\nData summaries from Calling Bullshit (Bergstrom & West, 2020). Fun video to help with thinking about various summaries of center, and when to use which.\nThe shape of data from crash course in statistics.\n\n\n\n\n\n\nBergstrom, C. T., & West, J. D. (2020). Calling bullshit: The art of skepticism in a data-driven world. Random House.",
    "crumbs": [
      "5. Simple Summaries",
      "• 5. Summarizing summary"
    ]
  },
  {
    "objectID": "book_sections/associations.html",
    "href": "book_sections/associations.html",
    "title": "6. Associations",
    "section": "",
    "text": "Correlation is not causation\nOf course, it’s not just the association between variables we care about — it’s what such associations imply. We want to:\nIn future chapters we will see when and how we can achieve these higher goals. But for now, know that while such goals are noble,\n“Correlation is not causation.” You’ve probably heard that before — but what does it actually mean? Let’s start by unpacking the two key concepts in that statement:\nVideo\n\n\nFigure 2: This man is not starting and stopping the train. (tweet)\nCorrelation is often confused for causation because it’s easy to assume that if two things are associated, one must be causing the other — especially when the association feels intuitive or lines up with our expectations, but this is wrong. While correlation may hint at causation, a direct cause is neither necessary nor sufficient to generate a correlation. Take the video in Figure 2 – an alien might think this man is starting and stopping the train, but clearly he has nothing to do with the train starting or stopping.\nThere are three basic reasons why and when we can have a correlation without a causal relationship– Coincidence, Confound, and Reverse causation.\nFigure 3: Potential confounding in parviflora RILs. The observed association between proportion hybrid seed and anther–stigma distance (A), might be due to the fact that both anther–stigma distance (B), and proportion hybrid seed increases with petal area (C), rather than a causal effect of anther–stigma distance itself.\nFigure 4: Visualizing an association between petal color and pollinator visitation. Both panels show that pink-petaled flowers are more likely to be visited by pollinators than white-petaled flowers. In the left panel, visit status is on the x-axis and petal color is shown within bars; in the right panel, petal color is on the x-axis and visit status is shown within bars. While the association is the same, the visual framing shifts the way we interpret direction — we typically place and think of explanatory variables (causes) on the x-axis and outcomes on the y-axis. However, these alternative visualizations make it clear that the data cannot speak to cause.\nThis issue isn’t just theoretical — I’m currently grappling with a real case in which the direction of causation is unclear. In natural hybrid zones, white-flowered parviflora plants tend to carry less genetic ancestry from xantiana (their sister taxon) than do pink-flowered parviflora plants. There are two potential explanations for this observation:\nI do not yet know the answer.\n# YANIV ADD FIGURE FROM SHELLEY",
    "crumbs": [
      "6. Associations"
    ]
  },
  {
    "objectID": "book_sections/associations.html#correlation-is-not-causation",
    "href": "book_sections/associations.html#correlation-is-not-causation",
    "title": "6. Associations",
    "section": "",
    "text": "Correlation means that two variables are associated.\n\nA positive association means that when one variable is large (or small) the other is often big (or large).\n\nA negative association means that when one variable is large (or small) the other is often small (or large).\n\nCausation means that changing one variable produces a change in the other. (For a deeper dive, see Wikipedia.)\n\n\n\n\n\nCoincidence: Chance is surprisingly powerful. In a world full of many possible combinations between variables, some strong associations will arise purely by luck. Later sections of the book will show how to evaluate the “NULL” hypothesis that an observed association arose by chance.\n\n\n\nConfounding: An association between two variables may reflect not a causal connection between them – but rather the fact that both are caused by a third variable (known as a confound). Such confounding may be at play in our RIL data – we observe that anther–stigma distance is associated with the proportion hybrid seed, but anther–stigma distance is also associated petal area (presumably because both are caused by flower growth), which itself is associated with the proportion of hybrids (Figure 2.1). So, does petal area or anther stigma distance (or both or neither) cause an increase in proportion of hybrid seed? The answer awaits better data, or at least better analyses (see section on causal inference), but I suspect that petal area, not anther stigma distance “causes” proportion hybrid. Unfortunately, we rarely know the confound, let alone its value. So, interpreting any association as causation requires exceptional caution.\n\n\nReverse causation: Figure 4 shows that pink flowers are more likely to receive a pollinator than are white flowers. We assume this means that pink attracts pollinators– and with the caveat that we must watch out for coincidence and confounds, this conclusion makes sense. However, an association alone cannot tell if pink flowers attracted pollinators or if pollinator visitation turned plants pink. In this case the answer is clear – petal color was measured for RILs in the greenhouse, and there’s no biological mechanism by which a pollinator could change petal color. However, these answers require us to bring in biological knowledge – the data alone can’t tell us which way the effect goes.\n\n\n\n\nPerhaps, as the RIL data suggests, white-flowered parviflora plants are less likely to hybridize with xantiana than are pink-flowered parviflora, so white-flowered plants have less xantiana ancestry (pink flowers cause more gene flow).\nAlternatively, all xantiana are pink-flowered, white-flowered parviflora can be white or pink. So maybe the pink flowers are actually caused by ancestry inherited from xantiana (more gene flow causes pink flowers).\n\n\n\n\n\n\n\n\n\nMaking things independent\n\n\n\n\n\nIf we cannot break the association between anther stigma distance and petal area by genetic crosses maybe we could do so by physical manipulation. For example, we could use tape or some other approach to move stigmas closer to or further from anthers.\n\n\n\n\nCausation without correlation\nNot only does correlation not imply causation, but we can have causation with no association.\n\n\n\n\n\n\n\n\n\nFigure 5: No relationship between height and Value over Replacement Player in the 2024-2025 NBA season.\n\n\n\n\nNo one will doubt that height gives a basketball player an advantage. Yet if we look across all NBA players, we see no relationship between height and standard measures of player success (e.g. salary, or “Value of Replacement Player” etc Figure 5), How can this be? The answer is that to make it to the NBA you have to be very good or very tall (and usually both) – so (6 foot 4, Shai Gilgeous-Alexander) has a value just a bit higher than (6 foot 11) Giannis Antetokounmpo.\nA related, but different issue – known as Countergradient variation is observed in ecological genetics. Here, measures of some trait, like growth rate, are similar across the species range (e.g. between northern and southern populations), but when grown in a common environment, the populations differ (e.g. the northern population grows faster). This might reflect divergence among population as a consequence of natural selection that may favor greater efficiency or acquisition of energy in northern regions.",
    "crumbs": [
      "6. Associations"
    ]
  },
  {
    "objectID": "book_sections/associations.html#making-predictions-is-hard",
    "href": "book_sections/associations.html#making-predictions-is-hard",
    "title": "6. Associations",
    "section": "Making predictions is hard",
    "text": "Making predictions is hard\n\nMaking predictions is hard, especially about the future.\n– Attributed to Yogi Berra\n\nAssociations describe data we have – they do not necessarily apply to other data. Of course, understanding such associations might help us make predictions, but we must consider the range and context of our data.\n\n\n\n\nTheir are different kinds of predictions we might want to make.\n\nWe may want to predict what we would expect for unsampled individuals from the same population as we are describing. In this case, a statistical association can be pretty useful.\nWe may want to predict what we would expect for individuals from a different population than what we are describing. In this case, a statistical association might help, but need some care.\nWe may want to predict what we would expect if we experimentally changed one of the value of an explanatory variable (e.g. if “I experimentally decreased anther-stigma distance, would plants set more hybrid seed?”) This is a causal prediction!\n\nMisalignment between expectations and the reality is a common trope in comedy and drama. For example, hilarity may ensue when an exotic dancer in a firefighter or police costume is mistaken for a true firefighter or policeman (See the scene from Arrested Development on the right (youtube link)). Such jokes show that we have an intuitive understanding that predictions can be wrong, and that the context plays a key role in our ability to make good predictions.\nWe again see such a case in our RIL data - leaf water content reliably predicts the proportion of hybrid seed set at three experimental locations, but is completely unrelated to proportion of hybrid seed at Upper Sawmill Road (location: US, Figure 6).\n\n\nCode to make the plot, below.\nfilter(ril_data, !is.na(location)) |&gt; \n  ggplot(aes(x= lwc,y =prop_hybrid))+\n  facet_wrap(~location,labeller = \"label_both\",nrow=1)+\n  geom_point(size = 2, alpha = .2)+\n  geom_smooth(method = \"lm\",se = FALSE, linewidth = 2)+\n  labs(x = \"Leaf water content\", \n       y = \"Proportion hybrid\")+\n  scale_x_continuous(breaks = seq(.78,.88,.04))\n\n\n\n\n\n\n\n\nFigure 6: Predictions might not generalize. The proportion of hybrid seed set reliably decreases with leaf water content in three locations (GC, LB, SR), but at the upper sawmill road site (US), there is no clear relationship. This cautions against assuming generalizable predictive patterns across environments.",
    "crumbs": [
      "6. Associations"
    ]
  },
  {
    "objectID": "book_sections/associations.html#there-is-still-value-in-finding-associations",
    "href": "book_sections/associations.html#there-is-still-value-in-finding-associations",
    "title": "6. Associations",
    "section": "There is still value in finding associations",
    "text": "There is still value in finding associations\nThe caveats above are important, but they should not stop us from finding associations. With appropriate experimental designs, statistical analyses, biological knowledge, and humility in interpretation, quantifying associations is among the most important ways to summarize and understand data.\nThe following sections provide the underlying logic, mathematical formulas, and R functions to summarize associations.",
    "crumbs": [
      "6. Associations"
    ]
  },
  {
    "objectID": "book_sections/associations.html#lets-get-started-with-summarizing-associations",
    "href": "book_sections/associations.html#lets-get-started-with-summarizing-associations",
    "title": "6. Associations",
    "section": "Let’s get started with summarizing associations!",
    "text": "Let’s get started with summarizing associations!\nThe following sections introduce how to summarize associations between variables by:\n\nDescribing associations between a categorical explanatory and numeric response variable including differences in conditional means, and Cohen’s D, and tools for visualizing associations between a categorical explanatory variable and a numeric response.\n\nDescribing associations between two categorical variables, including differences in conditional proportions, and the covariance.\n\nDescribing associations between two continuous variables, including the covariance and the correlation.\n\nThen we summarize the chapter, present practice questions, a glossary, a review of R functions and R packages introduced, and present additional resources.\nLuckily, these summaries are remarkably similar, so much of the learning in each section of this chapter reinforces what was learned in the others.",
    "crumbs": [
      "6. Associations"
    ]
  },
  {
    "objectID": "book_sections/associations/cat_cont.html",
    "href": "book_sections/associations/cat_cont.html",
    "title": "• 6. Categorical + numeric",
    "section": "",
    "text": "Summarizing associations: Difference in conditional means\nWe might expect that parviflora plants known to have attracted a pollinator would produce more hybrid seeds than those that were not. After all, pollen must be transferred for hybridization to occur, and visits from pollinators are the main way this happens. That seems biologically reasonable — but in statistics, such expectations must be tested with actual data.\nIn this section, we explore how to visualize and quantify associations between a categorical explanatory variable (e.g., whether a plant was visited by a pollinator) and a numeric response variable (e.g., the proportion of that plant’s seeds that are hybrids). We’ll see how group means and other summaries can reveal patterns in the data — and how to interpret what those patterns might mean biologically.\nIn statistics, we often differentiate between a\nFor example, for our RIL data planted at site GC, the “grand mean” proportion of hybrids formed across all RILs is around 0.15. .\nThe mean proportion of hybrids formed across all RILs is around 0.15.\ngc_rils |&gt;\n  summarise(grand_mean_prop_hybrid = mean(prop_hybrid, na.rm =TRUE))\ngrand_mean_prop_hybrid\n\n\n\n\n0.1505776\nSimilarly, the means of prop_hybrid conditional on visitation status are around 0.07 for flowers that “weren’t visited” and 0.36 for those that were visited.\ngc_rils |&gt;\n  group_by(visited)|&gt;\n  summarise(conditional_mean_prop_hybrid = mean(prop_hybrid, na.rm =TRUE))\nvisited\nconditional_mean_prop_hybrid\n\n\n\n\nFALSE\n0.0737613\n\n\nTRUE\n0.3611111\nA common summary of the association between a categorical explanatory variable and a numerical response is the difference in conditional means across groups. In this case, the difference in conditional means is approximately 0.29.\nmean_visited &lt;- gc_rils |&gt;\n  filter(visited)|&gt;\n  summarise(grand_mean_prop_hybrid = mean(prop_hybrid, na.rm =TRUE))\n\nmean_notvisited &lt;- gc_rils |&gt;\n  filter(!visited)|&gt;\n  summarise(grand_mean_prop_hybrid = mean(prop_hybrid, na.rm =TRUE))\n\n(mean_visited   - mean_notvisited) |&gt; pull() |&gt; round(digits = 3)\n\n[1] 0.287",
    "crumbs": [
      "6. Associations",
      "• 6. Categorical + numeric"
    ]
  },
  {
    "objectID": "book_sections/associations/cat_cont.html#cat_cont_summarizing-associations-difference-in-conditional-means",
    "href": "book_sections/associations/cat_cont.html#cat_cont_summarizing-associations-difference-in-conditional-means",
    "title": "• 6. Categorical + numeric",
    "section": "",
    "text": "Grand mean: The overall mean of a variable. and the\nConditional mean: The mean of one variable given the value of one (or more) other variables. With a single categorical variable, this is simply the group means.",
    "crumbs": [
      "6. Associations",
      "• 6. Categorical + numeric"
    ]
  },
  {
    "objectID": "book_sections/associations/cat_cont.html#cat_cont_summarizing-associations-cohens-d",
    "href": "book_sections/associations/cat_cont.html#cat_cont_summarizing-associations-cohens-d",
    "title": "• 6. Categorical + numeric",
    "section": "Summarizing associations: Cohen’s D",
    "text": "Summarizing associations: Cohen’s D\nAbove, we found that on average, visited plants produced 0.287 more hybrids than unvisited ones. That might seem like a big difference — but raw differences can be hard to interpret on their own. Is 0.287 a lot? A little? To better understand how meaningful that difference is, we can compare it to the variability in hybrid seed production. Cohen’s D helps us do just that — it standardizes the difference in means by the variability within groups (the pooled standard deviation), allowing for more intuitive comparisons across studies and systems. For this dataset, that gives us a D of 1.4 (see calculation below) — a very large effect size (see guide in margin). This suggests that being visited (during our observation window) is strongly associated with producing more hybrid seeds.\n\\[\\text{Cohen's D} = \\frac{\\text{Difference in means}}{\\text{pooled sd}}\\]\n\n\nThere aren’t hard and fast rules for interpreting Cohen’s D — this varies by field — but the rough guidelines are presented below. Our observed Cohen’s D of 1.4 is very large.\n\n\n\nSize\nRange of Cohen’s D\n\n\n\n\nNot worth reporting\n&lt; 0.01\n\n\nTiny\n0.01 – 0.20\n\n\nSmall\n0.20 – 0.50\n\n\nMedium\n0.50 – 0.80\n\n\nLarge\n0.80 – 1.20\n\n\nVery large\n1.20 – 2.00\n\n\nHuge\n&gt; 2.00\n\n\n\n\nCohen’s D - the difference in group means divided by the “pooled standard deviation” allows us to better interpret such difference.\n\nThe pooled standard deviation is simply the standard deviation of observations from their group mean. We can find it in R as follows:\n\n\n# finding the pooled standard deviation\npooled_sd &lt;- gc_rils |&gt;\n  group_by(visited)|&gt;\n  mutate(diff_from_mean = prop_hybrid - mean(prop_hybrid) )|&gt;\n  ungroup()|&gt;\n  summarise(sd_group = sd(diff_from_mean)) |&gt;\n  pull()\n\n# Print this out\nsprintf(\"The pooled sd is %s\",round(pooled_sd, digits = 3))\n\n[1] \"The pooled sd is 0.2\"\n\ncohensD &lt;- (mean_visited   - mean_notvisited) /  pooled_sd \n\n# Print this out\nsprintf(\"Cohen's D is (%s - %s)/(%s) = %s\",\n        round(mean_visited, digits = 3),\n        round(mean_notvisited   , digits = 3),\n        round(pooled_sd , digits = 3),\n        round(cohensD , digits = 3)\n        )\n\n[1] \"Cohen's D is (0.361 - 0.074)/(0.2) = 1.439\"",
    "crumbs": [
      "6. Associations",
      "• 6. Categorical + numeric"
    ]
  },
  {
    "objectID": "book_sections/associations/cat_cont.html#cat_cont_visualizing-a-categorical-x-and-numeric-y",
    "href": "book_sections/associations/cat_cont.html#cat_cont_visualizing-a-categorical-x-and-numeric-y",
    "title": "• 6. Categorical + numeric",
    "section": "Visualizing a categorical x and numeric y",
    "text": "Visualizing a categorical x and numeric y\nVisualizing the difference between means is surprisingly difficult.Visualizing the difference between means is surprisingly difficult. One particular concern is overplotting — because categorical variables have only a few possible values on the x-axis, data points can stack or overlap, which can obscure patterns in the data.\nBelow I work through a brief slide show revealing some challenges and some solutions.",
    "crumbs": [
      "6. Associations",
      "• 6. Categorical + numeric"
    ]
  },
  {
    "objectID": "book_sections/associations/two_categorical_vars.html",
    "href": "book_sections/associations/two_categorical_vars.html",
    "title": "• 6. Two categorical vars",
    "section": "",
    "text": "Unconditional proportions\nFigure 1: The proportion of plats that did (light grey) or did not (black) receive a visit from a pollinator.\nBefore describing associations between categorical variables, let us revisit our univariate summaries. The proportion of pink flowers (or plants receiving visits) is simply the number of pink flowers (or plants receiving visits) divided by the total number of plants whose petal color is known (or the number of plants with pollinator observation data).\nA proportion is essentially a mean where one outcome (e.g., pink flowers or being visited) is set to 1 and the other (e.g., white flowers or not being visited) is set to 0. Because visited is logical and R converts TRUE to 1 and FALSE to 0 when making a logical numeric, we can find this with the mean() function in R:\ngc_rils |&gt;\n  filter(!is.na(petal_color) & !is.na(mean_visits))|&gt;     # remove NAs\n  summarise(n_pink       = sum(as.numeric(petal_color == \"pink\")),\n            n_visited    = sum(as.numeric(mean_visits &gt; 0)),\n            n            = n(),\n            prop_pink    = mean(as.numeric(petal_color == \"pink\")),\n            prop_visited = mean(as.numeric(visited))) |&gt; \n  kable(digits = 4)                # for pretty formatting (ignore)\n\n\n\n\nn_pink\nn_visited\nn\nprop_pink\nprop_visited\n\n\n\n\n49\n24\n91\n0.5385\n0.2637\nSo our unconditional proportions – that is, our proportions without considering the other variable are:",
    "crumbs": [
      "6. Associations",
      "• 6. Two categorical vars"
    ]
  },
  {
    "objectID": "book_sections/associations/two_categorical_vars.html#unconditional-proportions",
    "href": "book_sections/associations/two_categorical_vars.html#unconditional-proportions",
    "title": "• 6. Two categorical vars",
    "section": "",
    "text": "Proportion pink-flowered, \\(P_\\text{pink} = \\frac{49}{91} = 0.5385\\).\n\nProportion visited, \\(P_\\text{visited} = \\frac{24}{91} = 0.2637\\).",
    "crumbs": [
      "6. Associations",
      "• 6. Two categorical vars"
    ]
  },
  {
    "objectID": "book_sections/associations/two_categorical_vars.html#associations-between-categorical-variables",
    "href": "book_sections/associations/two_categorical_vars.html#associations-between-categorical-variables",
    "title": "• 6. Two categorical vars",
    "section": "Associations between categorical variables",
    "text": "Associations between categorical variables\n\n\n\n\n\n\n\n\n\nFigure 2: The association between petal color and pollinator visitation. Petal color is on the x-axis and visit status is shown within bars. We see that pink-flowered plants are more likely to receive a visit from a pollinator.\n\n\n\n\nFigure 2 clearly shows that pink-flowered parviflora RILs planted at GC are more likely to be visited by a pollinator than are white-flowered RILs. At least, this is clear to me! Examine the plot yourself and\n\nArticulate what feature of this plot shows the point above,\n\nDescribe what the plot would look like if there was no association between petal color and pollinator visitation.\n\n\n\n\n\n\nA basic expectation from probability theory is that if two binary variables are independent (i.e. there is no statistical association between them), then the proportion of observations with both A and B equals the product of their proportions:\n\\[P_{AB|\\text{independence}} = P_{A} \\times P_{B}\\]\n\n\nThis is called the multiplication rule.\nTo evaluate how far our data deviate from independence, we can compare the actual joint proportion to what we’d expect if the two variables were independent. Assuming independence, \\(\\frac{49}{91}\\times \\frac{24}{91} =  0.142\\) of plants would be pink-flowered and receive a visit from a pollinator, but in reality nearly a quarter of our plants are pink-flowered and visited!\nTwo standard summaries of associations between categorical variables – conditional proportions and covariance – can be used to quantify this deviation from expectations under independence.\n\n\nDeviation from Independence: Conditional proportions\nPerhaps the most straightforward summary of an association between categorical variables is the conditional proportion. This is the proportion of a given outcome, calculated separately for each value of the explanatory variable. Because a proportion is essentially a mean, a conditional proportion is essentially a conditional mean – so this calculation and logic follows that in the previous section.\nAs we see in our example, this calculation uses the same approach as above, but grouped by petal color.\n\ngc_rils |&gt;\n  filter(!is.na(petal_color) & !is.na(visited))|&gt;     # remove NAs\n  group_by(petal_color)                            |&gt;\n  summarise(n_visited    = sum(as.numeric(visited)),\n            n            = n(),\n            prop_visited = mean(as.numeric(visited))) |&gt; \n  kable(digits = 4)                # for pretty formatting (ignore)\n\n\n\n\npetal_color\nn_visited\nn\nprop_visited\n\n\n\n\npink\n22\n49\n0.4490\n\n\nwhite\n2\n42\n0.0476\n\n\n\n\n\nBy conventional notation, we write conditional proportions as \\(P_\\text{A|B}\\), meaning ‘the proportion of A given B’ — where A is the outcome, B is the explanatory variable, and \\(|\\) means “given”. So:\n\n\nIn probability theory \\(|\\) means “given”. In R \\(|\\) means “or”. This is unfortunate and I am sorry. I wish I could change this.\n\nThe proportion of pink flowers receiving a visit from a pollinator is: \\(P_\\text{visited|pink} = \\frac{22}{49} = 0.449\\).\n\nThe proportion of white flowers receiving a visit from a pollinator is: \\(P_\\text{visited|white} = \\frac{2}{42} = 0.0476\\).\n\nIn summary, pink-flowered plants at site GC are roughly 10 times more likely to attract a pollinator than are white-flowered plants!\nWith these conditional proportions we can generalize the multiplication rule to non-independent variables:\n\\[P_{AB} = P_{B} \\times P{A|B}\\]\nApplying this to our case recovers our actual observations!\n\\[P_\\text{pink and visited} = P_\\text{pink} \\times P_\\text{visited|pink} = \\frac{49}{91}\\times \\frac{22}{49} =\\frac{22}{91} = 0.24\\]\n\n\nDividing the conditional proportions for two groups is technically called the “relative risk”.\n\n\n\nDeviations from Independence - The Covariance\nOur final summary of the association between categorical variables is the covariance. There are two ways to calculate the covariance. For now, we focus on the simplest way, and revisit this in the next section.\n\n\nThis formula is slightly wrong because it implicitly has a denominator of \\(n\\), not \\(n-1\\). To get the precise covariance, multiply this by \\(\\frac{n}{n-1}\\) (this is known as Bessel’s correction). But when \\(n\\) is big, this is close enough.\n\nThe first estimate of the covariance is the difference between observations and expectations under independence – i.e. \\(\\text{Covariance}_{A,B} = P_{AB}-P_{A} \\times P_{B}\\).\n\n\ngc_rils |&gt;\n  filter(!is.na(petal_color) & !is.na(visited))|&gt;     # remove NAs\n  mutate(pink_and_visited = petal_color == \"pink\" & visited,\n         pink_and_visited01 = as.numeric(pink_and_visited ))|&gt;    # observed joint proportion\n  summarise(n            = n(),\n            prop_pink_and_visited = mean(pink_and_visited01),\n            prop_visited = mean(as.numeric(visited)),\n            prop_pink    = mean(as.numeric(petal_color == \"pink\")),\n            indep_expect = prop_visited* prop_pink,\n            approx_cov   = prop_pink_and_visited - indep_expect,\n            exact_cov    = approx_cov * n / (n-1),\n            cov_function = cov(as.numeric(petal_color == \"pink\"), \n                               as.numeric(visited)))|&gt;\n  kable(digits = 4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nn\nprop_pink_and_visited\nprop_visited\nprop_pink\nindep_expect\napprox_cov\nexact_cov\ncov_function\n\n\n\n\n91\n0.2418\n0.2637\n0.5385\n0.142\n0.0997\n0.1009\n0.1009\n\n\n\n\n\n\n\nNote that the code above introduced R’s cov() function to find the covariance.\nCovariance gives us a numerical measure of how far our data deviate from what we’d expect under independence. In this case, the value is 0.10 — but is that meaningful? We’ll build up more intuition for interpreting covariances as we shift to continuous variables in the next section.\n\n\nAdditional summaries of associations between categorical variables.\nAt this point many textbooks would introduce two other standard summaries – odds ratios and relative risk (calculated above). I am not spending much time on them here. That is not because they are not useful (they are) – but because\n\nThey can get complicated.\n\nThey don’t lead naturally to the next steps in our learning journey.\n\nFeel free to read more about each on Wikipedia (links above) or in conversation with your favorite large language model.",
    "crumbs": [
      "6. Associations",
      "• 6. Two categorical vars"
    ]
  },
  {
    "objectID": "book_sections/associations/cont_cont.html",
    "href": "book_sections/associations/cont_cont.html",
    "title": "• 6. Two numeric vars",
    "section": "",
    "text": "The covariance\nThe covariance can also be used to describe the association between two numeric variables. For example, in our Clarkia RIL data, we could describe the association between \\(\\text{log}_{10}\\) petal area and the proportion of hybrid seeds using a covariance. As with two categorical variables, the covariance between two numeric variables reflects how much the observed association differs from what we’d expect if the variables were independent. There are two ways to calculate covariance — I introduce both here because each provides a different lens for understanding the concept, and each connects deeply to core ideas in statistics.",
    "crumbs": [
      "6. Associations",
      "• 6. Two numeric vars"
    ]
  },
  {
    "objectID": "book_sections/associations/cont_cont.html#cont_cont_the-covariance",
    "href": "book_sections/associations/cont_cont.html#cont_cont_the-covariance",
    "title": "• 6. Two numeric vars",
    "section": "",
    "text": "The covariance as a deviation from expectations\nIn the previous section, I introduced the covariance as the difference between the proportion of observations with a specific pair of values for two variables (e.g., pink flowers and being visited by a pollinator) and how frequently we would expect to see this pairing if the variables were independent: \\(\\text{Covariance}_{A,B} = (P_{AB}-P_{A} \\times P_{B})\\). Because we can think of proportions as a mean, we can use this same math to describe the covariance of two numeric variables, X and Y, as the difference between the mean of the products and the product of the means:\n\\[\\text{Covariance}_{X,Y} = (\\overline{XY}-\\overline{X} \\times \\overline{Y})\\]\n\n\nAs in the previous section this formula is slightly wrong because it implicitly has a denominator of \\(n\\), not \\(n-1\\). We apply Bessel’s correction to get the precise covariance (multiplying our answer by \\(\\frac{n}{n-1}\\)). But when \\(n\\) is big, this is close enough.\nSo, we can find the covariance between (\\(\\text{log}_{10}\\)) petal area and the proportion of hybrid seeds as the mean of a plant’s (\\(\\text{log}_{10}\\)) petal area times its proportion of hybrid seeds minus the mean (\\(\\text{log}_{10}\\)) petal area times the mean proportion of hybrid seeds, which equals 0.00756 (after applying Bessel’s correction).\n\ngc_rils |&gt;\n  filter(!is.na(log10_petal_area_mm), !is.na(prop_hybrid))|&gt;\n  summarise(mean_product        = mean(log10_petal_area_mm * prop_hybrid),\n            product_of_mean     =  mean(log10_petal_area_mm) * mean(prop_hybrid),\n            approx_covariance   = mean_product - product_of_mean,\n            actual_covariance   = approx_covariance * n() /(n() - 1))\n\n\n\n\nThe covariance as the mean of cross products\nAlternatively, we can think of the covariance as how far an individual’s value of X and Y jointly differ from their means. In this formulation,\n\nFind the deviation of X and Y from their means for each individual– \\((X_i-\\overline{X})\\), and \\((Y_i-\\overline{Y})\\), respectively (Figure 1, left).\n\nTake the product of these values to find the cross product (the area of a given rectangle in Figure 1, left).\n\nSum them to find the sum of cross products (Figure 1, right, top).\n\nDivide by the sample size minus one (Figure 1, right, bottom).\n\n\n\nThe equation for the covariance \\(\\text{Cov}_{X,Y} = \\frac{\\Sigma{(X_i-\\overline{X})(Y_i-\\overline{Y})}}{(n-1)}\\) should remind you of the equation for the variance \\(\\text{Var}_{X} = \\frac{\\Sigma{(X_i-\\overline{X})(X_i-\\overline{X})}}{(n-1)}\\) (compare Figure 1 to Figure 2 from 5. Summarizing variability). In fact the variance is simply the covariance of a variable with itself. See our section on summarizing variability for a refresher link. In fact you, can calcualte the variance as the mean of the square minus the sqaure of the mean.\nThis essentially finds the mean cross products, with Bessel’s correction:\n\\[\\text{Covariance }_{X,Y} = \\frac{\\Sigma{(X_i-\\overline{X})(Y_i-\\overline{Y})}}{(n-1)}\\]\n\n\n\n\n\n\n\n\nFigure 1: An animation to help understand the covariance. Left: We plot each point as the difference between x and y and their means. The area of that rectangle is the cross product. Middle: Shows how these cross products accumulate. Right: The cummulative sum of cross products and the running covariance estimate. The lower plot (covariance) is simply the top plot divided by (x-1).\n\n\n\n\n\nThe flipbook below works you through how to conduct these calculations:\n\n\n\n\n\n\nThe cov() function\nBoth ways of computing the covariance — as the mean of cross-products and as the difference between the product of means and mean of products — are helpful for understanding association. But students are practical and often ask: “Which of these formulae should we use to calculate the covariance?” There are a few answers to this question — the first is “it depends,” the second is “whichever you like,” and the third is “neither, just use the cov() function in R.” Here’s how:\n\n\nThe use = \"pairwise.complete.obs\" argument tells R to ignore NA values when calculating the covariance — just like na.rm = TRUE does when calculating the mean. You can use this argument or filter out NA values first.\n\ngc_rils |&gt;\n  summarise(covariance = cov(log10_petal_area_mm, prop_hybrid, use = \"pairwise.complete.obs\"))\n\n# A tibble: 1 × 1\n  covariance\n       &lt;dbl&gt;\n1    0.00756",
    "crumbs": [
      "6. Associations",
      "• 6. Two numeric vars"
    ]
  },
  {
    "objectID": "book_sections/associations/cont_cont.html#cont_cont_the-correlation",
    "href": "book_sections/associations/cont_cont.html#cont_cont_the-correlation",
    "title": "• 6. Two numeric vars",
    "section": "The correlation",
    "text": "The correlation\nMuch like the variance and the difference in means, the covariance is a very useful mathematical description, but its biological meaning can be difficult to interpret and communicate. We therefore usually present the correlation coefficient (represented by the letter, r) – a summary of the strength and direction of a linear association between two variables. This also corresponds to how closely the points fall along a straight line in a scatterplot: the stronger the correlation, the more the points cluster along a line (positive or negative).\n\nLarge absolute values of r indicate that we can quite accurately predict one variable from the other (i.e. points are near a line on a scatterplot).\nr values near zero mean that we cannot accurately predict values of one variable from another (i.e. points are not near a line on a scatterplot).\n\nThe sign of r describes if the values increase with each other (\\(r &gt; 0\\), a positive slope), or if one variable decreases as the other increases ($ r &lt; 0$, a negative slope).\n\nMathematically r is simply the covariance divided by the product of standard deviations (\\(s_X\\) and \\(s_Y\\)), and we can find it in R with the cor() function:\n\\[r_{X,Y} = \\frac{\\text{Covariance}_{X,Y}}{s_X \\times s_Y}\\]\n\ngc_rils |&gt;\n  filter(!is.na(log10_petal_area_mm), !is.na(prop_hybrid))|&gt;\n  summarise(covariance   = cov(log10_petal_area_mm, prop_hybrid),\n            cor_from_cov = covariance / (sd(log10_petal_area_mm) * sd(prop_hybrid)),\n            cor_from_function = cor(log10_petal_area_mm, prop_hybrid))\n\n# A tibble: 1 × 3\n  covariance cor_from_cov cor_from_function\n       &lt;dbl&gt;        &lt;dbl&gt;             &lt;dbl&gt;\n1    0.00756        0.317             0.317\n\n\n\n\n\n\n\nSize\nRange of \\(|r|\\)\n\n\n\n\nNot worth reporting\n&lt; 0.005\n\n\nTiny\n0.005 – 0.10\n\n\nSmall\n0.01 – 0.20\n\n\nMedium\n0.2 – 0.35\n\n\nLarge\n0.35 – 0.50\n\n\nVery large\n0.50 – 0.75\n\n\nHuge\n\\(&gt; 0.75\\)\n\n\n\nAs in Cohen’s D, what is a “large” or “small” correlation coefficient depends on the study, the question and the field of study, but there are rough guides (see table on right). So our observed correlation between \\(log_{10}\\) petal area and proportion hybrid is worth paying attention to, but not massive.",
    "crumbs": [
      "6. Associations",
      "• 6. Two numeric vars"
    ]
  },
  {
    "objectID": "book_sections/associations/cont_cont.html#coming-up-next",
    "href": "book_sections/associations/cont_cont.html#coming-up-next",
    "title": "• 6. Two numeric vars",
    "section": "Coming up next",
    "text": "Coming up next\nThese summaries — covariance and correlation — give us tools to describe how two numeric variables relate. Later, we’ll return to these ideas in the context of linear models, where we formalize the idea of one variable predicting another.",
    "crumbs": [
      "6. Associations",
      "• 6. Two numeric vars"
    ]
  },
  {
    "objectID": "book_sections/associations/summarizing_associations.html",
    "href": "book_sections/associations/summarizing_associations.html",
    "title": "• 6. Association Summary",
    "section": "",
    "text": "Chapter Summary\nLinks to: Summary. Chatbot tutor. Questions. Glossary. R functions. R packages. More resources.\nA cartoon on correlation from xkcd. The original rollover text says: “Correlation doesn’t imply causation, but it does waggle its eyebrows suggestively and gesture furtively while mouthing look over there”. See this link for a more detailed explanation.\nAssociations reveal how variables relate to one another - e.g. if they tend to increase together, differ across groups, or cluster. Differences in conditional means (or proportions) describe how a numeric (or categorical) response variable varies across levels of a categorical explanatory variable. For two numeric variables, covariance captures how deviations from their means align, and correlation standardizes this to a unitless scale between -1 and 1. While these summaries can highlight patterns, interpretation requires care: strong associations don’t necessarily imply causation, and predictions may not hold across contexts or datasets.",
    "crumbs": [
      "6. Associations",
      "• 6. Association Summary"
    ]
  },
  {
    "objectID": "book_sections/associations/summarizing_associations.html#summarizing_associations_chapter-summary",
    "href": "book_sections/associations/summarizing_associations.html#summarizing_associations_chapter-summary",
    "title": "• 6. Association Summary",
    "section": "",
    "text": "Chatbot tutor\n\nPlease interact with this custom chatbot (link here) I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you got what you needed from it.",
    "crumbs": [
      "6. Associations",
      "• 6. Association Summary"
    ]
  },
  {
    "objectID": "book_sections/associations/summarizing_associations.html#summarizing_associations_practice-questions",
    "href": "book_sections/associations/summarizing_associations.html#summarizing_associations_practice-questions",
    "title": "• 6. Association Summary",
    "section": "Practice Questions",
    "text": "Practice Questions\nTry these questions! By using the R environment you can work without leaving this “book”. To help you jump right into thinking and analysis, I have loaded the ril data, cleaned it some, an have started some of the code!\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ1) Extend the analysis above to examine the association between leaf water content (lwc) and the proportion of hybrid seeds (prop_hybrid). The correlation between lwc and prop_hybrid is: -0.202-0.203-0.000914-0.000403\nQ2) Based on the analysis above, which variable – leaf water content (lwc), or petal area (log10_petal_area_mm) is more plausibly interpreted as influencing proportion hybrid seed set (prop_hybrid)?\n\n Equally likely — because the absolute values of their correlation coefficients are similar Petal area — because it has the stronger correlation coefficient Neither — the covariances are both near zero Petal area There is a substantial association, and because these are experimental RILs, it's plausible that pollinators are attracted to larger petals — not low leaf water content. There is no relevant information here — correlation does not imply causation\n\nQ3) Based on the observed negative association between leaf water content and proportion hybrid seed set, which explanation best accounts for this pattern?\n\n Chance — strange associations sometimes appear randomly. Reverse causation — pollinator visits might reduce leaf water content. A direct causal link — pollinators are attracted to plants with dry leaves. Confounding — low leaf water content might be genetically or physiologically linked with a trait that influences pollinator attraction (e.g., it might be negatively associated with petal area) and ultimately hybrid seed set.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nThe set of questions below focuses on comparing the association between petal color and pollinator visitation to the association between petal color and proportion hybrid seed. Use the webR console above to work through these!\nQ4) The difference in conditional mean hybrid proportion between pink and white flowers is: \nQ5) The pooled standard deviation of hybrid proportion between pink and white flowers is: 0.2320.1930.214\nQ6) Which trait is more strongly associated with petal color — the proportion of hybrid seeds or visits from a pollinator (visits)?\n\n Pollinator visits — Pink flowers had about 0.6 more visits than white flowers, but only about 0.18 greater proportion of hybrid seeds. Hybrid seeds — Pink flowers had about 4.7 times as many hybrid seeds as white flowers, but only about 2.6 times as many visits. Hybrid seeds — Cohen’s D for the relationship between petal color and proportion hybrid seeds was large, while Cohen’s D for the relationship between petal color and visits was medium. You cannot compare strength of associations when the response variables are measured on different scales.\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ7 SETUP We collected 131 plants (74 parviflora, 57 xantiana) from a natural hybrid zone between xantiana and parviflora at Sawmill Road. We then genotyped these plants at a chloroplast marker that distinguishes between chloroplasts originating from parviflora and xantiana. All 74 parviflora plants had a parviflora chloroplast, while 49 of the 57 xantiana plants had a xantiana chloroplast (the remaining 8 had a parviflora chloroplast).\nQ7A) If having a xantiana chloroplast and being a xantiana plant were independent, what proportion of plants would you expect to be xantiana and have a xantiana chloroplast? \n\n\nRefresher on the multiplication rule for independent events\n\nIf two binary variables are independent, the expected joint proportion (i.e. the probability of A and B) is the product of their proportions:\n\\[ P(A \\text{ and } B) = P(A) \\times P(B) \\]\n\nQ7B) Quantify the difference between the proportion of plants that are xantiana and have xantiana chloroplasts vs. what we expect if these two binary variables were independent. \nQ7C) What is the covariance between being a xantiana plant and having a xantiana chloroplast? Hint: remember Bessel’s correction. \n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ8) In the code above, I calculated the correlation and covariance between lwc and prop_hybrid using their mathematical formulas. However, my calculated values don’t match those returned by cor(). Why not?\n\n The manual method failed to remove all rows with missing values — while cov() and cor() used pairwise.complete.obs, the custom code did not. There is a mistake in the correlation formula — the covariance should be divided by the product of the means, not the standard deviations. R sometimes has the wrong formulae – that's why I always type the formula's in myself. The standard deviations used were incorrect because sd() doesn't apply Bessel's correction. The discrepancy is due to numerical precision — it's expected and not worth worrying about.\n\n\n\n\n\n\n\n\n\n\n\n\nQ9 SETUP Consider the plots above\nQ9A) In which plot are x and y most tightly associated? abcd\nQ9B) In which plot are x and y most tightly linearly associated? abcd\nQ9C) In which plot do x and y have the largest correlation coefficient? abcd\nQ9C) In which plot are does x do the worst job of predicting y? abcd",
    "crumbs": [
      "6. Associations",
      "• 6. Association Summary"
    ]
  },
  {
    "objectID": "book_sections/associations/summarizing_associations.html#summarizing_associations_glossary-of-terms",
    "href": "book_sections/associations/summarizing_associations.html#summarizing_associations_glossary-of-terms",
    "title": "• 6. Association Summary",
    "section": "📊 Glossary of Terms",
    "text": "📊 Glossary of Terms\n\n\n🔗 1. Types of Association\n\nAssociation: A relationship or pattern between two variables, without assuming causation.\nCorrelation: A numerical summary of how two variables move together.\n\nPositive: As one increases, the other tends to increase.\n\nNegative: As one increases, the other tends to decrease.\n\n\nCausation: A relationship in which changes in one variable directly produce changes in another.\n\n\n\n\n⚖️ 2. Categorical Associations\n\nConditional Proportion: The proportion of a category (e.g., visited flowers) within levels of another variable (e.g., pink or white petals).\n\nWritten as \\(P(A|B)\\), the probability of A given B.\n\nMultiplication Rule: If two variables are independent, then \\(P(A \\text{ and } B) = P(A) \\times P(B)\\).\nRelative Risk: The ratio of conditional proportions between two groups.\nConfounding Variable: A third variable that creates a false appearance of association between two others.\n\n\n\n\n🔢 3. Numeric Associations\n\nCovariance (cov()): Measures how two numeric variables co-vary.\n\nPositive: variables increase together.\n\nNegative: one increases as the other decreases.\n\nSensitive to scale.\n\nCross Product: For two variables, the product of their deviations from their means:\n\\((X_i - \\bar{X})(Y_i - \\bar{Y})\\)\nCorrelation Coefficient (cor()): A unitless summary of linear association, ranging from -1 to 1.\n\\(r = \\frac{\\text{Cov}_{X,Y}}{s_X s_Y}\\)\n\nr ≈ 0: No linear relationship\n\nr &gt; 0: Positive linear relationship\n\nr &lt; 0: Negative linear relationship\n\n\n\n\n\n📏 4. Comparing Group Means\n\nConditional Mean: The average of a numeric variable within each group of a categorical variable.\nDifference in Means: A common summary of how a numeric variable differs across groups.\nCohen’s D: Standardized difference between two group means.\n\\(D = \\frac{\\bar{X}_1 - \\bar{X}_2}{s_{pooled}}\\)\nPooled Standard Deviation: A weighted average of within-group standard deviations, used in Cohen’s D.\n\n\n\n\n📈 5. Visual Summaries of Associations\n\nScatterplot: Plots individual observations for two numeric variables. Good for spotting trends and calculating correlation.\nBoxplot: Shows distributions (medians, IQRs) across groups.\nBarplot of Conditional Proportions: Visualizes proportions of one categorical variable within levels of another.\nSina Plot: A jittered density-style plot used to show distributions of numeric values within categories, especially useful when overplotting is an issue.",
    "crumbs": [
      "6. Associations",
      "• 6. Association Summary"
    ]
  },
  {
    "objectID": "book_sections/associations/summarizing_associations.html#summarizing_associations_key-r-functions",
    "href": "book_sections/associations/summarizing_associations.html#summarizing_associations_key-r-functions",
    "title": "• 6. Association Summary",
    "section": "Key R Functions",
    "text": "Key R Functions\n\n\n📊 Visualizing Associations\n\nstat_summary(): Adds summary statistics like means and error bars to plots.\ngeom_smooth(): Adds a trend line to scatterplots.\n\n\n\n\n📈 Summarizing Associations Between Variables\n\ngroup_by() ([dplyr]): Groups data for grouped summaries like conditional proportions or means.\nsummarise() ([dplyr]): Summarizes multiple rows into a single value, e.g., a mean, covariance, or correlation.\n\nmean() ([base R]): Computes means (or proportions). In this chapter we combine this with group_by() to find conditional means (or conditional proportions).\ncov(): Calculates covariance between two numeric variables.\ncor(): Calculates the correlation coefficient.\n\nWe often combine these below with the following chain of operations.\n- For conditional means: data|&gt;group_by()|&gt;summarize(mean()).\n- For associations: data |&gt;group_by()|&gt;summarize(cor()).",
    "crumbs": [
      "6. Associations",
      "• 6. Association Summary"
    ]
  },
  {
    "objectID": "book_sections/associations/summarizing_associations.html#summarizing_associations_r-packages-introduced",
    "href": "book_sections/associations/summarizing_associations.html#summarizing_associations_r-packages-introduced",
    "title": "• 6. Association Summary",
    "section": "R Packages Introduced",
    "text": "R Packages Introduced\n\n\nGGally: Extends ggplot2 with convenient functions for exploring relationships among multiple variables. The ggpairs() function produces a matrix of plots showing pairwise associations, including histograms, scatterplots, and correlation coefficients.\nggforce: Provides advanced geoms for ggplot2. This chapter uses geom_sina() to reduce overplotting by jittering points while preserving density.",
    "crumbs": [
      "6. Associations",
      "• 6. Association Summary"
    ]
  },
  {
    "objectID": "book_sections/associations/summarizing_associations.html#summarizing_associations_additional-resources",
    "href": "book_sections/associations/summarizing_associations.html#summarizing_associations_additional-resources",
    "title": "• 6. Association Summary",
    "section": "Additional resources",
    "text": "Additional resources\n\nOther web resources:\n\nRegression, Fire, and Dangerous Things (1/3): A fantastic essay about challenges in going from correlation to causation.\n\nSpurious correlations: A humorous collection of weird correlations from the world.\n\nGuess the correlation: A fun video game in which you see a plot and must guess the correlation. This is great for building an intuition about the strength of a correlation.\n\nVideos:\n\nCorrelation Doesn’t Equal Causation: Crash Course Statistics #8.\nCalling Bullshit has a fantastic set of videos on correlation and causation.\n\nCorrelation and Causation: “Correlations are often used to make claims about causation. Be careful about the direction in which causality goes. For example: do food stamps cause poverty?”\n\nWhat are Correlations? :“Jevin providers an informal introduction to linear correlations.”\n\nSpurious Correlations?: “We look at Tyler Vigen’s silly examples of quantities appear to be correlated over time), and note that scientific studies may accidentally pick up on similarly meaningless relationships.”\n\nCorrelation Exercise” “When is correlation all you need, and causation is beside the point? Can you figure out which way causality goes for each of several correlations?”\nCommon Causes: “We explain how common causes can generate correlations between otherwise unrelated variables, and look at the correlational evidence that storks bring babies. We look at the need to think about multiple contributing causes. The fallacy of post hoc propter ergo hoc: the mistaken belief that if two events happen sequentially, the first must have caused the second.”\n\nManipulative Experiments: “We look at how manipulative experiments can be used to work out the direction of causation in correlated variables, and sum up the questions one should ask when presented with a correlation.",
    "crumbs": [
      "6. Associations",
      "• 6. Association Summary"
    ]
  },
  {
    "objectID": "book_sections/linear_models.html",
    "href": "book_sections/linear_models.html",
    "title": "7. Linear Models",
    "section": "",
    "text": "Statistical models are not scientific models\nIn this section, we’ll introduce statistical models as simplified descriptions of the world. You’ll notice that you’re already familiar with some simple models. For example, the mean and variance provide a basic model for a single numeric variable, while a conditional mean and pooled variance describe a numeric response variable with a categorical explanatory variable.\nThis is a BIOstatistics book. It is written by and for biologists interested in biological ideas. We are inspired by biological questions generated by scientific understanding and scientific models of the world. However, a major way in which we evaluate such scientific models is via statistical models and hypotheses. Confusing a statistical model for a scientific model is a common and understandable mistake that we should avoid. Scientific models and statistical models are different:\nBecause statistical models know nothing about science it is our job to build scientific studies best suited for clean statistical interpretation, build statistical models that best represent our biological questions, and interpret statistical results as statistical. We must always recenter biology in interpreting any statistical outcome.",
    "crumbs": [
      "7. Linear Models"
    ]
  },
  {
    "objectID": "book_sections/linear_models.html#statistical-models-are-not-scientific-models",
    "href": "book_sections/linear_models.html#statistical-models-are-not-scientific-models",
    "title": "7. Linear Models",
    "section": "",
    "text": "Scientific models are based on our understanding of the science – in this case biology. Biological models come from us making simplified abstractions of complex systems (like considering predator-prey interactions, plant pollination, cancer progression, or meiosis (Figure 1 A)). Great scientific models explain what we see, make interesting predictions, and are consistent with our broader scientific understanding.\nStatistical models on the other hand, are mathematical ways to describe patterns in data (e.g. Figure 1 B). Statistical models know nothing about Lotka-Voltera, pollination or human physiology.\n\n\n\nScientific & statistical models: The Clarkia case study\nIn our Clarkia example, the big-picture scientific model is that when parviflora came back into contact with its close relative, xantiana, it evolved traits — such as smaller petals — to avoid producing hybrids. No single statistical model or study fully captures this scientific model. Instead, we design experiments to evaluate pieces of the model. For example, we:\n\nCompare petal area between parviflora plants from populations that occur with xantiana (sympatric populations) and those from populations far away from xantiana (allopatric populations).\nConduct an experiment by planting individuals from sympatric and allopatric parviflora populations in the same environment as xantiana, and comparing the amount of hybrid seed set by plants from each origin.\nGenerate Recombinant Inbred Lines (RILs) between sympatric and allopatric parviflora populations, and examine whether petal area is associated with the proportion of hybrid seeds produced.\n\nAs you can see, statistical models don’t “know” anything about biology — they simply describe patterns in data. It’s up to us, as biologists, to design experiments carefully, choose the right statistical models, and interpret results in light of our biological hypotheses.",
    "crumbs": [
      "7. Linear Models"
    ]
  },
  {
    "objectID": "book_sections/linear_models.html#linear-models",
    "href": "book_sections/linear_models.html#linear-models",
    "title": "7. Linear Models",
    "section": "Linear models",
    "text": "Linear models\nLinear models are among the most common types of statistical model. Linear models estimate the conditional mean of the \\(i^{th}\\) observation of a continuous response variable, \\(\\hat{Y}_i\\) for a (combination) of value(s) of the explanatory variables (\\(\\text{explanatory variables}_i\\)):\n\\[\\begin{equation}\n\\hat{Y}_i = f(\\text{explanatory variables}_i)\n\\end{equation}\\]\n\n\nConditional mean: The expected value of a response variable given specific values of the explanatory variables (i.e., the model’s best guess for the response based on the explanatory variables).\nThese models are “linear” because we get this estimate of the conditional mean, \\(\\hat{Y}_i\\), by adding up all components of the model. That is, each explanatory variable \\(y_{j,i}\\) is multiplied by its effect size \\(b_j\\). So, for example, \\(\\hat{Y}_i\\) equals the parameter estimate for the “intercept”, \\(a\\) plus its value for the first explanatory variable, \\(y_{1,i}\\), times the effect of this variable, \\(b_1\\), plus its value for the second explanatory variable, \\(y_{2,i}\\) times the effect of this variable, \\(b_2\\), and so on for all included predictors.\n\\[\\begin{equation}\n\\hat{Y}_i = a + b_1  y_{1,i} + b_2 y_{2,i} + \\dots{}\n\\end{equation}\\]\n\nOPTIONAL / ADVANCED, FOR MATH NERDS:. If you have a background in linear algebra, it might help to see a linear model in matrix notation.\nThe first matrix below is known as the design matrix. Each row corresponds to an individual, and each entry in the \\(i\\)th row corresponds to that individual’s value for a given explanatory variable. We take the dot product of this matrix and our estimated parameters to get the predictions for each individual. The equation below has \\(n\\) individuals and \\(k\\) explanatory variables. Note that every individual has a value of 1 for the intercept.\n\\[\\begin{equation}\n\\begin{pmatrix}\n    1 & y_{1,1} & y_{2,1} & \\dots  & y_{k,1} \\\\\n    1 & y_{1,2} & y_{2,2} & \\dots  & y_{k,2} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    1 & y_{1,n} & y_{2,n} & \\dots  & y_{k,n}\n\\end{pmatrix}\n\\cdot\n\\begin{pmatrix}\n    a \\\\\n    b_{1}\\\\\n    b_{2}\\\\\n    \\vdots  \\\\\n     b_{k}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n    \\hat{Y}_1 \\\\\n    \\hat{Y}_2\\\\\n    \\vdots  \\\\\n    \\hat{Y}_n\n\\end{pmatrix}\n\\end{equation}\\]",
    "crumbs": [
      "7. Linear Models"
    ]
  },
  {
    "objectID": "book_sections/linear_models.html#residuals-and-variance-in-linear-models",
    "href": "book_sections/linear_models.html#residuals-and-variance-in-linear-models",
    "title": "7. Linear Models",
    "section": "Residuals and Variance in Linear Models",
    "text": "Residuals and Variance in Linear Models\nAs with a simple mean, actual observations usually differ from their predicted (conditional mean) values.\nThe difference between an observed value for an individual (\\(Y_i\\)) and the predicted value from the linear model (\\(\\hat{Y}_i\\)) is called the residual, \\(e_i\\):\n\n\nResidual: The difference between an observed value and its predicted value from a linear model (i.e., the conditional mean given that observation’s values for the explanatory variables).\n\\[e_i = Y_i - \\hat{Y}_i\\]\nYou can also rearrange this to think about it the other way around:\n\\[Y_i = \\hat{Y}_i + e_i\\]\nJust like when we summarized variability around a simple mean, we can summarize variability around a model’s predictions by looking at the spread of the residuals. That is, linear models don’t just give us a conditional mean — they also give us a way to estimate how far observations tend to vary around that mean.\nWe calculate the residual variance (and residual standard deviation) of the residuals like this:\n\\[\\text{Residual variance} = \\frac{ \\sum e_i^2}{n-1}\\]\n\n\nA small caveat: The equation for the residual variance is a bit off — the denominator should actually be \\(n - p\\), where \\(p\\) is the number of parameters estimated in our model (including the intercept). But it’s close enough for now. We can worry about being precise later — in much of statistics, being approximately right is good enough.  \nWe can use the residual variance to find the residual standard deviation: \\(\\text{Residual standard deviation} = \\sqrt{\\frac{\\sum e_i^2}{n-1}}\\).\nWe can think of the residual standard deviation as telling us, on average, how far away from their predicted value individuals are expected to be. In fact, when we calculated the pooled standard deviation earlier, we were already doing a special case of what we now call the residual standard deviation — just in the simple situation of two groups.",
    "crumbs": [
      "7. Linear Models"
    ]
  },
  {
    "objectID": "book_sections/linear_models.html#assumptions-caveats-and-our-limited-ambitions",
    "href": "book_sections/linear_models.html#assumptions-caveats-and-our-limited-ambitions",
    "title": "7. Linear Models",
    "section": "Assumptions, Caveats and our limited ambitions",
    "text": "Assumptions, Caveats and our limited ambitions\nIn this section, we focus on building and interpreting linear models as descriptions of data. We will put off a formal discussion of what makes a linear model reasonable — and how to diagnose whether a model fits its assumptions — until later. In the meantime, as we build and interpret models, we’ll keep an informal eye out for clues that something might be wrong — but we’ll learn formal tools to evaluate if a specific model meets assumptions of a linear model later.\nThis doesn’t mean that interpreting linear models is more important than evaluating whether they are any good (in fact, in real research, interpreting and evaluating models are inseparable). It just means it’s hard to teach a bunch of interconnected ideas and approaches all at once — and after years of experimentation, I’ve found this approach to be the most successful. That said, before you conduct any serious linear modeling work, you should definitely jump ahead and read PART 3 of this book, where we discuss model assumptions and how to check them carefully.",
    "crumbs": [
      "7. Linear Models"
    ]
  },
  {
    "objectID": "book_sections/linear_models.html#lets-get-started-introducing-linear-models-as-summaries",
    "href": "book_sections/linear_models.html#lets-get-started-introducing-linear-models-as-summaries",
    "title": "7. Linear Models",
    "section": "Let’s get started introducing linear models as summaries!",
    "text": "Let’s get started introducing linear models as summaries!\nThroughout this chapter we will model a single response variable – the proportion of hybrids that a mom produced. As you see below, much of this chapter recasts concepts you have already learned in the framework of a linear model.\n\nWe will start with the simplest linear model – the mean. While we have already introduced the mean, introducing it in the context of a linear model helps us prepare for our next steps!\nWe will then introduce linear models with an explanatory categorical predictor. Starting with a binary predictor (petal color), we will recast our understanding of a conditional mean in the context of a linear model. We will then expand to a categorical explanatory variable with multiple levels (location).\nNext we show how linear models can predict one numeric variable from another – in this case we predict proportion hybrid seed from petal area. Building on our understanding of covariance and correlation, we introduce the slope and intercept.\nFinally we introduce linear models with two predictors. Specifically, we extend beyond what we have learned so far - we build a linear model to predict a numeric response (proportion of hybrid seeds) from a categorical (petal color) and numeric (petal area) predictor.\n\nWe conclude (as usual) with a chapter summary, practice questions, a glossary, a review of R functions and R packages introduced, and present additional resources. On the whole, this chapter extends our ability to summarize data while preparing us for the more sophisticated linear modeling and hypothesis testing we will pursue later in the book.\n\n\n\n\nSuzuki, Y., Endo, M., Cañas, C., Ayora, S., Alonso, J. C., Sugiyama, H., & Takeyasu, K. (2014). Direct analysis of holliday junction resolving enzyme in a DNA origami nanostructure. Nucleic Acids Research, 42(11), 7421–7428. https://doi.org/10.1093/nar/gku320",
    "crumbs": [
      "7. Linear Models"
    ]
  },
  {
    "objectID": "book_sections/linear_models/mean.html",
    "href": "book_sections/linear_models/mean.html",
    "title": "• 7. Mean",
    "section": "",
    "text": "The mean again?\nWe are beginning our tour of interpreting linear models with the mean. We start with the mean, not because I doubt that you understand what a mean and variance are — I know that you know how to calculate the mean (\\(\\overline{y} = \\frac{\\sum y_i}{n}\\)) and the variance (\\(s^2 = \\frac{\\sum (y_i - \\overline{y})^2}{n-1}\\)). Instead, we are starting here because your solid understanding of these concepts will help you better understand linear models.\nIn a simple linear model with no predictors, the intercept is the mean and the only other term is the residual variation. So we predict the \\(i^{th}\\) individual’s value of the response variable to be:\n\\[\\hat{y}_i = b_0\\]\nWhere \\(b_0\\) is the intercept (i.e. the sample mean). In reality, of course, observations rarely match prediction perfectly. So an individual \\(i\\)’s actual value, \\(y_i\\) is its predicted value plus the difference between its observed and predicted value \\(e_i\\) (aka the residual).\n\\[{y}_i = b_0 + e_i\\]",
    "crumbs": [
      "7. Linear Models",
      "• 7. Mean"
    ]
  },
  {
    "objectID": "book_sections/linear_models/mean.html#the-mean-again",
    "href": "book_sections/linear_models/mean.html#the-mean-again",
    "title": "• 7. Mean",
    "section": "",
    "text": "“But we already had a section on the mean, and besides I’ve known what a mean was for years. Why another section on this?”\n\nYou, probably.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Mean"
    ]
  },
  {
    "objectID": "book_sections/linear_models/mean.html#the-lm-function-in-r",
    "href": "book_sections/linear_models/mean.html#the-lm-function-in-r",
    "title": "• 7. Mean",
    "section": "The lm() function in R",
    "text": "The lm() function in R\nIn R you build linear models with the lm() syntax:\nlm(response ~ explanatory1 + explanatory2 + ..., data = data_set). In a simple model with no predictors you type: lm(response ~ 1, data = data_set).\nSo to model the proportion of hybrid seed in GC with no explanatory variables, type:\n\n\n\n\n\n\n\n\n\nFigure 1: A bunch of Clarkia seeds. How many do you think are hybrids?\n\n\n\n\n\nlm(prop_hybrid ~ 1, data = gc_rils)\n\n\nCall:\nlm(formula = prop_hybrid ~ 1, data = gc_rils)\n\nCoefficients:\n(Intercept)  \n     0.1506  \n\n\nThe output gives us the estimated intercept — which, in this case with no predictors, is simply the mean (see above). The code below verifies this (except that R provides a different number of digits).\n\nsummarise(gc_rils, mean_p_hyb = mean(prop_hybrid, na.rm=TRUE))\n\n# A tibble: 1 × 1\n  mean_p_hyb\n       &lt;dbl&gt;\n1      0.151\n\n\nInterpretation This means that we model the \\(i^{th}\\) individual’s proportion of hybrid seed as the sample mean, 0.1506,\n\\[\\hat{Y}_i = 0.1506\\]\nand understand that its value will deviate from the sample mean by some amount, \\(e_i\\).\n\\[Y_i = 0.1506 + e_i\\]",
    "crumbs": [
      "7. Linear Models",
      "• 7. Mean"
    ]
  },
  {
    "objectID": "book_sections/linear_models/mean.html#residuals",
    "href": "book_sections/linear_models/mean.html#residuals",
    "title": "• 7. Mean",
    "section": "Residuals",
    "text": "Residuals\n\n\n\n\n\n\nRecall that the residual \\(e_i\\) is the difference between a data point and its value predicted from a model.\n\n\n\nWhat is \\(e_i\\) in the math above? It is the residual the distance between the predicted value of the \\(i^{th}\\) observation in a linear model, \\(\\hat{y}_i\\), and its actual value \\(y_i\\). Hovering over a point in Figure 2 reveals its residual value.\n\n\nCode\nlibrary(plotly)\nprop_hybrid_plot &lt;-  gc_rils                          |&gt;\n  filter(!is.na(prop_hybrid))                         |&gt;\n  mutate(i = 1:n(),\n         e_i = prop_hybrid - mean(prop_hybrid),\n         e_i = round(e_i, digits = 3),\n         y_hat_i = round(mean(prop_hybrid),digits = 3),\n         y_i = round(prop_hybrid, digits = 3))     |&gt;\n  ggplot(aes(x = i, y = y_i, y_hat_i = y_hat_i, e_i = e_i, color = i==3))+\n  geom_point(size = 4, alpha = .6)+\n  scale_color_manual(values = c(\"black\",\"darkgreen\"))+\n  geom_hline(yintercept = summarise(gc_rils,mean(prop_hybrid)) |&gt; pull(),\n             linetype = \"dashed\", color = \"red\", size = 2)+\n  labs(y = \"Proportion hybrid\", title =\"This plot is interactive!! Hover over a point to see its residual\")+\n  theme(legend.position = \"none\")\n\nggplotly(prop_hybrid_plot)\n\n\n\n\n\n\n\n\nFigure 2: An interactive plot showing the proportion of hybrid seeds for each Clarkia xantiana subspecies parviflora recombinant inbred line (RIL) planted at GC. Each point represents a line’s observed proportion hybrid seed, and the dashed red line shows the sample mean across all lines. Hovering over a point reveals its residual — the difference between the observed value and the mean. Individual 3 is shown in green as an example to focus on.\n\n\n\n\n\nWorked example.\nTake, for example, individual 3 (\\(i = 3\\), green point in the plot above), where 1/4 of its seeds are hybrids:\n\nIts observed value, \\(y_i\\), is the proportion of its seeds that are hybrids, which is 0.25.\n\nIts predicted value, \\(\\hat{y}_i\\), is the proportion of all seeds that are hybrids (dashed red line), which is 0.151.\n\nIts residual value, \\(e_i = y_i - \\hat{y}_i\\), is the difference between the proportion of its seeds that are hybrids and proportion of all seeds that are hybrids, which is \\(0.250 -0.151 = 0.099\\). Scroll over the third data point in Figure 2 to verify this.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Mean"
    ]
  },
  {
    "objectID": "book_sections/linear_models/mean.html#use-augment-to-see-residuals-and-more",
    "href": "book_sections/linear_models/mean.html#use-augment-to-see-residuals-and-more",
    "title": "• 7. Mean",
    "section": "Use augment() to see residuals (and more!)",
    "text": "Use augment() to see residuals (and more!)\nYou can use the augment() function in the broom package to see predictions and residuals. The code to the right uses this capability to calculate the sum of squared residuals, while the code below shows (some of) the output from augment().\n\nlibrary(broom)\nlm(prop_hybrid ~ 1, data = gc_rils)     |&gt;\n  augment()                             |&gt;\n  select(prop_hybrid, .fitted, .resid)\n\n\n\n\n\n\n\n\nCalculating \\(\\text{SS}_\\text{residual}\\) from augment() output\nYou can use the output of augment() to calculate the sum of squared residuals by taking residuals (from .resid), squaring them, and then summing them, as shown below:\n\nlibrary(broom)\nlm(prop_hybrid ~ 1, data = gc_rils)        |&gt;\n augment()                |&gt;\n mutate(sq_resid=.resid^2)|&gt;\n summarise(SS=sum(sq_resid))\n\n# A tibble: 1 × 1\n     SS\n  &lt;dbl&gt;\n1  5.62",
    "crumbs": [
      "7. Linear Models",
      "• 7. Mean"
    ]
  },
  {
    "objectID": "book_sections/linear_models/mean.html#the-mean-minimizes-textss_textresidual",
    "href": "book_sections/linear_models/mean.html#the-mean-minimizes-textss_textresidual",
    "title": "• 7. Mean",
    "section": "The mean minimizes \\(\\text{SS}_\\text{residual}\\)",
    "text": "The mean minimizes \\(\\text{SS}_\\text{residual}\\)\nYou already know the formula for the mean. But let’s imagine you didn’t. What properties would you want a good summary of the center of the data to have? What criteria would you use to say one summary was the best? We have already discussed this at length, and concluded it depends on the shape of the data etc…\nWhile you should consider the shape of our data when summarizing it, a common criterion of the “best” estimate is the one that minimizes the sum of squared residuals. Figure 3 shows that of all proposed means for the proportion of hybrid seeds set at GC, the arithmetic mean minimizes the sum of squared residuals. This is always the case!\nBy looping over many proposed means (\\(0\\) to \\(0.3\\)), the plot below illustrates that the arithmetic mean minimizes the sum of squared residuals: The sum of squared residuals for a given proposed mean is shown: In color on all three plots (yellow is a large sum of squared residuals, red is intermediate, and purple is low); Geometrically as the size of the square in the center plot; On the y-axis of the right plot.\n\n\n\n\n\n\n\n\nFigure 3: The mean minimizes the sum of squared residuals: The left panel shows the observed data points connected to proposed means (shown as a horizontal line), with colors indicating how much the sum of squared residuals differ from the minimum sum of squared residuals. The center panel visualizes the total squared error geometrically. The right panel shows the sum of squared residuals as a function of proposed mean values, with the current proposed mean highlighted with a point. The proposed value that minimizes the sum of squared residuals equals the arithmetic mean.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Mean"
    ]
  },
  {
    "objectID": "book_sections/linear_models/mean.html#optional-extra-learning",
    "href": "book_sections/linear_models/mean.html#optional-extra-learning",
    "title": "• 7. Mean",
    "section": "OPTIONAL EXTRA LEARNING",
    "text": "OPTIONAL EXTRA LEARNING\n\nOptional / advanced reading:\nAbove, I showed that for our case study, the arithmetic mean minimizes the sum of squared residuals, and then I asserted that this is always the case. For those of you who enjoy calculus and want a formal proof—rather than a professor saying “trust me”—I’ve provided a link that shows this more generally.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Mean"
    ]
  },
  {
    "objectID": "book_sections/linear_models/lm_cat_pred.html",
    "href": "book_sections/linear_models/lm_cat_pred.html",
    "title": "• 7. Categorical predictor",
    "section": "",
    "text": "Example: Proportion Hybrid Seeds = f(Petal Color)\nThe previous section showed that we can think about a simple mean as an intercept in a linear model with no explanatory variables. But in the introduction to linear models, I introduced \\(\\hat{y}_i\\) as a conditional mean. This section builds on our understanding of the intercept as a mean, moving toward our goal of understanding \\(\\hat{y}_i\\) as a conditional mean. To do so, we will start with a binary explanatory variable, and then move on to a categorical variable with multiple levels. Because I find this hard to introduce abstractly, we’ll jump right back into the Clarkia hybridization example.\nAfter exploring the mean proportion of hybrid seed as a summary, we are ready to explore variables that could explain variation in the proportion of hybrid seeds. To start, let’s focus on petal color as perhaps white petals do not attract pollinators and consequently make fewer hybrids. In this case the conditional means are:\ngc_rils                                             |&gt;\n  filter(!is.na(petal_color) , !is.na(prop_hybrid)) |&gt;\n  group_by(petal_color)                             |&gt;\n  summarise(mean_prop_hybrid = mean(prop_hybrid))\n\n# A tibble: 2 × 2\n  petal_color mean_prop_hybrid\n  &lt;chr&gt;                  &lt;dbl&gt;\n1 pink                  0.260 \n2 white                 0.0322\nWe can represent these conditional means in an equation by modeling the mean proportion of hybrid seeds for individual \\(i\\), conditional on its petal color:\n\\[\\text{PROP HYBRID SEED}_i = b_0 + b_1 \\times \\text{WHITE}_i + e_i\\]\nWe can think of the equation above as representing two separate cases:\n\\[\\text{PROP HYBRID SEED}_{i|\\text{Pink petal}} = b_0 + e_i\\] \\[\\text{PROP HYBRID SEED}_{i|\\text{White petal}}  = b_0 + b_1 + e_i\\]\nWe know (from above) that\nBelow we see that the lm() function in R can do this for us!",
    "crumbs": [
      "7. Linear Models",
      "• 7. Categorical predictor"
    ]
  },
  {
    "objectID": "book_sections/linear_models/lm_cat_pred.html#example-proportion-hybrid-seeds-fpetal-color",
    "href": "book_sections/linear_models/lm_cat_pred.html#example-proportion-hybrid-seeds-fpetal-color",
    "title": "• 7. Categorical predictor",
    "section": "",
    "text": "\\(b_0\\), the intercept, is the mean proportion hybrid seed set for the “reference level.” In this case the reference level is pink. We know this because PINK is not in the equation.\n\\(b_1\\) is the difference in the mean proportion hybrids set on white and pink-petaled plants.\nWHITE is an indicator variable.\n\nWHITE equals 1 for white-petaled plants.\n\nWHITE equals 0 for pink-petaled plants.\n\n\n\n\n\n\n\\(b_0\\) – the mean proportion of seeds from pink flowers that were hybrids equals 0.260.\n\n\\(\\text{PROP HYBRID SEED}_{i|\\text{White petal}}\\) equals 0.0322.\nSo we can solve for \\(b_1\\) – the difference between the mean proportion of hybrids on white and pink-petaled plants – as \\(\\text{PROP HYBRID SEED}_{i|\\text{White petal}} - b_0\\) (0.0322 - 0.260 = -0.2278).",
    "crumbs": [
      "7. Linear Models",
      "• 7. Categorical predictor"
    ]
  },
  {
    "objectID": "book_sections/linear_models/lm_cat_pred.html#building-a-model-with-lm",
    "href": "book_sections/linear_models/lm_cat_pred.html#building-a-model-with-lm",
    "title": "• 7. Categorical predictor",
    "section": "Building a model with lm()",
    "text": "Building a model with lm()\nWe can build linear models in R with the lm() function: lm(response ~ explanatory, data = data_set). So, the following code models the proportion of hybrid seeds as a function of petal color.\n\nlm(prop_hybrid ~ petal_color, data = gc_rils)\n\n\nCall:\nlm(formula = prop_hybrid ~ petal_color, data = gc_rils)\n\nCoefficients:\n     (Intercept)  petal_colorwhite  \n          0.2598           -0.2277  \n\n\nThe resulting coefficients match the values we calculated by hand, aside from minor rounding differences.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Categorical predictor"
    ]
  },
  {
    "objectID": "book_sections/linear_models/lm_cat_pred.html#residuals",
    "href": "book_sections/linear_models/lm_cat_pred.html#residuals",
    "title": "• 7. Categorical predictor",
    "section": "Residuals",
    "text": "Residuals\nAs in the case for the mean, we calculate residuals as the difference between each observed value, \\(y_i\\), and its predicted value, \\(\\hat{y}_i\\). However, in this case, \\(\\hat{y}_i\\), differs for plants with different petal colors. Again hovering over a point in Figure 1 reveals its residual. Also, as before we can use the augment() function to see predictions and residuals.\n\n\nCode\nlibrary(plotly)\nprop_hybrid_plot_color &lt;-  gc_rils                    |&gt;\n  filter(!is.na(prop_hybrid),!is.na(petal_color))     |&gt;\n  mutate(i = 1:n())                                   |&gt;\n  group_by(petal_color)                               |&gt; \n  mutate(y_hat_i = mean(prop_hybrid))                 |&gt;\n  ungroup()                                           |&gt;\n  mutate(y_i     = round(prop_hybrid, digits = 3),\n         y_hat_i = round(y_hat_i, digits = 3),\n         e_i     = round(y_i - y_hat_i, digits = 3))  |&gt;\n  mutate(color2 = ifelse(i == 3, \"3\" , petal_color))|&gt;\n  ggplot(aes(x = i, y = y_i, y_hat_i = y_hat_i, e_i = e_i, color = color2))+\n  geom_point(size = 4, alpha = .6)+\n  geom_hline(data = gc_rils    |&gt; \n               filter(!is.na(prop_hybrid),!is.na(petal_color)) |&gt; \n               group_by(petal_color)|&gt; \n               summarise(prop_hybrid = mean(prop_hybrid)),\n             aes(yintercept = prop_hybrid), linetype = \"dashed\", size = 2)+\n  facet_wrap(~petal_color)+\n  labs(y = \"Proportion hybrid\", title =\"This plot is interactive!! Hover over a point to see its residual\")+\n  theme_dark()+\n  scale_color_manual(values= c(\"darkgreen\",\"pink\",\"white\"))+\n  theme(legend.position = \"none\")\n\nggplotly(prop_hybrid_plot_color)\n\n\n\n\n\n\n\n\nFigure 1: An interactive plot showing the proportion of hybrid seeds for each Clarkia xantiana subspecies parviflora recombinant inbred line (RIL) planted at GC split by petal color. Each point represents a line’s observed proportion hybrid seed, and the dashed line shows the group mean across all lines with pink (left) and white (right) petals. Hovering over a point reveals its residual — the difference between the observed value and the group mean. The green point provides an example datapoint to focus on for understanding.\n\n\n\n\n\nWorked example.\nTake, for example, individual 3 (\\(i = 3\\), green point in the plot above), where 1/4 of its seeds are hybrids:\n\nIts observed value, \\(y_i\\), is the proportion of its seeds that are hybrids, which is 0.25.\n\nIts predicted value, \\(\\hat{y}_i\\), is the proportion of seeds from pink-flowered plants that are hybrids (dashed line), which is 0.260.\n\nIts residual value, \\(e_i = y_i - \\hat{y}_i\\), is the difference between the proportion of its seeds that are hybrids and proportion of seeds from pink-flowered plants that are hybrids, which is \\(0.250 - 0.260 =  - 0.010\\). Scroll over the third data point in Figure 2 to verify this.\n\nNote that this data point had a large residual when we modeled proportion of hybrid seeds as a simple mean, but now has a residual value near zero when we model proportion of hybrid seeds as a function of petal color.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Categorical predictor"
    ]
  },
  {
    "objectID": "book_sections/linear_models/lm_cat_pred.html#categorical-predictors-with-more-than-two-levels",
    "href": "book_sections/linear_models/lm_cat_pred.html#categorical-predictors-with-more-than-two-levels",
    "title": "• 7. Categorical predictor",
    "section": "Categorical Predictors with More Than Two Levels",
    "text": "Categorical Predictors with More Than Two Levels\nAbove we considered a binary predictor, but the same modeling approach works for nominal predictors with more than two categories. For example, modeling, proportion of seeds that are hybrid as a function of location would look something like this:\n\\[\\text{PROP HYBRID SEED}_i = f(\\text{location})\\] \\[\\text{PROP HYBRID SEED}_i = b_0 + b_1 \\times \\text{LB}_i + b_2 \\times \\text{SR}_i  + b_3 \\times \\text{US}_i+ e_i\\]\nIn this case,\n\n\\(b_0\\), the intercept, is the mean proportion of hybrid seeds for the “reference level.” In this case the reference level is GC. We know this because GC is not in the equation.\n\n\\(b_1\\) is the difference in the mean proportion hybrids set at LB and at the reference location (GC).\nLB is an indicator variable.\n\nLB equals 1 for plants planted at LB.\n\nLB equals 0 for plants not planted at LB.\n\n\n\\(b_2\\) is the difference in the mean proportion hybrids set at SR and at the reference location (GC).\n\n… and so on, as summarized in the table below.\n\n\n\n\n\n\n\nTerm in Model\nInterpretation\n\n\n\n\nIntercept (\\(b_0\\))\nMean proportion of hybrid seeds at GC (reference group)\n\n\n\\(b_1\\) (locationLB)\nDifference between LB and GC: mean at LB − mean at GC\n\n\n\\(b_2\\) (locationSR)\nDifference between SR and GC: mean at SR − mean at GC\n\n\n\\(b_3\\) (locationUS)\nDifference between US and GC: mean at US − mean at GC\n\n\n\n\n\nPredicted proportion hybrid seeds by location:   - \\(\\hat{y}_{i|\\text{GC}} = b_0\\) - \\(\\hat{y}_{i|\\text{LB}} = b_0 + b_1\\) - \\(\\hat{y}_{i|\\text{SR}} = b_0 + b_2\\) - \\(\\hat{y}_{i|\\text{US}} = b_0 + b_3\\) **",
    "crumbs": [
      "7. Linear Models",
      "• 7. Categorical predictor"
    ]
  },
  {
    "objectID": "book_sections/linear_models/lm_cat_pred.html#building-a-model-with-multiple-levels-with-lm",
    "href": "book_sections/linear_models/lm_cat_pred.html#building-a-model-with-multiple-levels-with-lm",
    "title": "• 7. Categorical predictor",
    "section": "Building a model with multiple levels with lm()",
    "text": "Building a model with multiple levels with lm()\nThe syntax for building a model for a categorical variable with multiple levels is the same as that for a binary categorical variable: lm(response ~ explanatory, data = data_set). The following code models the proportion of hybrid seeds as a function of planting location:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nOptional / advanced: “How does R pick the reference level? and how can you change it?”\nBy default, R uses the alphabetically first level of a categorical variable as the reference group. This isn’t always the best way to think about our data. Later we will work on changing the order of our levels, but if you can’t wait, check out the [forcats](https://forcats.tidyverse.org/) package.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Categorical predictor"
    ]
  },
  {
    "objectID": "book_sections/linear_models/regression.html",
    "href": "book_sections/linear_models/regression.html",
    "title": "• 7. Linear regression",
    "section": "",
    "text": "A review of covariance and correlation\nWe have previously quantified the association between two numeric variables as:\nWhile these measures, and particularly \\(r\\), are essential summaries of association, neither allows us to model a numeric response variable as a function of numeric explanatory variable.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Linear regression"
    ]
  },
  {
    "objectID": "book_sections/linear_models/regression.html#a-review-of-covariance-and-correlation",
    "href": "book_sections/linear_models/regression.html#a-review-of-covariance-and-correlation",
    "title": "• 7. Linear regression",
    "section": "",
    "text": "A covariance: How much observations of \\(x\\) and \\(y\\) jointly deviate from their means in an individual, quantified as: \\(\\text{cov}_{x,y}= \\frac{n}{n-1}(\\overline{xy}-\\bar{x} \\times\\bar{y}) = \\frac{\\sum (x_i-\\bar{x})\\times(y_i-\\bar{y})}{n-1}\\).\n\nA correlation, \\(r\\): How reliably \\(x\\) and \\(y\\) covary – a standardized measure of the covariance. The correlation is quantified as \\(r_{xy}= \\frac{\\text{cov}_{x,y}}{s_x\\times s_y}\\), where \\(s_x\\) and \\(s_y\\) are the standard deviations of \\(x\\) and \\(y\\), respectively.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Linear regression"
    ]
  },
  {
    "objectID": "book_sections/linear_models/regression.html#linear-regression-set-up",
    "href": "book_sections/linear_models/regression.html#linear-regression-set-up",
    "title": "• 7. Linear regression",
    "section": "Linear regression set up",
    "text": "Linear regression set up\nSo, how do we find \\(\\hat{y}_i\\)? The answer comes from our standard equation for a line — \\(y = mx + b\\). But statisticians have their own notation:\n\\[\\hat{y}_i = b_0 + b_1 \\times x_i\\] The variables in this model are interpreted as follows:\n\n\\(\\hat{y}_i\\) is the conditional mean of the response variable for an individual with a value of \\(x_i\\). Note this is a guess at what the mean would be if we had many such observations - but we usually have one or fewer observations of y at a specific value of x.\n\n\\(b_0\\) is the intercept – the value of the response variable if we follow it to \\(x = 0\\).\n\n\\(b_1\\) is the slope – the expected change in \\(y\\), for every unit increase in \\(x\\).\n\n\\(x_i\\) is the value of the explanatory variable, \\(x\\) in individual \\(i\\).",
    "crumbs": [
      "7. Linear Models",
      "• 7. Linear regression"
    ]
  },
  {
    "objectID": "book_sections/linear_models/regression.html#math-for-linear-regression",
    "href": "book_sections/linear_models/regression.html#math-for-linear-regression",
    "title": "• 7. Linear regression",
    "section": "Math for linear regression",
    "text": "Math for linear regression\nBefore delving into the R of it all, let’s start with some math. To make things concrete, we’ll consider the proportion of hybrid seed as a function of petal area.\n\nMath for slope\nWe find the slope, \\(b_1\\), as the covariance divided by the variance in \\(x\\):\n\\[b_1 = \\frac{cov_{x,y}}{s^2_x}  =  \\frac{\\frac{1}{(n-1)}\\sum(x_i-\\bar{x})(y_i-\\bar{y})}{\\frac{1}{(n-1)}\\sum(x_i-\\bar{x})^2}=\\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum(x_i-\\bar{x})^2}\\]\n\n\nMath for intercept\nWe find the intercept, \\(b_0\\) (sometimes called \\(a\\)), as\n\\[b_0 = \\bar{y}-b_1\\bar{x}\\]\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nggplot(gc_rils, \n       aes(x = petal_area_mm, \n           y = prop_hybrid))+\n  geom_smooth(method = \"lm\", \n              se = FALSE)+\n  geom_point(size = 6)\n\n\nInterpretation\n\n\n\n\n\n\n\n\n\nFigure 1: A scatterplot showing the relationship between petal area (in mm²) and the proportion of hybrid seeds in parviflora RILs, with a fitted linear regression line.\n\n\n\n\nThe calculations above (displayed in Figure 1) correspond to the following linear model:\n\\[\\text{PROPORTION HYBRID}_i =  -0.208 + 0.00574  \\times \\text{PETAL AREA}_i\\]\n\nThe intercept of -0.208 means that if we followed the line down to a petal area of zero, the math would predict that such a plant would produce a nonsensical negative 20% hybrid seed. This is a mathematically useful construct but is biologically and statistically unjustified (see below). Of course, we can still use this equation (and implausible intercept), to make predictions in the range of our data.\nThe slope of 0.00574 means that every additional \\(mm^2\\) increase in area, we expect about a half percent more hybrid seeds.\n\n\nDo not predict beyond the range of the data.\nPredictions from a linear model are only valid within the range of our observed data. For example, while our model estimates an intercept of –0.208, we certainly don’t expect a microscopic flower to receive fewer than zero pollinator visits. Similarly, we shouldn’t expect a plant with a petal area of \\(300 \\text{ mm}^2\\) to produce more than 150% hybrid seeds.\nThese exaggerated examples highlight an important point: we should not make predictions far outside the range of our data. Even a less extreme prediction — say, estimating that a plant with 200 mm² of petal area will produce 95% hybrid seeds — may not be reliable. Although this number may seem biologically plausible, we have no reason to trust that our model’s predictions hold beyond the range of data we collected.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Linear regression"
    ]
  },
  {
    "objectID": "book_sections/linear_models/regression.html#residuals",
    "href": "book_sections/linear_models/regression.html#residuals",
    "title": "• 7. Linear regression",
    "section": "Residuals",
    "text": "Residuals\nAs seen previously, predictions of a linear model do not perfectly match the data. Again, the difference between observed, \\(y_i\\), and expected, \\(\\hat{y}_i\\), values is the residual \\(e_i\\). Try to estimate the residual of data point three (green point) in Figure 2.\n\n\nCode\nggplot(gc_rils|&gt; mutate(i = 1:n()), \n       aes(x = petal_area_mm, y = prop_hybrid))+\n    geom_smooth(method = \"lm\", se = FALSE)+\n        geom_point(aes(color = (i==3), size = (i == 3)))+\n        scale_size_manual(values = c(3,6))\n\n\n\n\n\n\n\n\nFigure 2: The same scatterplot showing the relationship between petal area as in Figure 1. The highlighted point in turquoise marks plant 3, used as an example in the residual calculation. This point’s vertical distance from the regression line represents its residual\n\n\n\n\n\n\nCONCEPT CHECK Looking at Figure 2, visually estimate \\(e_3\\) (the residual of point three in green) \n\n\nHow to solve this.\n\nSolution: The green point (\\(i = 3\\)) is at y = 0.25, and x is just a bit more than 50. At that x value, the regression line (showing \\(\\hat{y}\\)) is somewhat below 0.125 (I visually estimate _{ mm}= 0.090$). So 0.250 - 0.090 = 0.160.\n\n\n\nMath to get us closer\n\nMath Approach: As above, let’s visually approximate the petal area of observation 3 as 51 mm. Plugging this visual estimate into our equation (with coefficients from our linear model):\n\\[\\hat{y}_{\\text{petal area} \\approx 50 mm}-0.207834 + 0.005738 * 51 = 0.084\\] \\[e_3 = y_ -  \\hat{y} = 0.250 - 0.084 = 0.166\\]\n\n\n\nThe slope minimizes \\(\\text{SS}_\\text{residuals}\\)\nAs seen for the mean, by looping over many proposed slope(\\(-0.01\\) to \\(0.02\\)) the plot below illustrates that the slope from the formula above minimizes the sum of squared residuals: The sum of squared residuals for a given proposed slope (with intercept plugged in from math) is shown: In color on all three plots (yellow is a large sum of squared residuals, red is intermediate, and purple is low); Geometrically as the size of the square, in the center plot; On the y-axis of the right plot.\n\n\n\n\n\n\n\n\nFigure 3: The slope minimizes the sum of squared residuals: The left panel shows observed data points along with a proposed regression line. Vertical lines connect each point to its predicted value, illustrating residuals. The color of the line corresponds to the total residual sum of squares. The center panel visualizes the total squared error as a square whose area reflects the sum of squared residuals. The right panel plots the sum of squared residuals as a function of the proposed slope, with a moving point highlighting the current slope value. The slope that minimizes the sum of squared residuals defines the best-fitting line.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Linear regression"
    ]
  },
  {
    "objectID": "book_sections/linear_models/regression.html#using-the-lm-function",
    "href": "book_sections/linear_models/regression.html#using-the-lm-function",
    "title": "• 7. Linear regression",
    "section": "Using the lm() function",
    "text": "Using the lm() function\nAs before, we build a linear model in R as lm(response ~ explanatory). So to predict the proportion of hybrid seed as a function of petal area (in mm) type:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nAgain, we can view residuals and other useful things with the augment() function. Let’s go back to the webr section above and pipe the output of lm() to augment()\n\nCONCEPT CHECK: Use the augment function to find the exact residual of data point 3 to three decimal places \n\n\nSolution\n\nPaste the code below into the webR window above to find the answer.\n\nlibrary(broom)\nlm(prop_hybrid ~ petal_area_mm, data = gc_rils) |&gt;\n  augment() |&gt;\n  slice(3)       # You can just go down to the third, but I wanted to show you this trick",
    "crumbs": [
      "7. Linear Models",
      "• 7. Linear regression"
    ]
  },
  {
    "objectID": "book_sections/linear_models/regression.html#looking-forward.",
    "href": "book_sections/linear_models/regression.html#looking-forward.",
    "title": "• 7. Linear regression",
    "section": "Looking forward.",
    "text": "Looking forward.\nYou’ve now seen how we can model a linear relationship between two numeric variables, how to calculate the slope and intercept, and how the best-fitting line is the one that minimizes the sum of squared residuals. You’ve also interpreted these model components in a biological context, visualized residuals, and learned how to fit and explore a linear model in R.\n\nIn the next section, we extend linear models to include two explanatory variables.\n\nLater in this book, we’ll introduce concepts of uncertainty and hypothesis testing and model evaluation to deepen our understanding of what these models can (and cannot) tell us, and when to be appropriately skeptical about their predictions.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Linear regression"
    ]
  },
  {
    "objectID": "book_sections/linear_models/regression.html#optional-extra-learning.",
    "href": "book_sections/linear_models/regression.html#optional-extra-learning.",
    "title": "• 7. Linear regression",
    "section": "OPTIONAL EXTRA LEARNING.",
    "text": "OPTIONAL EXTRA LEARNING.\n\n\\(r^2\\) and the proportion variance explained\nSo far, we’ve focused on just one type of sum of squares — the residual sum of squares, \\(\\text{SS}_\\text{residual}\\) (also called \\(\\text{SS}_\\text{error}\\)). As you well know by now:\n\n\\(\\text{SS}_\\text{residual}\\) describes the distance between predictions and observations, and is quantified as the sum of squared differences between observed values, \\(y_i\\), and the conditional means, \\(\\hat{y}_i\\). \\(\\text{SS}_\\text{residual} = \\sum{(e_i)^2} = \\sum{(y_i-\\hat{y}_i)^2}\\).\n\nWe will explore two other types of sums of squares later, but in case you can’t wait:\n\n\\(\\text{SS}_\\text{model}\\) describes the distance between predictions and the grand mean, and is quantified as is the sum of squared differences between conditional means, \\(\\hat{y}_i\\), and the grand mean, \\(\\bar{y}\\). \\(\\text{SS}_\\text{model} = \\sum{(\\hat{y}_i-\\bar{y})^2}\\).\n\\(\\text{SS}_\\text{total}\\) describes the overall variability in the response variable, and is quantified as the sum of squared differences between observed values \\(y_i\\), and the grand mean, \\(\\bar{y}\\). \\(\\text{SS}_\\text{total} = \\sum{(y_i-\\bar{y})^2}\\).\n\nDividing \\(\\text{SS}_\\text{model}\\) by \\(\\text{SS}_\\text{total}\\) describes “the proportion of variance explained” by our model, \\(r^2\\):\n\\[r^2 = \\frac{\\text{SS}_\\text{model}}{\\text{SS}_\\text{total}}\\]\nReassuringly, the square root of \\(r^2\\) equals the absolute value of \\(r\\)\n\nlibrary(broom)\nlm(prop_hybrid ~ petal_area_mm, data = gc_rils) |&gt;\n  augment()|&gt;\n  summarise(mean_y   = mean(prop_hybrid),\n            SS_error = sum(.resid^2),\n            SS_total = sum( (prop_hybrid - mean_y)^2 ),\n            SS_model = sum( (.fitted - mean_y)^2 ),\n            r2       = SS_model /  SS_total,\n            r        = cor(prop_hybrid, petal_area_mm),\n            sqrt_r2  = sqrt(r2))\n\n# A tibble: 1 × 7\n  mean_y SS_error SS_total SS_model    r2     r sqrt_r2\n   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1  0.151     4.95     5.59    0.638 0.114 0.338   0.338",
    "crumbs": [
      "7. Linear Models",
      "• 7. Linear regression"
    ]
  },
  {
    "objectID": "book_sections/linear_models/two_predictors.html",
    "href": "book_sections/linear_models/two_predictors.html",
    "title": "• 7. Two predictors",
    "section": "",
    "text": "General Linear models in R\nWe found that both petal area and petal color predict the proportion of hybrid seeds in parviflora RILs. We’ve seen how to model each relationship individually, but what if you want to consider both traits at once? – Does petal size still matter when accounting for petal color? Or do visits ‘explain away’ the apparent association of proportion hybrid seeds with petal size?\nThe “general linear model” allows us to tackle such questions by modeling multiple explanatory variables at once (Figure 1). To do so, we extend our familiar simple linear model to include both kinds of predictors at once.\nIn this case, each predictor gets its own coefficient:\nMathematically, the prediction for an individual becomes:\n\\[\\hat{y}_i = b_0 + b_1 x_{1i} + b_2 x_{2i}\\] To make this more concrete, let’s define:\nWith this biological grounding we can re-cast our equation as\n\\[\\widehat{\\text{PROPORTION OF HYBRID SEED}}_i = b_0 + b_1   \\times  \\text{PETAL AREA MM}^2 +  b_2 \\times \\text{WHITE}_i\\]\nThe mathematical trick for estimating parameters (e.g. \\(b_0\\), \\(b_1\\), and \\(b_2\\)) in a general linear model exceeds what I care that you learn. So let’s find these in R with the lm() function.\nTo conduct a general linear model in R just add terms to your linear model:\nlm(response ~ explan_1 + explan_2, data = data). For our case:\nlm(prop_hybrid ~ petal_area_mm + petal_color, data = gc_rils)\n\n\nCall:\nlm(formula = prop_hybrid ~ petal_area_mm + petal_color, data = gc_rils)\n\nCoefficients:\n     (Intercept)     petal_area_mm  petal_colorwhite  \n       -0.091439          0.005759         -0.239181",
    "crumbs": [
      "7. Linear Models",
      "• 7. Two predictors"
    ]
  },
  {
    "objectID": "book_sections/linear_models/two_predictors.html#general-linear-models-in-r",
    "href": "book_sections/linear_models/two_predictors.html#general-linear-models-in-r",
    "title": "• 7. Two predictors",
    "section": "",
    "text": "Beware of associated predictors (multicollinearity): If pink-petaled parviflora RILs tended to have bigger petals than white ones, it becomes trickier to cleanly separate their effects. The overall model predictions (\\(\\hat{y}_i\\)) can be reliable, but the estimated coefficients (\\(b_1\\), \\(b_2\\)) can become unstable or hard to interpret. For now know that it’s fine if predictors are somewhat related, but when they are strongly correlated, we have to be cautious when interpreting individual coefficients.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Two predictors"
    ]
  },
  {
    "objectID": "book_sections/linear_models/two_predictors.html#conditional-means-from-general-linear-models",
    "href": "book_sections/linear_models/two_predictors.html#conditional-means-from-general-linear-models",
    "title": "• 7. Two predictors",
    "section": "Conditional means from General Linear Models",
    "text": "Conditional means from General Linear Models\nWith the estimates above, we can return to our general linear model equation to find conditional means: \\[\\widehat{\\text{PROPORTION OF HYBRID SEED}}_i = b_0 + b_1   \\times  \\text{PETAL AREA MM}^2 +  b_2 \\times \\text{WHITE}_i\\] \\[\\widehat{\\text{PROPORTION OF HYBRID SEED}}_i =  -0.0894 + 0.00576  \\times  \\text{PETAL AREA MM}^2 - 0.244192   \\times \\text{WHITE}_i\\]\n\n\n\n\n\ni\npetal_area_mm\npetal_color\nprop_hybrid\n\n\n\n\n1\n43.9522\nwhite\n0.000\n\n\n2\n55.7863\npink\n0.125\n\n\n3\n51.7031\npink\n0.250\n\n\n4\n57.2810\nwhite\n0.000\n\n\n\n\n\n\nA worked example and challenge questions\nThe table below shows values of the explanatory and response variables for the first four samples in the gc_rils dataset. Individuals \\(i=1\\) and \\(i=3\\) are show in black and red x’s in Figure 2, and I provide R code for calculating the prediction for individual \\(i=1\\) below. In the webR session below:\n\nFind the residual for individual \\(i=1\\).\n\nFind the predicted proportion hybrid seed and the residual for individual \\(i=3\\).\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nHow to solve this.\n\n\n # prediction for i = 1\n # b0   + b1 * petal area + b2 * white\ny_1 &lt;- -0.0894 + 5.76e-3 * 44 - 0.244 * 1       \nprint(paste(\"Prediction for individual i=1:\",y_1))\n\n[1] \"Prediction for individual i=1: -0.07996\"\n\n# Residual for ind i=1\ne_1 &lt;- 0.00 - y_1 # Residual = observed - predicted\nprint(paste(\"Residual for individual i=1:\",e_1))\n\n[1] \"Residual for individual i=1: 0.07996\"\n\n# prediction for i = 3\n# b0   + b1 * petal area + b2 * white\ny_3 &lt;- -0.0894 + 5.76e-3 * 51.7 #   no b2 because its pink  \nprint(paste(\"Prediction for individual i=3:\",y_3))\n\n[1] \"Prediction for individual i=3: 0.208392\"\n\n# Residual for ind i=1\ne_3 &lt;- 0.25 - y_3 # Residual = observed - predicted\nprint(paste(\"Residual for individual i=3:\",e_3))\n\n[1] \"Residual for individual i=3: 0.0416079999999999\"\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Proportion of hybrid seeds as a function of petal area and petal color in parviflora RILs. This plot essentially replicates Figure 1, with the following caveats: it shows the same slope for each petal color, matching our model assumptions. The plot is interactive. Individuals \\(i=1\\) and \\(i=3\\) are highlighted with black and red Xs, respectively.\n\n\n\n\n\n\nlibrary(broom)\n\n# Minimal code for a figure\n# like that on the left\n\nmy_lm &lt;- lm(prop_hybrid ~ \n              petal_color + \n              petal_area_mm, \n            data = gc_rils)\n\naugmented_data &lt;-my_lm |&gt;\n   augment()\n\naugmented_data |&gt;\n    ggplot(aes(\n      x = petal_area_mm,\n      y = prop_hybrid,\n      color = petal_color)\n      )+\n    geom_point()+\n    geom_smooth(aes(\n        y = .fitted),\n      method = \"lm\",\n      se = FALSE)",
    "crumbs": [
      "7. Linear Models",
      "• 7. Two predictors"
    ]
  },
  {
    "objectID": "book_sections/linear_models/two_predictors.html#wrapping-up-two-predictors-one-model",
    "href": "book_sections/linear_models/two_predictors.html#wrapping-up-two-predictors-one-model",
    "title": "• 7. Two predictors",
    "section": "Wrapping Up: Two Predictors, One Model",
    "text": "Wrapping Up: Two Predictors, One Model\nAbove, we extended a simple linear model to include two predictors — one continuous and one categorical. We could, of course, have had two categorical predictors, two numeric predictors, or more than two predictors as well. By adding multiple predictors, we can better account for biological complexity — modeling how several traits simultaneously influence a response. We can even include interactions between variables, as I show in the optional content below.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Two predictors"
    ]
  },
  {
    "objectID": "book_sections/linear_models/two_predictors.html#optional-extra-learning.",
    "href": "book_sections/linear_models/two_predictors.html#optional-extra-learning.",
    "title": "• 7. Two predictors",
    "section": "OPTIONAL EXTRA LEARNING.",
    "text": "OPTIONAL EXTRA LEARNING.\n\nIncluding interactions.\nEarlier, we assumed that the increase in proportion of hybrid seeds with increased petal area was the same for pink and white-flowered plants, but as shown in Figure 1, this might not be true. It looks like petal area is strongly associated with proportion hybrid seed in pink, but not white-flowered plants.\nWe can further extend linear models to include such interactions (i.e. differences in slopes).\n\\[\\hat{y}_i = \\text{INTERCEPT} + \\text{SLOPE} \\times \\text{PETAL AREA} +\\] \\[ b_2 \\times \\text{WHITE}+ b_3 \\times   \\text{WHITE} \\times \\text{PETAL AREA}\\]\nWe can do this in R by adding this interaction explicitly:\nlm(response ~ explan1 + explan2 + explan1:explan2), where : notes the interaction.\n\nlm(prop_hybrid ~ petal_area_mm + \n                 petal_color   + \n                 petal_area_mm:petal_color, \n    data = gc_rils) \n\n\n\n\n\n\n\nx\n\n\n\n\n(Intercept)\n-0.3289259\n\n\npetal_area_mm\n0.0095891\n\n\npetal_colorwhite\n0.2326197\n\n\npetal_area_mm:petal_colorwhite\n-0.0075498\n\n\n\n\n\nFrom these estimates, we can model proportion of hybrid seeds for pink and white-petaled flowers as follows:\n\n\\[\\hat{y}_{i|\\text{PINK}} = -0.330 + 0.00960 \\times \\text{PETAL AREA}\\]\nand\n\n\\[\\hat{y}_{i|\\text{WHITE}} = -0.330 + 0.00960 \\times \\text{PETAL AREA}\\] \\[+0.237 -0.00766 \\times \\text{PETAL AREA}\\]\n\\[\\hat{y}_{i|\\text{WHITE}} = -0.330 +0.237 +\\text{PETAL AREA} \\times (0.00960 -0.00766)\\]\n\\[\\hat{y}_{i|\\text{WHITE}} = -0.093 +\\text{PETAL AREA} \\times (0.001946)\\]\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Two predictors"
    ]
  },
  {
    "objectID": "book_sections/linear_models/two_predictors.html#haty_itextpink--0.330-0.00960-times-textpetal-area",
    "href": "book_sections/linear_models/two_predictors.html#haty_itextpink--0.330-0.00960-times-textpetal-area",
    "title": "• 7. Two predictors",
    "section": "\\[\\hat{y}_{i|\\text{PINK}} = -0.330 + 0.00960 \\times \\text{PETAL AREA}\\]",
    "text": "\\[\\hat{y}_{i|\\text{PINK}} = -0.330 + 0.00960 \\times \\text{PETAL AREA}\\]\nand\n\n\\[\\hat{y}_{i|\\text{WHITE}} = -0.330 + 0.00960 \\times \\text{PETAL AREA}\\] \\[+0.237 -0.00766 \\times \\text{PETAL AREA}\\]\n\\[\\hat{y}_{i|\\text{WHITE}} = -0.330 +0.237 +\\text{PETAL AREA} \\times (0.00960 -0.00766)\\]\n\\[\\hat{y}_{i|\\text{WHITE}} = -0.093 +\\text{PETAL AREA} \\times (0.001946)\\]\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Two predictors"
    ]
  },
  {
    "objectID": "book_sections/linear_models/lm_summary.html",
    "href": "book_sections/linear_models/lm_summary.html",
    "title": "• 7. Linear model summary",
    "section": "",
    "text": "Chapter summary\nLinks to: Summary. Chatbot tutor. Questions. Glossary. R functions. R packages. More resources.\nLinear models provide a unified framework for estimating the expected value (i.e., the conditional mean) of a numeric response variable as a function of one or more explanatory variables. These models are additive: the expected value is found by summing components of the model — the intercept plus the effect of each variable multiplied by its value. The sum of squared differences between observed values and predicted values describes how closely the data match the model’s predictions. Linear models can be descriptive tools that capture the structure, variation, and relationships in a dataset. In later chapters, we will build on this foundation to evaluate models more critically — assessing how well they fit, how reliable their predictions are, and how to diagnose their limitations.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Linear model summary"
    ]
  },
  {
    "objectID": "book_sections/linear_models/lm_summary.html#lm_summary_chapter-summary",
    "href": "book_sections/linear_models/lm_summary.html#lm_summary_chapter-summary",
    "title": "• 7. Linear model summary",
    "section": "",
    "text": "Chatbot tutor\n\nPlease interact with this custom chatbot (link here) I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you got what you needed from it.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Linear model summary"
    ]
  },
  {
    "objectID": "book_sections/linear_models/lm_summary.html#lm_summary_practice-questions",
    "href": "book_sections/linear_models/lm_summary.html#lm_summary_practice-questions",
    "title": "• 7. Linear model summary",
    "section": "Practice Questions",
    "text": "Practice Questions\nTry these questions! By using the R environment you can work without leaving this “book”. To help you jump right into thinking and analysis, I have loaded the ril data, cleaned it some, and have started some of the code!\n\nQ1) What is the key difference between a scientific and a statistical model?\n\n Statistical models make predictions, scientific models do not. Statistical models can be wrong, scientific models cannot. Statistical models describe patterns, scientific models are built on understanding processes. Statistical models make assumptions, scientific models do not. There are no differences between scientific and statistical models.\n\n\nQ2 Consider the R code and output below. The (Intercept) describes:\n\n The mean bill depth of penguins in this dataset. The mean bill depth of all penguins. The mean bill depth of penguins of the “reference level” species. The expected bill depth if all the predictors are set zero.\n\n\nlibrary(palmerpenguins)\nlm(bill_depth_mm~1, penguins)\n\n\nCall:\nlm(formula = bill_depth_mm ~ 1, data = penguins)\n\nCoefficients:\n(Intercept)  \n      17.15  \n\n\nQ3) Without running this code, predict which sex will be the reference level in the model: lm(bill_length_mm ~ sex, penguins)? malefemalewhichever has the smaller meanimpossible to predict\n\n\nQ4 - Q7) Linear models with categorical predictors. The penguins data has data from three species of penguins – Adelie, Chinstrap and Gentoo. To answer the following questions, consider the R code and output below and no other information (that is, do not load these data into R).\n\nlm(bill_length_mm~species, penguins)\n\n\nCall:\nlm(formula = bill_length_mm ~ species, data = penguins)\n\nCoefficients:\n     (Intercept)  speciesChinstrap     speciesGentoo  \n          38.791            10.042             8.713  \n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ4) What is the mean bill length of Adelie penguins in the dataset? 38.79110.04248.833not enough information\nQ5) What is the mean bill length of Chinstrap penguins in the dataset? 38.79110.04248.833not enough information\nQ6) How many mm longer Chinstrap bills as compared to Gentoo bills in this dataset? 10.0428.7131.329not enough information\nQ7) What is the mean bill length of all penguins in this dataset? 38.79119.18245.0426743.92not enough information\n\n\nQ8 - Q13) Mathematics of linear regression. Use the summaries below to conduct a linear regression that models the response variable, bill depth, as a function of the explanatory variable, bill length.\n\n\n\n\n\nmean_depth\nmean_length\ncov_length\nsd_depth\nsd_length\n\n\n\n\n17.59\n46.57\n0.62\n0.78\n3.11\n\n\n\n\n\n\n\n\nmin_depth\nmax_depth\nmin_length\nmax_length\nsample_size\n\n\n\n\n16.4\n19.4\n40.9\n58\n34\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ8) The correlation between these variables is .\nQ9) The slope in this model is .\nQ10) The intercept in this model is .\nQ11) According to the model, what is the predicted bill depth (in mm) for a penguin with a 50 mm long bill .\nQ12) A penguin with a 20 mm deep and 50 mm long bill will have a residual of  mm.\nQ13) According to the model, what is the predicted bill depth for a penguin with a 500 mm long bill  mm deep bill.\n\n\nHaven’t gotten it yet?\n\nOf course you haven’t!! The correct answer, of course, is that you cannot predict outside the range of our data. See Figure 1!\n\n\n\nQ14 - Q16) More than one explanatory variable. Use the summaries below to conduct a linear regression that models the response variable, bill depth, as a function of the explanatory variable, bill length.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(palmerpenguins)\nlibrary(broom)\n\n\ngentoo_data &lt;- penguins        |&gt;\n  filter(species == \"Gentoo\") \n\nlm(bill_depth_mm ~ bill_length_mm +sex, data = gentoo_data)|&gt;\n  augment() |&gt;\n  ggplot(aes(x=bill_length_mm , y=bill_depth_mm,color = sex))+\n  geom_point(size = 5, alpha = .7)+\n  geom_smooth(method = \"lm\",se = FALSE, linetype = \"dashed\",linewidth = 2)+\n  geom_smooth(aes(y = .fitted), se=FALSE, linewidth = 2)+\n  theme(legend.position = \"top\", \n        axis.title  = element_text(size = 28),\n        axis.text   = element_text(size = 28),\n        legend.text = element_text(size = 28),\n        legend.title = element_text(size = 28))\n\n\n\n\n\n\n\n\nFigure 2: Bill depth as a function of bill length for Gentoo penguins, separated by sex. Solid lines show the predicted values from a multiple regression model including both bill length and sex as predictors. Dashed lines show simple linear fits ignoring other predictors. The plot highlights both the overall trend with bill length and differences in mean depth between males and females.\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ14) In Figure 2 there is a male penguin with a bill that is about 56 mm long (the second most extreme right point). Approximate, by eye its residual. \nQ15) Based on Figure 2, which of the following are nearly identical for male and female Gentoo penguins? (Select all that apply.)\n\n Mean bill lengths Mean bill depths Slopes Intercepts This plot shows that nearly nothing is nearly identical between male and female Gentoo penguins\n\n.\nQ16) Use the web R space above to model flipper length as a function of body mass and sex of Chinstrap penguins. The sum of squared residuals is:.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Linear model summary"
    ]
  },
  {
    "objectID": "book_sections/linear_models/lm_summary.html#lm_summary_glossary-of-terms",
    "href": "book_sections/linear_models/lm_summary.html#lm_summary_glossary-of-terms",
    "title": "• 7. Linear model summary",
    "section": "📊 Glossary of Terms",
    "text": "📊 Glossary of Terms\n\n\n📚 1. Concepts of Modeling\n\nStatistical Model: A mathematical description of patterns in data, often used to summarize, predict, or test hypotheses.\nScientific Model: A conceptual model based on biological understanding, explaining processes in the real world.\n\n\n\n\n🔀 2. Different Predictor Types\n\nCategorical Predictor: A variable with discrete groups. Modeled by differences in intercepts across groups.\nNumeric Predictor: A continuous variable. Modeled by slopes showing expected change in the response per unit change in the predictor.\nIndicator Variable: A numeric coding of a categorical variable (e.g., 0 for “pink,” 1 for “white”).\nReference Level: The baseline category in a categorical predictor against which other groups are compared.\n\n\n\n3. Components of a Linear Model\n\nConditional Mean, \\(\\hat{y}_i\\): The predicted value of a response variable for given explanatory variable values.\n\nGeneral linear model form: \\(\\hat{Y}_i = f(\\text{explanatory variables}_i)\\).\nLinear combination form: \\(\\hat{Y}_i = b_0 + b_1 x_{1,i} + b_2 x_{2,i} + \\dots + b_k x_{k,i}\\).\n\n\nIntercept (b₀): The expected value of the response when all explanatory variables are zero (sometimes called \\(a\\)).\nSlope (b₁): The expected change \\(\\hat{y}_i\\) with change in the predictor.\n\nFor numeric predictor: The expected change in the response for a one-unit increase in a numeric explanatory variable. \\(b_1 = \\text{cov}_{x,y}/\\sigma^2_x\\).\n\nFor binary predictor: The difference in the meane of non-reference and reference level.\n\n\n\\(x_{1,i}\\) The value of explanatory variable, \\(1\\), in individual \\(i\\).\n\nFor numeric predictors: The value of the explanatory variable.\n\nFor binary predictors: The value of the indicator variable.\n\n\\(x_1\\) equals 0 for the reference group.\n\n\\(x_1\\) equals 1 for the non-reference group.\n\n\n\n\n\n4. Concepts for Linear Models.\n\nObserved value (yᵢ): The actual value of the response variable: \\(y_i = \\hat{y}_i +e_i\\).\n\nResidual (eᵢ): The difference between an observed value and its model-predicted value, \\(e_i = y_i-\\hat{y}_i\\).\n\nResidual Sum of Squares: \\(\\sum e_i^2\\)\nResidual Standard Deviation: A measure of typical residual size — how far off predictions tend to be \\(\\sum e_i^2/(n-1)\\).\n\n\n\n\n🚫 5. Model Limitations\n\nExtrapolation: Making predictions outside the range of observed data — generally unsafe.\nMulticollinearity: When explanatory variables are highly correlated, making it hard to separate their individual effects.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Linear model summary"
    ]
  },
  {
    "objectID": "book_sections/linear_models/lm_summary.html#lm_summary_key-r-functions",
    "href": "book_sections/linear_models/lm_summary.html#lm_summary_key-r-functions",
    "title": "• 7. Linear model summary",
    "section": "Key R Functions",
    "text": "Key R Functions\n\n📈 Building Linear Models\n\nlm(): Fits linear models. Syntax: lm(response ~ explanatory, data = dataset).\naugment() ([broom]): Adds predictions and residuals to your dataset for easy exploration.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Linear model summary"
    ]
  },
  {
    "objectID": "book_sections/linear_models/lm_summary.html#lm_summary_r-packages-introduced",
    "href": "book_sections/linear_models/lm_summary.html#lm_summary_r-packages-introduced",
    "title": "• 7. Linear model summary",
    "section": "R Packages Introduced",
    "text": "R Packages Introduced\n\n\nbroom: Tidies model outputs (like fitted values and residuals) into neat data frames.\nggplot2: Used for visualizing data and model fits.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Linear model summary"
    ]
  },
  {
    "objectID": "book_sections/linear_models/lm_summary.html#lm_summary_additional-resources",
    "href": "book_sections/linear_models/lm_summary.html#lm_summary_additional-resources",
    "title": "• 7. Linear model summary",
    "section": "Additional resources",
    "text": "Additional resources\n\nWeb resources:\n\nLinear modelling: introduction. From Analysing Data using Linear Models by Stéphanie M. van den Berg.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Linear model summary"
    ]
  },
  {
    "objectID": "book_sections/ordination.html",
    "href": "book_sections/ordination.html",
    "title": "8. Ordination",
    "section": "",
    "text": "Let’s get started with ordination!\nA common set of tools, known as ordination methods, summarize high-dimensional datasets into a few major axes of variation. These summaries can be incredibly informative, revealing key patterns in the data. For example, John Novembre showed that summarizing whole-genome data by its major axes of variation revealed a structure in European genetic variation that closely mirrors the geographic map of Europe. This example highlights the best of ordination methods because it:\nWe will work through the intuition and mechanics of how to conduct a few standard ordination techniques. Our focus will be on building a conceptual understanding and a pragmatic “know-how”, rather than a rigorous mathematical foundation. Along the way, we’ll also wrestle with practical questions. For example “What do we do about missing data?”, “Should we scale our variables?”, and “When are two variables redundant? (and why should I care?)”\nAs usual, we conclude by summarizing the chapter, presenting a chatbot tutor, practice questions, a glossary, a review of R functions and R packages introduced, and provide links to additional resources.",
    "crumbs": [
      "8. Ordination"
    ]
  },
  {
    "objectID": "book_sections/ordination.html#lets-get-started-with-ordination",
    "href": "book_sections/ordination.html#lets-get-started-with-ordination",
    "title": "8. Ordination",
    "section": "",
    "text": "We’ll begin with Principal Component Analysis (PCA) — a linear method that looks for the directions of greatest variance in your data. We will start with a familiar dataset: Clarkia individuals from the GC site, for which we’ve measured anther-stigma distance, petal area, and leaf water content. These traits may relate to different aspects of plant performance or pollination biology — but PCA won’t “know” that. It will simply tell us how plants vary. We split this exploration into three sections: (1) We get started with pca, (2) We look under the hood of PCA to understand a bit o how it works, and (3) We look out for common issues with PCA gone wrong.\nNext we take a quick tour of alternatives to pca, including Multiple Correspondence Analysis for nominal datasets, Factor Analysis of Mixed Data for when a dataset has both continuous and categorical variables, Principal cordinates analysis and non-metric dimensional scaling for nonlinear data, and t-SNE and UMAP for high dimnesional data.\n\n\n\n\n\n\nNovembre, J., Johnson, T., Bryc, K., Kutalik, Z., Boyko, A. R., Auton, A., Indap, A., King, K. S., Bergmann, S., Nelson, M. R., Stephens, M., & Bustamante, C. D. (2008). Genes mirror geography within europe. Nature, 456(7218), 98–101. https://doi.org/10.1038/nature07331",
    "crumbs": [
      "8. Ordination"
    ]
  },
  {
    "objectID": "book_sections/ordination/pca.html",
    "href": "book_sections/ordination/pca.html",
    "title": "• 8. PCA quick start",
    "section": "",
    "text": "PCA in R with prcomp()\nOne reason biologists are so interested in associations between variables is that very little in this world is independent. Take our parviflora RILs, for example. If you recall, these recombinant inbred lines were made to disentangle correlations between traits by mixing up genomes from two populations and allowing for multiple generations of recombination. Although this effort was largely successful – trait correlations persist in our RILs because traits are tightly linked on a chromosome or are physiologically connected. As such, for example, petal area is strongly positively related to anther stigma distance, and negatively correlated with leaf water content.\nPCA summarizes variability into major axes of variation in a sample by finding combinations of traits that explain the most variability in the data, and are independent of each other. PCA first find principal component 1 (PC1) as the linear combination of all traits that explain the most variability in our sample. It goes on to do the same for PC2, and so on. We are left with as many principal components as original traits — but all are independent and the first few principal components explain more of the variance than the last few.\nNow let’s run a quick PCA and then unpack what it’s doing. We run a PCA in R following these steps:",
    "crumbs": [
      "8. Ordination",
      "• 8. PCA quick start"
    ]
  },
  {
    "objectID": "book_sections/ordination/pca.html#pca-in-r-with-prcomp",
    "href": "book_sections/ordination/pca.html#pca-in-r-with-prcomp",
    "title": "• 8. PCA quick start",
    "section": "",
    "text": "PCA requires complete data. If you have missing observations for individuals, you can either 1. Drop these individuals. 2. Impute (a fancy word for informed guessing) missing values with tools like missMDA. 3. Use more advanced PCA methods that can handle missing data directly, such as probabilistic PCA.\n\nFormat the data.\n\nOnly keep numeric (and continuous-ish) variables.\n\nDeal with missing data (see aside on right).\n\nMake sure variables are roughly symmetric (otherwise transform).\n\nRemove redundant variables.\n\nDecide should data be centered and scaled? (The answer is usually yes).\n\nRun PCA with the prcomp() function.\nInterpret PCA results including:.\n\nProportion variance explained,\nTrait loadings, and\n\nPC values (aka scores)\n\nVisualize the results.\nConnect visualization to the underlying biology.\n\n\nFormatting the data\nBefore we format the data let’s have a look at it\n\n\nCode for selecting data for PCA from RILs planted at GC\nril_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_rils.csv\"\nril_data &lt;- readr::read_csv(ril_link) |&gt;\n  dplyr::mutate(growth_rate = case_when(growth_rate ==\"1.8O\" ~ \"1.80\",\n                                          .default = growth_rate),  \n                growth_rate = as.numeric(growth_rate),\n                made_hybrid = prop_hybrid &gt; 0)\n\ngc_rils_4pca  &lt;- ril_data |&gt;\n  na.omit()|&gt;\n  filter(location == \"GC\", !is.na(prop_hybrid), ! is.na(mean_visits))|&gt;\n  select(made_hybrid, petal_area_mm, asd_mm, growth_rate,  stem_dia_mm, lwc )\n\n\n\n\n\n\n\n\n\nWe usually want to “center” and “scale” the data (i.e. subtract off the mean and divide by the standard deviation). We discuss why we do (or do not) choose to do this later. For now, know that you can center and scale within prcomp(center = TRUE, scale = TRUE), or before running the PCA with the scale() function.\n\nscaled_gc_rils_4pca &lt;- gc_rils_4pca |&gt;\n  select(-made_hybrid)|&gt;                # Remove categorical variables\n  mutate_all(scale)                     # Subtract mean and divide by SD\n\n\n\n\n\n\n\n\n\nRun PCA with the prcomp() function.\nThis is the easy part – if your data are centered and scaled, and there are no NA’s just give your data as an argument to the prcomp() function:\n\ngc_ril_pca &lt;- prcomp(scaled_gc_rils_4pca)         # Run the pca\n\n\n\nInterpreting PCA output\nBecause a PCA returns numerous things we want to know about (percent variance explained by each PC, trait loadings onto PCs, individual values on each PC), prcomp() returns something with an awkward structure (known as an S3 object in R). To make this easier to deal with, we will use the tidy() and augment() functions from the broom package introduced earlier.\n\nLooking into proportion variance explained\nIt is too easy to fool ourselves into looking at something very interesting that explains relatively little of the variation in our dataset. Remember that principal components sequentially explain less and less of the overall variability in our dataset. So it is always worth reporting the proportion variance captured by focal principal components. Here we see that PC1 captures roughly 30% of the variability in our data, while PC5 captures about 11% of the overall variability.\n\nlibrary(broom)                    # Load the broom library\ntidy(gc_ril_pca, matrix = \"pcs\")  # Extract percent variance explained from pc object\n\n\n\n\n\n\nPC\nstd.dev\npercent\ncumulative\n\n\n\n\n1\n1.241\n0.308\n0.308\n\n\n2\n1.137\n0.258\n0.566\n\n\n3\n0.957\n0.183\n0.749\n\n\n4\n0.838\n0.140\n0.890\n\n\n5\n0.742\n0.110\n1.000\n\n\n\n\n\nThere are a few common visualizations of the proportion of variance explained by each PC.\n\nA scree plot (Figure 1 A) shows the amount of variation soaked up by the principal component on the y-axis. Scree plots usually have points for the standard deviation attributable to each PC, and those points are connected by lines.\nA proportion variance explained plot (Figure 1 B) is very similar, but the y-axis is the proportion variance explained by each PC (that is the variance (i.e. std.dev\\(^2\\)) explained by each PC divided by the sum of standard variances), and is shown as a bar plot.\nA cumulative proportion variance explained plot (Figure 1 C) shows the cumulative sum of a proportion variance explained plot and is shown with points and lines. In this case, the value for PC2 equals the sum of the proportions of variance explained by PC1 and PC2.\n\n\n\nCode for making scree plots and PVE plots.\nlibrary(patchwork)\n\npc_summary &lt;- tidy(gc_ril_pca, matrix = \"pcs\")\n\nscree_plot &lt;- ggplot(pc_summary, aes(x = PC, y = std.dev))+\n  geom_point(size = 3)+\n  geom_line(lty= 2)+\n  ggtitle(\"A) Scree plot\")\n\npve_plot &lt;-  ggplot(pc_summary, aes(x = PC, y = percent))+\n  geom_col()+\n  ggtitle(\"B) PVE plot\")\n\ncumulative_pve_plot &lt;- bind_rows( \n  tibble(PC = 0 , std.dev = 0, percent = 0, cumulative = 0),\n  pc_summary)|&gt;\n  ggplot( aes(x = PC, y = cumulative))+\n  geom_point(size = 3)+\n  geom_line(lty= 2)+\n  ggtitle(\"C) Cumulative PVE\")\n\nscree_plot + pve_plot  + cumulative_pve_plot \n\n\n\n\n\n\n\n\nFigure 1: Visualizing percent variance explained by PCs. Principal component analysis (PCA) reduces dimensionality by compressing variance into the first principal components. The plots above show how much variability is explained by each PC. A is a scree plot showing the standard deviation of each principal component, reflecting how much variation each one captures from the original data. B displays a bar plot of the proportion of total variance explained by each component. C shows the cumulative proportion of variance explained, beginning at zero and increasing with each additional PC, reaching 100% by the fifth component.\n\n\n\n\n\n\n\nLooking into trait loading\n\n\n\n\n\nLoading of each trait onto each PC\n\nSo about 30% of the variability is captured by PC1, but what even is PC1? I’m glad you asked this question! PC1 (like all principal components) is a combination of variables each weighted by a certain amount. For PC1, this combination point in the direction of the most variability in the (scaled and centered) data set. In this case, the value of PC1 in individual, \\(i\\), equals:\npetal_area x 0.56 + asd x 0.48 + growth_rate x 0 + stem_dia x 0.43 + lwc x -0.53.\nHere each trait refers to the scaled and centered value in individual \\(i\\). Similarly, “loadings” of traits on each PCA are presented in the table to the right and in Figure 2.\n\nlibrary(broom)                            # load broom package\ntidy(gc_ril_pca, matrix = \"loadings\") |&gt;  # Get trait loadings\n  arrange(PC)                             # Sort the data so we see PC1 first\n\n\n\nCode for visualizing trait loadings onto PCs\nlibrary(forcats)\ngc_ril_pca |&gt;\n    tidy(matrix = \"loadings\") |&gt;\n    mutate(variable = fct_reorder(column, -1*value * as.numeric(PC==1),.fun = max))|&gt;\n    ggplot(aes(x = PC, y = variable, fill= value))+\n    geom_tile(color = \"black\")+\n    scale_fill_gradient2(low = \"firebrick\", mid = \"white\", high = \"steelblue\", midpoint = 0) +\n    geom_text(aes(label = round(value, digits = 3)))+\n  labs(fill = \"Loading\")+\n  scale_x_discrete(expand = c(0,0))+\n  scale_y_discrete(expand = c(0,0))+\n  theme(axis.title.y = element_blank(),\n        plot.background = element_rect(fill =  \"white\",color =\"white\"),\n        panel.background  = element_rect(fill =  \"white\",color =\"white\"),\n        legend.position = \"bottom\",\n        legend.key.width = unit(2, \"cm\"))\n\n\n\n\n\n\n\n\nFigure 2: Trait loadings on each PC axis.\n\n\n\n\n\n\n\nLooking into PC values\nWe can now find each individual’s value on each PC1. To do so, we\n\nTake the scaled and centered values of each trait,\n\nMultiply this by the trait loading on a given PC,\n\nTake the sum.\n\nRinse and repeat for all individuals on all PCs\n\nSo individual 1’s value for PC1 is:\n(0.56 x -1.26) + (0.48 x -1.09) + (0 x 0.54) + (0.43 x -0.65) + (-0.53 x -0.31).\nThis equals -1.33. Rather than repeating this for all individuals and all PCs, we can find these values with the augment() function in the broom package:\n\naugment(gc_ril_pca)\n\n\n\n\n\n\n\n\n\n\nVisualizing PCs\nNow that we know what our PCA means, let’s have a look at our results. Below, I show how to use the autoplot() function in the ggfortify package. I present four plots:\n\nAll give the prcomp() output to the autoplot() function to make a plot.\n\nAll find the proportion variance explained by PCs on the x and y axes, and set the “aspect ratio” as \\(\\text{pve}_\\text{y}/\\text{pve}_\\text{x}\\).\n\nWhy mess with aspect ratio? In PCA, each axis (e.g., PC1 and PC2) explains a different amount of variance, so an honest plot presents axes proportional to their importance. Otherwise, distances and angles in the plot can be misleading. Setting the aspect ratio as the ratio of proportion of variance explained, to prevent such misinterpretation.\n\n\nThe plot in tab two (Trait loadings) generates a classic “pca biplot” by adding trait loadings.\n\nThe plot in tab three (+ categories) uses color to add a categorical variable to the plot. This allows us to see if some categorical variable e.g., species (or in this case, if a flower made at least one hybrid) is associated with some part of PC space.\n\nThe plot in tab four (PC1 v PC5) shows that we can use x and y arguments in autoplot() to show other PCs.\n\n\nPC1 v PC2Trait loadings+ categoriesPC1 v PC5\n\n\n\n\nlibrary(ggfortify) \npc_var_explained &lt;- tidy(gc_ril_pca, matrix = \"pcs\") \npercent_pc1 &lt;- pc_var_explained |&gt; filter(PC == 1) |&gt;  pull(percent)\npercent_pc2 &lt;- pc_var_explained |&gt; filter(PC == 2) |&gt;  pull(percent)\n\nautoplot(gc_ril_pca, size=4, alpha = .7)+\n  theme(aspect.ratio = percent_pc2 / percent_pc1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(ggfortify) \npve &lt;- tidy(gc_ril_pca, matrix = \"pcs\") \npve_pc1 &lt;- pve |&gt; filter(PC == 1) |&gt;  pull(percent)\npve_pc2 &lt;- pve |&gt; filter(PC == 2) |&gt;  pull(percent)\n\nautoplot(gc_ril_pca,  size=4, alpha =.7,\n         loadings = TRUE, loadings.colour = 'blue',\n         loadings.label = TRUE, loadings.label.size = 7)+\n  theme(aspect.ratio = pve_pc2 / pve_pc1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(ggfortify) \npve &lt;- tidy(gc_ril_pca, matrix = \"pcs\") \npve_pc1 &lt;- pve |&gt; filter(PC == 1) |&gt;  pull(percent)\npve_pc2 &lt;- pve |&gt; filter(PC == 2) |&gt;  pull(percent)\n\nautoplot(gc_ril_pca,data = gc_rils_4pca, color = 'made_hybrid', \n         size = 5, alpha = .8,\n         loadings = TRUE, loadings.colour = 'black',\n         loadings.label = TRUE, loadings.label.size = 5)+\n  scale_color_manual(values = c(\"brown\",\"forestgreen\"))+\n  theme(aspect.ratio = pve_pc2 / pve_pc1, \n        legend.position = \"bottom\",\n        legend.key.width = unit(2, \"cm\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(ggfortify) \npve &lt;- tidy(gc_ril_pca, matrix = \"pcs\") \npve_pc1 &lt;- pve |&gt; filter(PC == 1) |&gt;  pull(percent)\npve_pc5 &lt;- pve |&gt; filter(PC == 5) |&gt;  pull(percent)\n\nautoplot(gc_ril_pca, x = 1, y = 5, size=4, alpha =.7,\n         loadings = TRUE, loadings.colour = 'blue',\n         loadings.label = TRUE, loadings.label.size = 5)+\n  theme(aspect.ratio = pve_pc5 / pve_pc1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBiological Interpretation of PCA plots\nNow that we can create and interpret PCA plots, let’s return to the underlying biological questions they can address. Some common questions to ask are:\n\nAre individuals clustered or spread out?\n\nIn our case individuals are quite spread out. This lets us know that recombination during the creation of our RILs did a reasonable job of dissecting the underlying traits.\n\nClustering would suggest that there is some underlying structure in the data. If clustering was noticeable, we would want to know which traits are driving that structure.\n\nWhich traits influence the main axes? In the biplot, look at the arrows:\n\nWhich traits are most aligned with PC1 or PC2?\n\nAre any traits opposing each other?\n\nDo categories separate in PCA space? In our case we see that nearly all observations in the bottom left of our plot set no hybrid seeds, while the top right (aligned with large petal area) seemed to have a lot of hybrids.\nIs anything interesting happening beyond PC1 and PC2? Check out other PCs (like PC5) to see if subtle patterns or groupings emerge that aren’t visible on the first two axes. But beware, do not overinterpret patterns in PCs that explain very little variability. \nDo you notice a “horseshoe” shape? Sometimes points a PCA plot form a curved or arched pattern. This is a known artifact that often shows up when PCA tries to fit a curved pattern using straight-line axes. This can be misleading – samples at the two ends of the gradient may appear close together even though they’re quite different. When you see a horseshoe, consider transforming key variables or using an alternative ordination approach less sensitive to this issue.\n\n\nNow that we can make and interpret PCA plots, let’s look under the hood and see how they work.",
    "crumbs": [
      "8. Ordination",
      "• 8. PCA quick start"
    ]
  },
  {
    "objectID": "book_sections/ordination/pcaII.html",
    "href": "book_sections/ordination/pcaII.html",
    "title": "• 8. PCA deeper dive",
    "section": "",
    "text": "How PCA Works\nWe just ran a PCA, looked at plots, and interpreted what the axes meant in terms of traits in our parviflora RILs. We also introduced the core idea: PCA finds new axes — principal components — that are combinations of traits, oriented to explain as much variation as possible. These components are uncorrelated with one another, and together they form a rotated coordinate system that helps us summarize multivariate variation.\nThe description of PCA and interpretation of results presented in the previous chapter are the bare minimum we need to understand a PCA analysis. Here, we will go beyond this bare minimum – dipping into the inner workings of PCA to better understand how it works and what to watch out for. This is essential for critically interpreting PCA results. A challenge here is that there is a bunch of linear algebra under the hood – and while I love linear algebra, I don’t expect many of you to know it. So, we will try to understand PCA without knowing linear algebra.\nWe will look at the two key steps in running a PCA.\nCode for selecting data for PCA from RILs planted at GC\nril_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_rils.csv\"\nril_data &lt;- readr::read_csv(ril_link) |&gt;\n  dplyr::mutate(growth_rate = case_when(growth_rate ==\"1.8O\" ~ \"1.80\",\n                                          .default = growth_rate),  \n                growth_rate = as.numeric(growth_rate),\n                made_hybrid = prop_hybrid &gt; 0)\n\ngc_rils_4pca  &lt;- ril_data |&gt;\n  na.omit()|&gt;\n  filter(location == \"GC\", !is.na(prop_hybrid), ! is.na(mean_visits))|&gt;\n  select(made_hybrid, petal_area_mm, asd_mm, growth_rate,  stem_dia_mm, lwc )",
    "crumbs": [
      "8. Ordination",
      "• 8. PCA deeper dive"
    ]
  },
  {
    "objectID": "book_sections/ordination/pcaII.html#how-pca-works",
    "href": "book_sections/ordination/pcaII.html#how-pca-works",
    "title": "• 8. PCA deeper dive",
    "section": "",
    "text": "Making a covariance (or correlation) matrix.\n\nFinding Principal Components.",
    "crumbs": [
      "8. Ordination",
      "• 8. PCA deeper dive"
    ]
  },
  {
    "objectID": "book_sections/ordination/pcaII.html#building-a-covariance-or-correlation-matrix.",
    "href": "book_sections/ordination/pcaII.html#building-a-covariance-or-correlation-matrix.",
    "title": "• 8. PCA deeper dive",
    "section": "Building a covariance (or correlation) matrix.",
    "text": "Building a covariance (or correlation) matrix.\nAfter removing categorical variables from our data, the first step in PCA (if we do this without using prcomp()) is to build a matrix that shows the association between each trait in our data set.\nWe usually want to give each trait the same weight so we either center and scale (for each value of a trait we subtract the mean and divide by the trait’s standard deviation) and then find the covariance between each trait with the cov() function.\n\n\nMaking a covariance matrix from a centered and scaled data set, provides the exact same result as making a correlation matrix from the original data with the cor() function. So sometimes people talk about PCs from the correlation matrix (i.e. centered and scaled data), or the covariance matrix (i.e. centered but not scaled data).\n\n# Scaling the data\nscaled_gc_rils_4pca &lt;- gc_rils_4pca |&gt;\n  select(-made_hybrid)|&gt;                # Remove categorical variables\n  mutate_all(scale)                     # Subtract mean and divide by SD\n\n# Finding all pairwise covariances on centered and scaled data\ncor_matrix &lt;- cov(scaled_gc_rils_4pca)\n\nA few simple patterns jump out from this correlation matrix:\n\nAll values on the diagonal equal one. That’s because all traits are perfectly correlated with themselves (this is true of all correlation matrices).\nThe matrix is symmetric – values on the bottom triangle are equal to those on the top triangle (this is also true of all correlation matrices).\n\nSome traits are positively correlated with each other – e.g. there is a large correlation between anther stigma distance and petal area – perhaps because they both describe flower size.\nSome traits are negatively correlated with each other– e.g. nearly all other traits are negatively correlated with leaf water content.\n\n\n\nCode for visualizing trait correlations\nlibrary(tibble)\nlibrary(forcats)\ncor_matrix |&gt;\n  data.frame()|&gt;\n  rownames_to_column(var = \"trait_1\") |&gt;\n  pivot_longer(-trait_1, names_to = \"trait_2\", values_to = \"cor\")|&gt;\n  mutate(trait_1 = str_remove(trait_1,\"_mm\"),\n         trait_2 = str_remove(trait_2,\"_mm\"))|&gt;\n  mutate(trait_1 = fct_relevel(trait_1, \"stem_dia\",\"petal_area\",\"asd\",\"growth_rate\",\"lwc\"),\n         trait_2 = fct_relevel(trait_2, \"stem_dia\",\"petal_area\",\"asd\",\"growth_rate\",\"lwc\"),\n         cor = round(cor,digits = 3))|&gt;\n  ggplot(aes(x = trait_1, y=trait_2, fill = cor))+\n  geom_tile(color = \"black\")+\n  scale_fill_gradient2(\n    high = \"#0072B2\",    # Blue (colorblind-safe)\n    mid = \"white\",       # Center\n    low = \"#D55E00\",     # Orange (colorblind-safe)\n    midpoint = 0)+\n  geom_text(aes(label = cor),size = 5)+\n  labs(title=\"Trait correlations\")+\n  theme(legend.position = \"bottom\",\n        legend.key.width = unit(2, \"cm\"),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n\nFigure 1: Pairwise correlations among traits in parviflora recombinant inbred lines. Each cell shows the covariance of scaled and centered data (i.e. correlation) between two traits, with color indicating strength and direction of association (blue = positive, orange = negative). Traits include stem diameter, petal area, anther–stigma distance (asd), growth rate, and leaf water content (lwc). The matrix is symmetric, and all diagonal values are 1 by definition.",
    "crumbs": [
      "8. Ordination",
      "• 8. PCA deeper dive"
    ]
  },
  {
    "objectID": "book_sections/ordination/pcaII.html#finding-principal-components",
    "href": "book_sections/ordination/pcaII.html#finding-principal-components",
    "title": "• 8. PCA deeper dive",
    "section": "Finding Principal Components",
    "text": "Finding Principal Components\nIn PCA, we’re trying to find new axes that summarize the patterns of variation in our dataset. These principal components are chosen to explain as much variance as possible. As we have seen, each principal component is a weighted combination of the original traits. We have also seen that each principal component “explains” a given proportion of the variation in the multivariate data.\nHow does PCA find combinations of trait loading that sequentially explain most of the variance? Traditionally, PCA is done by first calculating a covariance or correlation matrix (as above), then performing eigenanalysis on that matrix to find the loadings and the proportion of variance explained. Eigenanalysis is a bit like “ordinary least squares” (OLS) for finding a best-fit line, with a few key differences:\n\n\nNot quite eigenanalysis Rather than using eigenanalysis, prcomp() and most modern PCA approaches use singular value decomposition (SVD). prcomp() and eigen() both help us find principal components, but they work differently. eigen() is the traditional method: it takes a covariance or correlation matrix and performs eigen decomposition to find the axes of greatest variation. prcomp(), on the other hand, uses singular value decomposition (SVD) directly on the original (centered and optionally scaled) data matrix. This makes it more numerically stable and slightly faster, especially for large datasets. Both methods give nearly identical results if the data is prepared the same way, though the signs of loadings may differ.\n\nEigenanalysis usually deals with more than two variables.\n\nThere’s no split between “explanatory” and “response” variables.\n\nInstead of minimizing the vertical distance between observed and predicted values of y (as in OLS), eigenanalysis finds directions that minimize the sum of squared perpendicular distances — in multidimensional space — from each point to the axis (principal component).\n\nIn R we can conduct an eigenanalysis with the eigen() function.\n\nEigenvalues are the variance explained\nHere the eigen values are the variance attributable to each principal component, and the square root of these values match the standard deviation provided by prcomp() (see below and the previous section):\n\nlibrary(broom)\neigen_analysis &lt;- eigen(cov(scaled_gc_rils_4pca))\npca_analysis   &lt;- prcomp(scaled_gc_rils_4pca)\n\ntidy(pca_analysis, matrix = \"pcs\")|&gt;\n  mutate(eigen_var = eigen_analysis$values ,\n         eigen_sd = sqrt(eigen_var))|&gt;\n  select(-percent, - cumulative)|&gt;\n  relocate(std.dev, .after = eigen_sd)\n\n\n\n\n\n\nPC\neigen_var\neigen_sd\nstd.dev\n\n\n\n\n1\n1.540\n1.241\n1.241\n\n\n2\n1.292\n1.137\n1.137\n\n\n3\n0.915\n0.957\n0.957\n\n\n4\n0.702\n0.838\n0.838\n\n\n5\n0.551\n0.742\n0.742\n\n\n\n\n\n\n\nEigenvectors are (nearly) the trait loadings\n\n\n\n\n\n\n\n\n\nFigure 2: Comparing PCA loadings from prcomp() and eigenvectors from eigen(). Each panel shows one principal component (PC1–PC5). For each trait, the x-axis shows its loading from prcomp(), and the y-axis shows the corresponding value in the eigenvector from eigen(). The dashed lines indicate the near-perfect agreement between the two methods, differing only in sign. This demonstrates that (when everything is working right) prcomp() and eigen() produce equivalent results (up to sign) when the data are properly centered and scaled.\n\n\n\n\nThe trait loadings from eigenanalysis are basically the same as those from prcomp(). Figure 2 shows that, in our case, the trait loadings on each PC are the same, with the caveat that the sign may flips. This doesn’t change our interpretation (because PCs are relative, so sign does not matter), but is worth noting.\n\n\nFind PC values by adding up each trait weighted by its loading\nWe already covered this.. If you like linear algebra, you can think about this as the dot product of the scaled and centered trait matrix and the matrix of trait loadings.\n\nThe problem(s) with missing data in PCA. Missing data causes two problems in PCA.\n\nYou cannot simply make a covariance or correlation matrix with missing data. This can be computationally overcome as follows cor( use = \"pairwise.complete.obs\"), but is only reliable if data are missing at random.\n\nYou cannot find PC values for individuals with missing data. Again there are computer tricks to overcome this, but they must be considered in the context of the data:\n\n\nYou can set missing values to zero. But this brings individuals with a bunch of missing values closer to the center of your PC plot.\n\n\n\nYou can drop individuals with missing data. But again, if patterns of missingness are non-random this can be a problem.\n\n\n\n\nYou can impute (smart guess) the missing values (using packages like missMDA, or mice) or use a PCA approach like emPCA or probabilistic PCA that do this for you. But then you are working on guesses of what your data may be and not what they are.",
    "crumbs": [
      "8. Ordination",
      "• 8. PCA deeper dive"
    ]
  },
  {
    "objectID": "book_sections/ordination/pcaII.html#why-we-usually-scale-our-variables",
    "href": "book_sections/ordination/pcaII.html#why-we-usually-scale-our-variables",
    "title": "• 8. PCA deeper dive",
    "section": "Why we usually scale our variables",
    "text": "Why we usually scale our variables\nWe usually center and scale our variables so that our PC gives equal value to all variables. Sometimes we may not want to do this – perhaps we want to take traits with more variability more seriously. While this might be ok if variables are on very similar scales, it can be quite disastrous. Now that we’ve seen how PCA works using scaled data, let’s explore what happens when we skip the scaling step.\nIn our GC RIL data for example, petal area (measured in \\(mm^2\\)) has a much greater variance than any other trait (because the covariance of a variance with itself is the variance). You can see this on the main diagonal of Figure 3 where the variance in petal area is 216, while the second largest value in this matrix is less than two.\n\ncentered_gc_rils_4pca &lt;- gc_rils_4pca |&gt;\n  select(-made_hybrid)|&gt;                # Remove categorical variables\n  mutate_all(scale, scale = FALSE)      # Subtract mean and divide by SD\n\n# Finding all pairwise covariances on centered (but not scaled) data\ncov_matrix &lt;- cov(centered_gc_rils_4pca)\n\n\n\nCode for visualizing trait covariances\nlibrary(tibble)\nlibrary(forcats)\ncov_matrix |&gt;\n  data.frame()|&gt;\n  rownames_to_column(var = \"trait_1\") |&gt;\n  pivot_longer(-trait_1, names_to = \"trait_2\", values_to = \"cov\")|&gt;\n  mutate(trait_1 = str_remove(trait_1,\"_mm\"),\n         trait_2 = str_remove(trait_2,\"_mm\"))|&gt;\n  mutate(trait_1 = fct_relevel(trait_1, \"stem_dia\",\"petal_area\",\"asd\",\"growth_rate\",\"lwc\"),\n         trait_2 = fct_relevel(trait_2, \"stem_dia\",\"petal_area\",\"asd\",\"growth_rate\",\"lwc\"),\n         cov = round(cov,digits = 3))|&gt;\n  ggplot(aes(x = trait_1, y=trait_2, fill = cov))+\n  geom_tile(color = \"black\")+\n  scale_fill_gradient2(\n    high = \"#0072B2\",    # Blue (colorblind-safe)\n    mid = \"white\",       # Center\n    low = \"#D55E00\",     # Orange (colorblind-safe)\n    midpoint = 0)+\n  geom_text(aes(label = cov),size = 5)+\n  labs(title=\"Trait covariances\")+\n  theme(legend.position = \"bottom\",\n        legend.key.width = unit(2, \"cm\"),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n\nFigure 3: Covariance matrix of traits in parviflora RILS before scaling. Each cell shows the covariance between a pair of traits, with color indicating magnitude (darker blue = larger covariance). The diagonal shows trait variances. Petal area dominates the matrix with a variance over 215, while all other trait variances are below 2. This disparity illustrates how unscaled PCA can be overwhelmed by traits with larger absolute variation.\n\n\n\n\n\nNow we can see the consequences of not scaling or centering on our PCs:\n\ngc_ril_centered_pca &lt;- prcomp(centered_gc_rils_4pca)\n\nBecase the variance in petal area dominates the covariance matrix, PC1 explains 99.85% of the variability in the data (Table 1) and maps perfectly on to petal area (Table 2).\n\n\n\n# Finding proportion \n# variance explained by PC\ngc_ril_centered_pca   |&gt;\n  tidy(matrix = \"pcs\")\n\n\n\n\n\nTable 1: PVE for unscaled data.\n\n\n\n\n\n\nPC\nstd.dev\npercent\ncumulative\n\n\n\n\n1\n14.6898\n0.9985\n0.9985\n\n\n2\n0.4104\n0.0008\n0.9993\n\n\n3\n0.3511\n0.0006\n0.9999\n\n\n4\n0.1507\n0.0001\n1.0000\n\n\n5\n0.0196\n0.0000\n1.0000\n\n\n\n\n\n\n\n\n\n\n# Finding trait loadings\ngc_ril_centered_pca         |&gt;\n  tidy(matrix = \"loadings\") |&gt; \n  filter(PC==1)\n\n\n\n\n\nTable 2: Trait loadings for unscaled data.\n\n\n\n\n\n\ncolumn\nPC\nvalue\n\n\n\n\npetal_area_mm\n1\n0.9999\n\n\nasd_mm\n1\n0.0076\n\n\ngrowth_rate\n1\n-0.0067\n\n\nstem_dia_mm\n1\n0.0010\n\n\nlwc\n1\n-0.0003\n\n\n\n\n\n\n\n\n\n\n\nNow that we have a sense of how PCA works, lets think harder about what to worry about",
    "crumbs": [
      "8. Ordination",
      "• 8. PCA deeper dive"
    ]
  },
  {
    "objectID": "book_sections/ordination/pcaIII.html",
    "href": "book_sections/ordination/pcaIII.html",
    "title": "• 8. PCA – Gotchas",
    "section": "",
    "text": "Proceed Cautiously Ahead\nNow that we can run a PCA and know how they work, let’s think hard about how to interpret PCA results. These expand on warnings I made in the PCA quickstart and/or in the previous chapter, but now we can approach them with a bit of sophistication. I first start with a list of things to worry about. You should ask these questions of every PCA you see. Next I look more deeply into the idea that PCA goes after the variance in the data, so the data matter a lot.\nI previously listed some first things to do when you see a PCA. These include:",
    "crumbs": [
      "8. Ordination",
      "• 8. PCA -- Gotchas"
    ]
  },
  {
    "objectID": "book_sections/ordination/pcaIII.html#proceed-cautiously-ahead",
    "href": "book_sections/ordination/pcaIII.html#proceed-cautiously-ahead",
    "title": "• 8. PCA – Gotchas",
    "section": "",
    "text": "Understanding the structure of the data.\n\nConnecting this structure to a biological interpretation (but watch out for artifacts – see below).\nThinking about what could have gone wrong in this PCA (I specifically noted to be wary of a horseshoe shape).\n\n\nPCA evaluation checklist:\n Could structure be an artifact? \nIn large scale datasets (such as -omics, or cases in which many people collected data or data are stratified), some portion of the variance might be caused by artifacts like who collected the data, which run of an instrument was used, what time the data was generated etc. rather than the motivating idea.\n\n\nYour life will be better if you try to randomly spread such potential artifacts randomly across your more exciting biological factors, rather than identifying their potential impact later.\nBefore rushing to interpret results biologically always look into the boring explanations. You can check for these issues by making exploratory data visualizations or evaluating associations between such boring variables (observer, batch, time of day etc) and PCs of interest.\n\n Was the interpretation of 2D shape justified? \nSometimes PCA plots show compelling shapes — curves, clusters, gradients — that are tempting to interpret biologically. But be cautious: these shapes don’t always reflect complex biology. For example, Luca Cavalli-Sforza famously used PCA to study human genetic variation. While foundational, some of his interpretations of the shapes in PC space were unjustified — Novembre & Stephens (2008) found that these patterns can arise from much simpler processes. Similarly, 2D structure can arise from nonlinearity in just one dimension.\nWhen you see an interesting shape in a PCA plot, don’t jump straight to a complex biological story. Ask if it could be generated by a simple process.\n\n Could the way in which missing data were handled impact PCs? \nIf you want to run a PCA with missing data you have some decisions to make. Ideally, no data are missing, but of course this is not always possible – for example in genomic analyses, data are often missing because not all sites are sequenced in all individuals.\n\nAssigning each missing data point to the mean for that trait is a common practice, but if some individuals are missing more data than others they will be brought towards the center of PC space. This can be misleading — it makes individuals with lots of missing data appear artificially close to the center of PC space, even if their true trait values are far from average. This is a known for the types of population genomic analyses I do (Yi & Latch (2022)).\n\nAlternatively imputing the missing data can give us more confidence in the shape of our PCs than we deserve and could mislead us by assigning wrong values to an individual’s traits.\n\nOne simple check is to plot the value of a PC on the y-axis against the proportion of missing data on the x-axis. Any linear or nonlinear trend is worrying.\n\n Did you do something silly?  This is somewhat embarrassing but I almost added id as a variable in this PCA. Of course id should not be numeric, and is meaningless. So if I made this mistake and got super lucky there is just some noise in PC1. But it could be way worse, if id non-randomly assigned, it gets a meaningful weighting in PC space and we get super confused. So be sure you don’t make any such silly mistakes.\n\n Should data (not) be scaled? \nWe discussed this at length in the previous section — we usually scale variables so that differences in scale and variability don’t mislead us. But do we always want to give equal weight to variables with very different amounts of variance?\nTake a PCA based on SNPs: scaling the data means that variation at a locus with a very rare allele is treated as just as important as variation at a highly polymorphic locus. That might not be a good idea. In some cases — like when differences in variance reflect meaningful biological differences, or when variables are on the same scale already — you may want to skip scaling. But be cautious – Lever et al. (2017) suggest that when variables are similar, we should not scale the data.\n\n Are variables redundant? \nPCA is most useful when variables are correlated, since it finds axes that summarize that structure. that capture the structure of correlations in the data. But data should not be redundant.\nFor example the full Clarkia dataset contains measurements of both petal area and petal circumference. Such variables are very similar, so putting them both into a PCA is essentially double-counting flower size. This redundancy means that a PCA will allocate more variance to the shared dimension, exaggerating its importance. Consider reducing highly correlated traits to one value (e.g. their mean) or removing the less interpretable variable. How correlated is too correlated? There is no hard and fast answer, but I suggest considering collapsing or removing variables when the absolute value of their correlation, \\(|r|\\), exceeds 0.9.\n\n What data went into the PCA and how even was sampling?  PCA finds combinations of traits that best capture the variance in the data. Thus, is quite sensitive to the sampling scheme. I expand on this below!",
    "crumbs": [
      "8. Ordination",
      "• 8. PCA -- Gotchas"
    ]
  },
  {
    "objectID": "book_sections/ordination/pcaIII.html#pca-depends-on-what-you-put-into-it",
    "href": "book_sections/ordination/pcaIII.html#pca-depends-on-what-you-put-into-it",
    "title": "• 8. PCA – Gotchas",
    "section": "PCA depends on what you put into it",
    "text": "PCA depends on what you put into it\nWhen John Novembre showed that a PCA of genetic variation of Europeans looked a lot like a map of Europe, he knew what he was doing. He made a few specific decisions in his sampling and analysis, including – making sure that all sampled individuals had all four great grandparents from the same location, and that sampling was even across Europe. Had John sampled mostly Spaniards with a few other individuals spread around from across Europe, PCs would not make a map of Europe - but rather a somewhat distorted map of Spain. This is all to say that the details of sampling impact results of a principal component analysis.\nJust as sampling matters, so does the choice of what variables go into the PCA. John used many genetic markers from across the genome. A PCA based on the ABO locus would find clusters of blood groups, while a PCA based on morphology would look different in some other way.\nFigure 1 uses data from a natural hybrid zone between Clarkia xantiana subspecies to illustrate the importance of sampling effort. Panels A-C show a PC-plot based off of three floral traits – petal area, anther stigma distance, and the time between pollen release and ovule receptivity – when different numbers of parviflora and xantiana are sampled. The top left panel (Figure 1 A) shows that with 45 samples of each subspecies from site SAW, PC1 captures 84.6% of the variance in the data. The three “xantiana” samples with PC1 values near zero may be first generation hybrids. Figure 1 shows that the variance attributable to PC1 decreases when sampling 45 parviflora and 3 xantiana, and Figure 1 C shows that this decreases further when sampling 3 parviflora and 45 xantiana. Figure 1 D shows the average proportion variance explained by PC1 across different sampling efforts. This illustrates the broader point: PCA doesn’t tell us about populations, but about variation in the data we choose and how we sample.\n\n\n\n\n\n\n\n\nFigure 1: Sampling effort alters the results of PCA. Panels A–C show principal component plots based on three floral traits in Clarkia xantiana (petal area, anther–stigma distance, and timing of pollen release vs. ovule receptivity). Data are from the “Sawmill Road” hybrid zone, with samples from both subspecies: parviflora is represented by red “P”s and xantiana by blue “X”s. Each panel presents a different sampling configuration: Panel A includes 45 individuals of each subspecies, Panel B includes 45 parviflora and 3 xantiana, and Panel C includes 3 parviflora and 45 xantiana. The proportion of total variance explained by PC1 decreases as the sampling becomes more uneven. Panel D summarizes this trend across combinations of sample sizes ranging from 1 to 45 for each subspecies. The color scale represents the average proportion of variance explained by PC1 across ten subsampling efforts for each parameter combination. Red letters A, B, and C correspond to Panels A–C.\n\n\n\n\n\n\n\n\n\nLever, J., Krzywinski, M., & Altman, N. (2017). Principal component analysis. Nature Methods, 14(7), 641–642. https://doi.org/10.1038/nmeth.4346\n\n\nNovembre, J., & Stephens, M. (2008). Interpreting principal component analyses of spatial population genetic variation. Nature Genetics, 40(5), 646–649. https://doi.org/10.1038/ng.139\n\n\nYi, X., & Latch, E. K. (2022). Nonrandom missing data can bias principal component analysis inference of population genetic structure. Mol Ecol Resour, 22(2), 602–611. https://doi.org/10.1111/1755-0998.13498",
    "crumbs": [
      "8. Ordination",
      "• 8. PCA -- Gotchas"
    ]
  },
  {
    "objectID": "book_sections/ordination/pcalternatives.html",
    "href": "book_sections/ordination/pcalternatives.html",
    "title": "• 8. PCAlternatives",
    "section": "",
    "text": "When data aren’t (all) continuous\nThe PCA framework is enormously popular because it provides a straightforward approach for working with multidimensional data. This is becoming even more relevant as the “big data” or “omic” era makes the collection of massive datasets commonplace. However, despite the incredible popularity of principal component analysis, it is not always the right tool for the job. PCA works by combining all traits into new “summary traits” - linear combinations of the originals, each with its own set of weightings. This means it assumes that the main patterns in the data can be captured by adding up traits in the right proportions. But this assumption doesn’t always hold. When data are nonlinear, categorical, or behave strangely (as is often the case in ecological datasets), PCA can give misleading results.\nIn this section, we introduce alternatives to PCA. Each alternative aims to solve a specific limitation of PCA. But before diving too deep into this section I want to warn you that this is both non-exhaustive (there are even more PCA-like approaches), and fairly superficial (I only briefly discuss these techniques). My goal here is to give you enough information to know what these methods do, when and why they are used, how they differ from PCA (and each other), and references / links so you can learn more.\nPCA assumes that data are continuous and have nice distributions. Although PCA can still be run when these assumptiosn are not met (John Novembre’s map of Europe was generated from 0/1/2 data – noting the number of ‘alternative’ alleles at a locus), there are PCA -like approaches made for categorical data and for a mix of data types.",
    "crumbs": [
      "8. Ordination",
      "• 8. PCAlternatives"
    ]
  },
  {
    "objectID": "book_sections/ordination/pcalternatives.html#when-data-arent-all-continuous",
    "href": "book_sections/ordination/pcalternatives.html#when-data-arent-all-continuous",
    "title": "• 8. PCAlternatives",
    "section": "",
    "text": "Multiple Correspondence Analysis (MCA)\nMCA is appropriate when all your variables are categorical, especially nominal variables with multiple categories. MCA applies a method similar to PCA to uncover patterns of association among the categorical variables and reduce dimensionality.\nYou can run an MCA using the mca() function from the MASS package in R, or for more in-depth analyses and nicer figures you may want to use the MCA function in the factoMineR package. You interpret the output much like a PCA. That is, each axis summarizes shared patterns across the variables, and the biplot shows which samples and categories tend to cluster together.\nThe code below and the resulting figure (Figure 1) show a worked example using the parviflora RIL dataset. The MCA analysis includes three categorical traits: visited (whether the plant received any visits), petal_color, and petal_area (binned as small / medium / large). Points are colored by whether each plant made at least one hybrid. Dim 1 (which is like PC1) separates plants with pink petals and visits from those with white petals and no visits — and aligns closely with whether or not the plant made a hybrid.\n\n\nCode for selecting data for MCA from RILs planted at GC\nlibrary(forcats)\nril_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_rils.csv\"\nril_data &lt;- readr::read_csv(ril_link) |&gt;\n  dplyr::mutate(growth_rate = case_when(growth_rate ==\"1.8O\" ~ \"1.80\",\n                                          .default = growth_rate),  \n                growth_rate = as.numeric(growth_rate),\n                made_hybrid = prop_hybrid &gt; 0)\n\ngc_rils_4_mca  &lt;- ril_data |&gt;\n  na.omit()|&gt;\n  filter(location == \"GC\")|&gt;\n  mutate(visited = ifelse(mean_visits&gt;0,\"visited\",\"not visited\"), \n         made_hybrid = ifelse(made_hybrid,\"made hybrid\",\"no hybrid\"),\n         petal_area_qual = cut(petal_area_mm,c(0,53.5,66.3,Inf)))  |&gt;\n  mutate(petal_area_qual = fct_recode(petal_area_qual, \n                                       small_petal = \"(0,53.5]\", \n                                       medium_petal = \"(53.5,66.3]\",\n                                       large_petal = \"(66.3,Inf]\"))|&gt;\n  dplyr::select(made_hybrid,  visited , petal_color, petal_area_qual)\n\n\n\nlibrary(FactoMineR)\nlibrary(factoextra)\nlibrary(forcats)\nlibrary(patchwork)\nmca_result &lt;- gc_rils_4_mca |&gt; \n  mutate_all(as.factor)|&gt;                      # The MCA function needs variables to be factors\n  MCA(quali.sup = \"made_hybrid\",graph = FALSE) # quali.sup  means ignore this column (made hybrid)\n                                               # we want this to look at our MCA, but not to go into MCA\n# Visualize individuals (samples)\nmca_plot &lt;- fviz_mca_ind( mca_result,  habillage = \"made_hybrid\", \n    repel = TRUE,col.var = \"darkgrey\" ,palette = \"Dark2\")+\n  theme(legend.position = \"bottom\", aspect.ratio = 25/37)\nmca_loadings&lt;- fviz_mca_var(mca_result, repel = TRUE,col.quali.sup = \"black\")\nmca_plot + mca_loadings\n\n\n\n\n\n\n\n\n\nFigure 1: Multiple Correspondence Analysis (MCA) of categorical traits in parviflora RILS. Left panel: Individuals are plotted in MCA space based on Dim 1 (37% variance explained) and Dim 2 (25%). Each point represents a RIL, colored by whether it made hybrid seed (made_hybrid) or not. Lines connect individual labels to the data point they are assocaited with. Right panel: Positions of each variable category in the same MCA space. Categories closer together (e.g., pink and visited) tend to co-occur. Dim 1 separates lines with small petals, no visits, and no hybrids (left) from those with larger petals and more visits (right).\n\n\n\n\n\n\n\nFactor Analysis of Mixed Data (FAMD)\nSo, PCA works for continuous variables and MCA works for categorical variables, but what if your dataset has both? Don’t despair, Factorial Analysis of Mixed Data (FAMD) allows us to analyze datasets that include both quantitative and categorical variables. FAMD is a blend of PCA and MDA giving appropriate weights to each. Running this on our parviflora RILs at GC with categorical variables visited and petal_color, and continuous variables petal_area_mm, asd_mm, growth_rate, stem_dia_mm, and lwc, we see that large values in Dim 1 are associated with making hybrid seed ( Figure 2 ).\nFAMD scales and balances the contributions of each variable type so that no one type dominates. You interpret the output like PCA: samples that cluster together in the reduced space tend to share trait combinations — whether continuous, categorical, or both. You can run FAMD with the FAMD() function in the FactoMineR package and make plots with functions in factoextra package.\n\n\nCode for selecting data for FAMD from RILs planted at GC\ngc_rils_4_famd  &lt;- ril_data |&gt;\n  na.omit()|&gt;\n  filter(location == \"GC\")|&gt;\n  mutate(visited = ifelse(mean_visits&gt;0,\"visited\",\"not visited\"), \n         made_hybrid = ifelse(made_hybrid,\"made hybrid\",\"no hybrid\"))|&gt;  \n  select(made_hybrid, visited, petal_color, petal_area_mm, asd_mm, growth_rate,  stem_dia_mm, lwc)\n\n\n\nlibrary(FactoMineR);  library(factoextra);  library(forcats); library(patchwork)\nfamd_result &lt;- FAMD(gc_rils_4_famd, sup.var = \"made_hybrid\" ,graph = FALSE) # Make FAMD\n\n# Visualize results\nfamd_plot &lt;- fviz_famd_ind( famd_result,  habillage = \"made_hybrid\", \n    repel = TRUE,palette = \"Dark2\",col.quali.var = \"white\")   +     # Plot inds\n  theme(legend.position = \"bottom\", aspect.ratio = 21.1/25.9)         \nfamd_scree_plot &lt;- fviz_screeplot(famd_result)\nfamd_quant_loadings&lt;- fviz_famd_var(famd_result, \"quanti.var\",repel = TRUE) # Plot loadings of quantitative traits\nfamd_qual_loadings&lt;- fviz_famd_var(famd_result, \"quali.var\",repel = TRUE,col.quali.sup = \"black\") # Plot loadings of qualitative traits\n\n# Combine plots\n(famd_plot + famd_quant_loadings) / (famd_scree_plot+ famd_qual_loadings) + \n  plot_layout(heights = c(3, 2))\n\n\n\n\n\n\n\n\n\nFigure 2",
    "crumbs": [
      "8. Ordination",
      "• 8. PCAlternatives"
    ]
  },
  {
    "objectID": "book_sections/ordination/pcalternatives.html#pcalternatives_pcoaANDnmds",
    "href": "book_sections/ordination/pcalternatives.html#pcalternatives_pcoaANDnmds",
    "title": "• 8. PCAlternatives",
    "section": "Distance-based methods for nonlinear numeric data: PCoA and NMDS",
    "text": "Distance-based methods for nonlinear numeric data: PCoA and NMDS\nIn many cases, data have complex distributions that might do weird things to PCA. For example, Brooke has collected data not just on the number of pollinator visits, but on how many of each species visited each plant. Similarly, studies of the microbiome or environmental microbiology, often describing the relative frequency of different microbes in various environments.\n\n\n\nEuclidean distance: The straight-line distance between two points. Great when your variables are continuous and on the same scale (e.g., plant height and leaf area). Results of a PCoA based on euclidean distance are often nearly identical to a PCA.\nHamming distance: Counts how many features differ. Often used for binary strings (e.g., how many loci differ in genotype).\nBray–Curtis dissimilarity: Compares counts, like how many visits each plant got from each pollinator or how many read counts of each so-called “observed taxonomic unit” (OTU). Bray–Curtis dissimilarity is sensitive to both presence and abundance.\nJaccard distance: Looks at shared vs. unique elements. Good for presence–absence data (e.g., which microbes are present in each soil sample).\n\nOne approach to deal with such cases is to develop a “distance matrix” – an \\(N \\times N\\) table that shows how different each pair of samples is from one another. There is no universal way to define a distance – the right distance depends on your data (see options on our right). We then summarize this \\(N \\times N\\) distance matrix into a lower-dimensional summary of the data.Two common approaches – PCoA and NMDS do this in slightly different ways.\n\nPrincipal Coordinate Analysis (PCoA)\nPrincipal Coordinate Analysis (PCoA) runs an analysis similar to PCA on any distance matrix you give it. Specifically it takes the distance matrix and “double-centers” the data (A math trick that allows us to treat a distance matrix like traits for a PCA) and then runs an eigenanalysis.\n\nRunning a PCoA: The most common way to run a PCoA in R is to use the vegdist() function in the vegan package to make a distance matrix, and then provide this distance matrix to the cmdscale() function. Follow this link from Bakker (2024) for a bit more information.\nInterpreting PCoA results: The output of a PCoA is nearly identical to PCA, but the axes are principal coordinates, and we do not consider trait “loading” because we do not consider traits.\n\n\n\nNon-metric Multidimensional Scaling (NMDS)\n\n\n\n\n\n\n\n\n\nFigure 3: NMDS plot of microbial community composition from environmental biofilm samples. Points represent biofilm samples collected across three sites (D1, D3, D6) and treatment types (fluid, inert control, mineral), plus an ambient control. The plot is based on Bray–Curtis dissimilarity of OTU (operational taxonomic unit) counts from environmental sequencing. Groupings indicate that microbial community composition differs across both location and treatment. Adapted from a blogpost by Caitlan Casar.\n\n\n\n\nWhen data are so messy that any actual distance is difficult to interpret, we turn to NMDS. Like PCoA NMDS starts with a distance matrix, but then instead of using eigenanlysis, NMDS uses a complex algorithm to preserve the rank similarity of pairs of samples in a pre-specified (usually two or three) number of dimensions. The NMDS approach is useful because it can handle messy data, but using this approach means we lose the concept of “variance explained”, and interpreting results as distances between samples.\n\nRunning a NMDS: Like PCoA, we begin an NMDS analysis by using the vegdist() function to make a distance matrix. We then use vegan’s and then provide this distance matrix to the metaMDS() function. Follow this link from Bakker (2024) for a bit more information.\nInterpreting NMDS results: Rather than reporting the percent variance explained, NMDS calculates stress – a summary of how well the NMDS analysis summarizes the data. Stress values less than 0.1 mean that NMDS is a reasonable summary, while values greater than 0.2 mean that NMDS does not do a good job of summarizing our data. We can also visualize data points in the MDS1 and MDS2 space (as in Figure 3) to understand the similarity between samples.",
    "crumbs": [
      "8. Ordination",
      "• 8. PCAlternatives"
    ]
  },
  {
    "objectID": "book_sections/ordination/pcalternatives.html#pcalternatives_tsneANDumap",
    "href": "book_sections/ordination/pcalternatives.html#pcalternatives_tsneANDumap",
    "title": "• 8. PCAlternatives",
    "section": "Capturing fine structure in high-dimensional space with t-SNE and UMAP",
    "text": "Capturing fine structure in high-dimensional space with t-SNE and UMAP\nAs datasets get larger, more heterogeneous, and more complex, there’s a growing need for fast ways to reveal structure in high-dimensional data. For example, single-cell RNA sequencing actually measures gene expression in thousands of individual cells—one cell at a time—while keeping data from each cell separate. Making sense of this high-dimensional data is challenging, and so we need techniques that allow for effective visualization. Two popular tools—t-SNE and UMAP—have become widely used for this purpose, and are now applied in many fields beyond single-cell analysis. These approaches can identify clusters in the data which might correspond to different cell types or states.\n\nt-SNE is a technique for visualizing high-dimensional data by preserving local similarities - it tries to make sure that points that are close in the original space stay close in the plot. This means t-SNE is great at finding clusters, but the distances between clusters are mieaningless. That is, while points within a cluster are typically similar, two clusters being far apart (or close together) doesn’t necessarily mean anything. This nice explanation of how to carefully conduct and evaluate a t-SNE analysis is worth checking out if you want to learn more. But t-SNE is rarely used now that we have UMAP.\nUMAP has largely taken over from t-SNE in many areas because it’s much faster and usually does a better job of preserving both local and some global structure. Like t-SNE, UMAP helps us find and visualize clusters. But unlike t-SNE, clusters that are closer together in UMAP space are often more similar, though the distances between them still don’t have a strict, interpretable meaning (as they would in PCA or PCoA). Read this fantastic UMAP tutorial, if you plan on running a UMAP, and look into this (slightly) more technical explanation by the people who invented this approach (Healy & McInnes (2024)).\n\nRunning UMAP in R: The most commonly used R package for UMAP is uwot, which is also used behind the scenes by the popular single-cell analysis package Seurat.\n\nThe umap2() function in uwot is preferred—it includes better defaults than the older umap() function.\nParameter choices matter, so it’s worth experimenting with options like n_neighbors (which affects how much local structure is preserved) and min_dist (which controls how tightly points are packed together in the plot).\n\n\n\n\n\n\n\n\nAn example of single-cell RNA-seq data from peripheral blood mononuclear cells visualized by PCA, t-SNE, and UMAP are shown below, taken from a blogpost by Matthew N. Bernstein.\n\n\n\n\n\nThe beautiful plots made by UMAP do not always truly highlight important biology. Read this warning in Chari (2023) before over-interpreting your results.\nMore broadly, with complex and non-transparent methods like t-SNE and UMAP it’s always worth looking for additional lines of evidence to ensure that we are not being misled.\n\n\nJust because you’ve reduced your multidimensional data into two dimensions doesn’t mean everything in the plot has a clear biological meaning. A few common pitfalls:\n\nOverinterpreting t-SNE and UMAP plots These methods are built for visualization, not interpretation. The axes have no meaning. The global structure (e.g., distances between faraway clusters) often doesn’t reflect biological distance at all — only the local neighborhood structure is trustworthy.\nReading direction into NMDS axes NMDS axes can flip, rotate, or stretch between runs. The relative arrangement of samples matters, not the absolute axis values or orientations. Always look at patterns, not coordinates.\nAssuming PCA alternatives give you “components” like PCA Only some methods (e.g., FAMD, MCA) return interpretable “axes” with loadings like PCA. For others, like NMDS and t-SNE, you don’t get a clear mapping from traits to axes.\nInterpreting distance as Euclidean when it’s not If your method used Jaccard or Bray–Curtis distances, don’t interpret sample spacing as if it came from straight-line distances in trait space.\n\nIn short: don’t read more into the plots than the method can give you. Use them to explore, not to conclude.\n\n\n\n\n\nBakker, J. D. (2024). Applied multivariate statistics in R. Pressbooks.\n\n\nChari, L., Tara AND Pachter. (2023). The specious art of single-cell genomics. PLOS Computational Biology, 19(8), 1–20. https://doi.org/10.1371/journal.pcbi.1011288\n\n\nHealy, J., & McInnes, L. (2024). Uniform manifold approximation and projection. Nature Reviews Methods Primers, 4(1), 82. https://doi.org/10.1038/s43586-024-00363-x",
    "crumbs": [
      "8. Ordination",
      "• 8. PCAlternatives"
    ]
  },
  {
    "objectID": "book_sections/ordination/ordination_summary.html",
    "href": "book_sections/ordination/ordination_summary.html",
    "title": "• 8. Ordination summary",
    "section": "",
    "text": "Chapter summary\nLinks to: Summary. Chatbot tutor. Questions. Glossary. R functions. R packages. More resources.\nModern biological datasets often have more traits than we can look at or make sense of directly. Ordination approaches help us see structure in these large and complex datasets. These techniques reduce dimensionality: they summarize patterns across many variables into just a few new ones that soak up as much variation in the data as possible. When data are numeric and well-behaved, we typically use PCA. But when variables are categorical, mixed, or messy (as is common in ecological or genomic data), PCA can mislead. In these cases, alternatives like MCA, FAMD, PCoA, NMDS, t-SNE, and UMAP (among others) may be more appropriate. Some work with distances, others with probabilities. Some produce axes you can interpret; others are just for visualizing structure. Whatever method you use, make sure you understand its assumptions, limitations, and how to interpret its output - such approaches are no better than the scientist using them.",
    "crumbs": [
      "8. Ordination",
      "• 8. Ordination summary"
    ]
  },
  {
    "objectID": "book_sections/ordination/ordination_summary.html#ordination_summarySummary",
    "href": "book_sections/ordination/ordination_summary.html#ordination_summarySummary",
    "title": "• 8. Ordination summary",
    "section": "",
    "text": "Chatbot tutor\n\nPlease interact with this custom chatbot (link here) I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you got what you needed from it.",
    "crumbs": [
      "8. Ordination",
      "• 8. Ordination summary"
    ]
  },
  {
    "objectID": "book_sections/ordination/ordination_summary.html#ordination_summaryPractice_questions",
    "href": "book_sections/ordination/ordination_summary.html#ordination_summaryPractice_questions",
    "title": "• 8. Ordination summary",
    "section": "Practice Questions",
    "text": "Practice Questions\nTry these questions! By using the R environment you can work without leaving this “book”. To help you jump right into thinking and analysis, I have loaded the necessary data, cleaned it some, and have started some of the code!\n\nWarm up\nQ1) For which of these cases is a PCA most appropriate?\n\n You want to measure how strongly two continuous variables are correlated. You want to reduce a dataset with many continuous variables to a few variables that capture most of the variation. You have messy data with missing values and a mix of continuous and categorical variables. You’re hoping to discover natural clusters in your data.\n\n\n\nClick here for explanation\n\n\nIf you chose option 1, you’re not totally off base — PCA does involve correlations between variables. But if all you want is to measure the strength or direction of association between two continuous variables, a correlation coefficient (like Pearson’s r) is simpler, more interpretable, and more appropriate.\nIf you chose option 2, congrats 🥳, This is the core purpose of PCA — reducing dimensionality by summarizing structure in multivariate continuous data.\nIf you chose option 3, you’re describing a tricky (but common) dataset. Unfortunately, PCA isn’t great at handling messy or mixed data. It requires complete, numeric data — missing values break it, and categorical variables need special handling.\nIf you chose option 4, that’s a common motivation and frequent misinterpretation. Sure, PCA might reveal clusters in the data, but that’s not what PCA is for. If clustering is your goal, use clustering methods (e.g. k-means, hierarchical clustering, which we will get to later – i hope) instead.\n\n\n\n\nExample Plot 1\nConsider the plot below - a PCA of the iris data set, as you answer questiosn two through six.\n\n\n\n\n\n\n\n\n\n\nQ2) Which of the plots above is most appropriate?\n\n A: It stretches PC2, making subtle vertical differences easier to see. B: It keeps the axes roughly square, which feels visually balanced and avoids exaggeration. C: It sets the axes proportional to the percent variance explained, making distances and angles in the plot geometrically meaningful. There is no 'best' because plot choice depends on audience and goals.\n\nQ3) Based on the plot(s) above which species appears to have the largest petals (i.e., petal length and width)?\n\n setosa versicolor virginica 🤷 PCA doesn’t tell us anything about trait values.\n\nQ4) Based on the plot(s) above, how would you interpret PC2?\n\n PC2 reflects petal area: higher values mean larger petals PC2 reflects petal area: higher values mean smaller petals PC2 reflects sepal area: higher values mean larger sepals PC2 reflects sepal area: higher values mean smaller sepals There is no clear biological interpretation of PC2 from this plot We should never attempt to interpret PCs biologically PC2 should be ignored because it explains so little of the variance.\n\nQ5 The correlation between petal width and petal area is 0.96. It is clear what to do in such cases, but one of these options is least justifiable. Which one?\n\n Don't change anything. These are different traits! Drop one of the petal traits (it doesn't really matter which one). If you drop one petal trait, be sure to drop a sepal trait Drop both petal length and width, and replace them with petal area.\n\n.\nBONUS: What would you do?\n\n\n\n\nQ6 Write a brief paragraph summarizing this plot.\n\n\n\n\n\n\nExample Plot 2\nI made a PC from wild Clarkia (not RILs) collected at the Sawmill road hybrid zone. Note the data are centered and scaled. Use this webR workspace to answer question seven (below).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ7) The plot above appears to show three clusters, but there is a problem (or two). What is th biggest problem?\n\n PCA is not meant for finding clusters PC2 only explains 19% of the variance so we should not pay much attention to it The middle cluster is largely made up of missing data Some variables are categorical so we should have run and FAMD analysis Something went into our PC that shouldn't have\n\n\n\nHint\n\nLook into trait loading, either by typing tidy(saw_pca, matrix = \"loadings\") or by adding loadings = TRUE,  loadings.label = TRUE to autoplot().\n\n\n\nExplanation\n\nYou can see that PC2 is basically id – a variable it is not anything we ever should not consider as biologically interesting.\n\n\n\n\nExample Plot 3\nUse the data below to answer questions eight and nine.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ8) What went wrong in this plot (Note, choose the worst mistake for this dataset)\n\n Nothing - it's great! PC1 splits the data into two groups! Axes are not proportional to PVE. Something went wrong. with PC1 - I should figure it out. The data are not centered and scaled.\n\nQ9) Fix all the mistakes above (not just the biggest one) and remake this plot. What percent of variability in the data is attributable to PC2 \n\n\nHint 1\n\nLook into trait loading, either by typing piping PCA output to tidy( matrix = \"loadings\") or by adding loadings = TRUE,  loadings.label = TRUE to autoplot().\n\n\n\nHint 2\n\nLook into at the variable associated with PC1. Remove the most filter() for all data except the mostextreme value.\n\n\n\nHint 3\n\nMake a pca, be sure to center and scale your axes!\n\n\n\n\nPCAlternatives\n\nQ10) In which of the following ordination methods is it most appropriate to interpret the distance between points in the plot as representing similarity between samples?\n\n NMDS, because it’s designed for messy ecological data. t-SNE, because it preserves local structure. PCA and PCoA — but only if axes are scaled proportional to variance explained. All of them — that’s what ordination plots are for!\n\n\n\nExplanation\n\nPCA (Principal Component Analysis) and PCoA (Principal Coordinate Analysis) use Euclidean geometry, so distances between points are interpretable (although PCoA distanses are in units of distance given by the distance matrix) — if you scale the axes to reflect the variance explained. If you don’t, you can distort distance and angle relationships.\nNMDS doesn’t preserve absolute distances — only the rank order of pairwise distances (who’s closest to whom). The same goes for t-SNE and UMAP, which emphasize local neighborhood structure but distort global distances. So while clusters might look compelling, distances between them often lie.\n\nYou’ve got a huge dataset from museum specimens: 4 continuous variables (e.g., bill length, wing chord, body mass, tarsus length)\n2 ordered categorical variables (e.g., molt stage from 1–5, plumage brightness from dull to vibrant)\nSeveral missing values in most rows\nYou want to visualize broad patterns and see if anything jumps out. Which approach is the least bad?\n\n PCA - just ignore missing values and let prcomp() do its thing. FAMD - if you impute missing data first, it can handle mixed types and give interpretable dimensions. UMAP - it will figure out the structure automatically. NMDS - it ignores variable types and missing values, so it must be fine.\n\n\n\nExplanation\n\nNone of these is perfect, but FAMD is probably your best bet if you carefully impute missing values (e.g., using missMDA::imputeFAMD()). PCA drops all incomplete rows; NMDS can’t handle mixed variable types or NA; UMAP will happily embed noise and call it structure.",
    "crumbs": [
      "8. Ordination",
      "• 8. Ordination summary"
    ]
  },
  {
    "objectID": "book_sections/ordination/ordination_summary.html#ordination_summaryGlossary_of_terms",
    "href": "book_sections/ordination/ordination_summary.html#ordination_summaryGlossary_of_terms",
    "title": "• 8. Ordination summary",
    "section": "📊 Glossary of Terms",
    "text": "📊 Glossary of Terms\n\n\n📚 1. Concepts of Ordination\n\nOrdination A general term for methods that reduce complex, high-dimensional data into fewer dimensions to help us find patterns or structure. Useful for summarizing variation, visualizing groups, or detecting gradients in data.\nDimensionality Reduction The process of collapsing many variables into a smaller number of “components” or “axes” that still explain most of the relevant variation.\nDistance Matrix A table showing how different each pair of observations is, based on some measure (e.g., Euclidean, Bray-Curtis). The starting point for many non-linear ordination methods.\nScores The new coordinates of each sample on the reduced axes (like PC1, PC2, or NMDS1, NMDS2). These are what we usually plot.\nLoadings The contribution of each original variable to a principal component. They tell us what the new axes “mean” in terms of the original traits.\nProportion of Variance Explained (PVE) How much of the total variation in the data is captured by each axis (e.g., PC1, PC2). Higher values suggest the axis captures an important pattern.\n\n\n\n🔢 2. PCA and Matrix-Based Methods.\n\nPrincipal Component Analysis (PCA) A technique that finds new axes (principal components) that capture as much variation as possible in your numeric data. Based on eigenanalysis of the covariance or correlation matrix.\nEigenvalues Numbers that describe how much variance is associated with each principal component.\nEigenvectors The directions (in trait space) along which the data vary the most — the basis of your new axes (PC1, PC2…).\nCentering and Scaling Centering subtracts the mean of each variable; scaling divides by the standard deviation. This standardizes variables to make them comparable before running PCA.\nSingular Value Decomposition A matrix factorization technique that expresses a matrix as the product of three matrices: U, D, and Vᵗ. PCA is often computed using SVD of the centered (and sometimes scaled) data matrix.\n\n\n\n🎲 3. Categorical and Mixed Data\n\nMultiple Correspondence Analysis (MCA) A PCA-like method for datasets made up of categorical variables (e.g., presence/absence, categories). Often used for survey or genetic marker data.\nFactor Analysis of Mixed Data (FAMD) Combines ideas from PCA and MCA to handle datasets with both continuous and categorical variables.\nMultiple Factor Analysis (MFA) Used when your data come in blocks (e.g., gene expression + morphology + climate). It balances across blocks to find shared structure.\n\n\n\n🧭 4. Distance-Based Ordination.\nPrincipal Coordinates Analysis (PCoA) finds axes that best preserve distances between samples, based on any distance matrix (e.g., Bray-Curtis, UniFrac, Jaccard). Results resemble PCA but from a different starting point.\nNon-metric Multidimensional Scaling (NMDS) A distance-based method that tries to preserve the rank order of distances. Great for ecology or microbiome data. Axes have no fixed meaning — they just help you visualize structure.\n\n\n🌐 5. Nonlinear Embedding Methods.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding) A nonlinear method that preserves local structure (similarity between nearby points) but often distorts global structure. Good for finding clusters but not for understanding axes.\n**UMAP (Uniform Manifold Approximation and Projection)* A newer nonlinear method like t-SNE, but faster and often better at preserving both local and global structure. Often used for visualizing high-dimensional data like RNA-seq or image features.",
    "crumbs": [
      "8. Ordination",
      "• 8. Ordination summary"
    ]
  },
  {
    "objectID": "book_sections/ordination/ordination_summary.html#ordination_summaryNew_r_functions",
    "href": "book_sections/ordination/ordination_summary.html#ordination_summaryNew_r_functions",
    "title": "• 8. Ordination summary",
    "section": "🛠️ Key R Functions",
    "text": "🛠️ Key R Functions\n\n\n🔢 Principal Component Analysis (PCA)\n\nprcomp(): Performs PCA for you. Rememmber to always type center = TRUE, and to usually type scale = TRUE, (unless you think scaling is inappropriate.\neigen(): Find eigenvectors and eigenvalues of a matrix. Pair this with cov() or cor() if you want to run a PCA without prcomp().\n\naugment() In the broom package: Adds PCA scores to the original data for plotting or further analysis.\ntidy() In the broom package: Makes a tidy tibble for key outputs the prcomp output (The specific outpput depends on the matrix = option).\n\ntidy(, matrix = \"pcs\"): Makes a tibble showing the variance attributable to each PC.\n\ntidy(, matrix = \"loadings\"): Makes a tibble describing the loading of each trait onto each PC.\n\ntidy(, matrix = \"scores\"): Makes a tibble showing the value of each sample on each PC. This is simple a long format version of the output of augment().\n\nautoplot(): In the ggfortify package. Plots PCA results with groupings, or loadings arrows.\ntheme(aspect.ratio = PVE_PC2/PVE_PC1): This is a specific use of ggplot2’s theme() function, which we will discuss more later. Be sure to add this to all of your PC (or PC-like) plots to help make sure readers do not misinterpret your results. Find PVE_PC1 and PVE_PC2 from your eigenvalues in you prcomp() output (or more easily tidy(, matrix = \"pcs\"):).\n\n\n\n\n🔎 Categorical & Mixed Data Ordination\n\nMCA(): In the FactoMineR package performs Multiple Correspondence Analysis for categorical variables.\nFAMD(): In the FactoMineR package performs a Factor Analysis of Mixed Data (categorical + continuous variables).\nfviz_... In the factoextra package are helper functions to make plots from MCA and FAMD output.\n\n\n\n\n🧭 Distance-based methods.\n\nvegdist(): In the vegan package makes a distance matrix for many common distance measures.\ncmdscale() Allows for classical multidimensional scaling on a distance matrix (as in PCoA).\nmetaMDS() In the vegan package performs NMDS on a distance matrix.\n\n\n\n\n🌐 UMAP & Visualization\n\numap2() In the uwot packageerforms Uniform Manifold Approximation and Projection (UMAP).",
    "crumbs": [
      "8. Ordination",
      "• 8. Ordination summary"
    ]
  },
  {
    "objectID": "book_sections/ordination/ordination_summary.html#ordination_summaryR_packages_introduced",
    "href": "book_sections/ordination/ordination_summary.html#ordination_summaryR_packages_introduced",
    "title": "• 8. Ordination summary",
    "section": "R Packages Introduced",
    "text": "R Packages Introduced\n\n\nbroom: Tidies output from PCA (tidy(), augment()) into easy-to-use data frames.\nggfortify: Simplifies plotting of PCA and other multivariate objects using autoplot().\nFactoMineR: Provides functions for multivariate analyses like MCA, FAMD, and MFA.\nfactoextra: Makes it easy to visualize outputs from FactoMineR analyses (fviz_mca_ind(), fviz_famd_var(), etc.).\nvegan: Used to compute distance matrices (vegdist()), run PCoA (cmdscale()) and NMDS (metaMDS()).\nuwot: Performs UMAP for dimensionality reduction of high-dimensional data.\n\n\n\nAdditional resources\n\nWeb resources:\n\nIntroduction to ordination – By our coding club.\nPrincipal component analysis – Lever et al. (2017).\n\nBe careful with your principal components – Björklund (2019).\n\nA gentle introduction to principal component analysis using tea-pots, dinosaurs, and pizza – Saccenti (2024).\n\nUniform manifold approximation and projection – Healy & McInnes (2024).\nSeeing data as t-SNE and UMAP do – Marx (2024).\n\nUnderstanding UMAP.\nHow to Use t-SNE Effectively by Wattenberg et al. (2016).\n\nVideos:\n\nStatQuest: Principal Component Analysis (PCA), Step-by-Step from StatQuest.\nHow to create a biplot using vegan and ggplot2 (From Riffomonas project).\nUsing the vegan R package to generate ecological distances (From Riffomonas project).\nRunning non-metric multidimensional scaling (NMDS) in R with vegan and ggplot2 (From Riffomonas project).\nPerforming principal coordinate analysis (PCoA) in R and visualizing with ggplot2 (From Riffomonas project).\n\n\n\n\n\n\nBjörklund, M. (2019). Be careful with your principal components. Evolution, 73(10), 2151–2158. https://doi.org/https://doi.org/10.1111/evo.13835\n\n\nHealy, J., & McInnes, L. (2024). Uniform manifold approximation and projection. Nature Reviews Methods Primers, 4(1), 82. https://doi.org/10.1038/s43586-024-00363-x\n\n\nLever, J., Krzywinski, M., & Altman, N. (2017). Principal component analysis. Nature Methods, 14(7), 641–642. https://doi.org/10.1038/nmeth.4346\n\n\nMarx, V. (2024). Seeing data as t-SNE and UMAP do. Nature Methods, 21(6), 930–933. https://doi.org/10.1038/s41592-024-02301-x\n\n\nSaccenti, E. (2024). A gentle introduction to principal component analysis using tea-pots, dinosaurs, and pizza. Teaching Statistics, 46(1), 38–52. https://doi.org/https://doi.org/10.1111/test.12363\n\n\nWattenberg, M., Viégas, F., & Johnson, I. (2016). How to use t-SNE effectively. Distill. https://doi.org/10.23915/distill.00002",
    "crumbs": [
      "8. Ordination",
      "• 8. Ordination summary"
    ]
  },
  {
    "objectID": "book_sections/data_viz.html",
    "href": "book_sections/data_viz.html",
    "title": "9. Better Figures",
    "section": "",
    "text": "Why Make a Plot?\nTo make these abstract ideas concrete, lets return to our parviflora RILs. A quick glance suggests that the probability of setting hybrid seed goes up as leaf water content goes down. While this observation is true, it is not the real story – the real story is that parviflora RILs with smaller petals make fewer hybrids. The initial observation is a red herring that arises because of the negative assocaition between leaf water content and petal area.\nIt’s nearly impossible to look at raw numbers in a dataset and come away with a holistic understanding. Communicating results by listing numbers is inefficient and overwhelming. While summaries of single variables, associations between variables can efficiently convey certain aspects of your data, they often hide important details. On their own, summary statistics can mislead, overlook critical patterns, and fail to provide readers with an intuitive way to evaluate your claims.\nA good plot is more than a condensed and efficient presentation of the data. Rather, making a plot is literally our opportunity to shape how the reader sees the data, and is therefore a critical medium by which we tell our scientific story. Because a plot is a crafted story with a purpose, we must think about this as we build our plots. Ask yourself:\nGraphs exist to communicate clear points. Together, a set of plots should form a cohesive narrative. When creating an explanatory plot, ask yourself:\nAs we work through this checklist, consider that at the extremes there are two types of plots.",
    "crumbs": [
      "9. Better Figures"
    ]
  },
  {
    "objectID": "book_sections/data_viz.html#why-make-a-plot",
    "href": "book_sections/data_viz.html#why-make-a-plot",
    "title": "9. Better Figures",
    "section": "",
    "text": "What’s the key story that the data are telling?\n\nWhat point am I trying to make?\nHow does this plot support that story?\n\nCan someone skeptical follow and verify it?\n\nHow can a plot be improved to clearly communicate this message?\n\nHow does this point fit into the larger story I want to tell?\n\n\n\nExploratory plots are plots we make to make sense of the story in the data.\nExplanatory plots tell this story to a broader audience.\n\n\nWhy Make Exploratory Plots?\n\nThe first principle is that you must not fool yourself – and you are the easiest person to fool.\n\n— Attributed to physicist Richard Feynman\n\n\nBefore telling a story, we must know the story we aim to tell. This includes understanding the overall message we aim to convey, recognizing the extent to which the data support this message, and not missing key elements of the data. As an example of the importance of looking at our data, lets look into datasaurus a more elaborate version of Anscombe’s quartet.\n\nlibrary(datasauRus); library(ggplot2); library(dplyr)\nsummary_stats &lt;- datasaurus_dozen                      |&gt; \n  group_by(dataset)                                    |&gt; \n  summarize(mean_x =  mean(x),   mean_y = mean(y), \n            stdev_x = sd(x),     stdev_y = sd(y), \n            cor_xy = cor(x, y)) \n\n\n\n\n\n\n\n\nThe plots below highlight the importance of looking at your data.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nHist of xHist of ySlopesScatterplot\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nAfter exploring these data, answer the following questions:\n\nWhich dataset has no y values between 45 and 55? \nWhich shape is made by one of the datasets in datasaurus? A turtleA starA heart\n\nAs see looking at histograms of x and y reveals some differences between the datasets, but examining a scatterplot is truly revealing!\n\n\nWhy make explanatory plots?\nAn explanatory plot effectively communicates results while giving skeptical readers the chance to evaluate our claims. Plots are such a critical tool in scientific communication that, in many lab meetings, papers are often discussed by focusing on the figures. Crafting a good explanatory plot is much like craft a good story. As you watch the video below, consider how the components of telling a good story can be mapped onto the idea of making a good plot.",
    "crumbs": [
      "9. Better Figures"
    ]
  },
  {
    "objectID": "book_sections/data_viz.html#watch-this-video-on-what-makes-a-good-story-and-consider-how-this-applies-to-storytelling-with-data-visualization.",
    "href": "book_sections/data_viz.html#watch-this-video-on-what-makes-a-good-story-and-consider-how-this-applies-to-storytelling-with-data-visualization.",
    "title": "9. Better Figures",
    "section": "Watch this video on what makes a good story, and consider how this applies to storytelling with data visualization.",
    "text": "Watch this video on what makes a good story, and consider how this applies to storytelling with data visualization.\n\nExample of Telling a Story With Plots:\nIn basketball, most shots are worth two points, while distant shots beyond the three-point line are worth three points. Around 2008, the NBA began embracing analytics, and analysts discovered that three-point shots provide more value than most two-point shots. As a result, teams shifted their strategy to prioritize three-pointers or high-percentage two-point shots close to the basket (podcast for the curious).\n1 A compares shot selection before and after the rise of analytics in the NBA. It demonstrates that before this shift, teams had no obvious trends in shot selection, while afterward, most teams focused on three-pointers and close-range shots. 1 B shows the dramatic rise in three-point attempts from 2006 to the present, providing historical context. Together, they tell the story of the NBA’s analytics revolution.\n\n\n\n\n\n\n\n\nFigure 1: Figure A is modified from images on Instagram @llewellyn_jean. Figure B is modified from an article on espn.com.\n\n\n\n\n\n\nis not perfect. For example, the team names are too small to read. But fixing this would be unnecessary—the team names don’t significantly contribute to the story we’re telling.\n\nAfter you create your plot, take a moment to reflect. How well does your figure make its intended point? How could it detract from your message? Then brainstorm ways to improve your plot to more clearly and honestly convey your point.\n\n\nThe Process\nComputational tools like ggplot2 are great for making good plots, but remember they are tools to help you, not constrain you. Many experts (and the internet) suggest that before jumping into ggplot, you should first:\n\n\n\nMy approach to figure-making in #ggplot ALWAYS begins with sketching out what I want the final product to look like. It feels a bit analog but helps me determine which #geom or #theme I need, what arrangement will look best, & what illustrations/images will spice it up. #rstats pic.twitter.com/GUjeEgqZxj— Shasta E. Webb, PhD (@webbshasta) May 22, 2020\n\n\n\nSketch your desired plot to conceptualize it.\nBe cautious of defaults and common plots, as they might not always serve your needs.\n\nCreating a good plot is an iterative process. You will likely go back and forth between pencil-and-paper sketches and ggplot until you reach a design you’re happy with.",
    "crumbs": [
      "9. Better Figures"
    ]
  },
  {
    "objectID": "book_sections/data_viz.html#lets-jump-into-key-concepts-in-data-viz",
    "href": "book_sections/data_viz.html#lets-jump-into-key-concepts-in-data-viz",
    "title": "9. Better Figures",
    "section": "Let’s jump into key concepts in data viz!",
    "text": "Let’s jump into key concepts in data viz!\nWhile bad plots can be bad in various ways, all good plots share certain characteristics. Specifically, all good plots are: Honest, Transparent, Clear, Accessible, and avoid distractions. This chapter provide specific practices you can use to make such good plots, and then introduces the idea of understanding your audience and mode of presentation, and how to write about plots. As always, I conclude with a chapter summary.\nBut before moving along, let’s take a break from Clarkia and focus on the distribution of student-to-teacher ratios across continents. We will use Figure 2 to exemplify how we can make good plots.\n\n\n\n\n\n\n\n\nFigure 2: It is hard to make a plot this bad.\n\n\n\n\n\nNote that improving a figure is an iterative process, so we slowly get better, and may take some wrong turns along the way. Figure 3 shows one path throght this process.\n\n\n\n\n\n\n\n\nFigure 3: Making a plot is an iterative process. gif taken from Cedric Scherer’s blogpost on The evolution of a ggplot.",
    "crumbs": [
      "9. Better Figures"
    ]
  },
  {
    "objectID": "book_sections/data_viz.html#looking-ahead",
    "href": "book_sections/data_viz.html#looking-ahead",
    "title": "9. Better Figures",
    "section": "Looking ahead",
    "text": "Looking ahead\nThere are two distinct challenges to making a good plot goal:\n\nUnderstanding the ideas behind making a good plot (this chapter).\n\nThe technical process of creating the plot in R (next chapter).\n\nThese are separate tasks. If you focus too early on the mechanics of R, you risk creating poorly designed visuals. Once you have a clear vision of the plot, implementing it in R becomes more straightforward (especially with the help of generative AI tools like ChatGPT, Claude, Gemini etc..)\n\n\n\n\nGould, P. (1981). Letting the data speak for themselves. Annals of the Association of American Geographers, 71(2), 166–176. https://doi.org/https://doi.org/10.1111/j.1467-8306.1981.tb01346.x",
    "crumbs": [
      "9. Better Figures"
    ]
  },
  {
    "objectID": "book_sections/data_viz/audience&delivery.html",
    "href": "book_sections/data_viz/audience&delivery.html",
    "title": "• 9. Audience & Format",
    "section": "",
    "text": "The Audience and the Format\nLabeling something as a “good plot” or a “bad plot” is overly simplistic. Sure, there are features that make a plot “better” or “worse”, but plots do not have an independent existence. Plots are presented to specific audiences in specific contexts. Considering your audience and the presentation format is key to making a truly effective visualization.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Audience & Format"
    ]
  },
  {
    "objectID": "book_sections/data_viz/audience&delivery.html#the-audience-and-the-format",
    "href": "book_sections/data_viz/audience&delivery.html#the-audience-and-the-format",
    "title": "• 9. Audience & Format",
    "section": "",
    "text": "The Audience\n\n\n\n\n\n\n\n\n\nFigure 1: An audience at a sceintific meeting (The 20204 EU Drones Conference to be precise). Photo posted to wikimedia commons and shared under Creative Commons Attribution 2.0 Generic License by the Belgian Presidency of the Council of the EU 2024.\n\n\n\n\nWe tell stories to an audience, not a wall. Just as you would communicate differently with a friend versus a colleague, your plot should be tailored to its intended audience. When designing a figure, ask yourself: (1) Who is this for? (2) What background knowledge or context do they bring (3) What do they need from this? (4) What do you want from them – Feedback? Funding? Action? Respect etc…?\n“Archetypes” of a potential audience for your plot include:\n\n🫵 You and Your Team: You are the first person to look at your plot. You want to know that you are not fooling yourself. You are deeply familiar with the data, methods and many peculiarities of the study.\n\nFor this audience: You may want to color points by technical covariates (e.g. when the reading was taken, who took it? which machine was used? etc) and use tools like plotly to understand outliers so that you don’t waste weeks digging into the wrong story.\n\n🧠 The Expert Critical Reader: This is the scientific audience for academic articles. Often a peer, a reviewer, or even a competitor. The expert wants precision, detail, honesty and clarity. The expert wants to quickly know what you did and what it means, but may be quite critical of your results so would like to be able to evaluate the evidence for themselves.\n\nFor this audience: You will likely want to guide their interpretation but while showing all the data and empowering them to verify your conclusions.\n\n🤷 The Curious Novice: Not everyone encountering your work will be an expert. You might communicate your work to students just beginning to master the subject, or biologists from a related subdiscipline. This audience too, would like to be critical, but often needs additional context, background, and structure to make the most of a plot.\n\nFor this audience: Your main job here is as a teacher. You should explain your plots very clearly, and their points should be obvious.\n\n📣 The Public or Non-Expert: You might present your work to a broad audience (e.g. TEDx style), to a lawyer, a jury, a politician etc. These people are likely interested in your work (they’re here aren’t they?), and do want to be able to evaluate its legitimacy, but may not have statistical or biological expertise.\n\nFor this audience: Your main job here is as a convincer. As the expert, you aim to help this audience see things your way. Make your plots clear and easily digestible. You should largely refer to published studies for legitimacy, but be prepared to handle questions from engaged participants.\n\n\n\n\nTailoring Presentations to Their Format\n\n\n\n\n\n\n\n\n\n\nFigure 2: A snippet from Van Halen’s 1982 tour rider detailing the ‘Munchies’ requirements, accessed from snopes.com.\n\n\n\n\n\n\nIn one notable incident, officials at what is now Colorado State University Pueblo refused to honor the request, leading the band to go on a rampage that involved throwing food all over a dining area as well as “unmentionable” acts in a nearby restroom. However, even more damage was caused to the basketball floor in the gymnasium due to the weight of the stage brought in.\n\n— Wikipedia\n\n\nScience is communicated in many formats. Here’s how to tailor your plots to different mediums:\n\n\n📘 Books / Papers: These plots are static and need to be self-contained. Readers should be able to understand the plot and its main point without relying on the accompanying text. You don’t know how people will read your paper.\n\n\nPlots as “Brown M&Ms”\nThe American hard rock band Van Halen had a strange issue in their “rider” - they insisted on having a bowl of M&Ms but no brown M&Ms Figure 2. While they did have a very rock bad diva response to this being broke (see the quote o the right), this request was more than a rock-diva thing. As explained by vocalist David Lee Roth this request was a way to ensure that the venue had closely read the requirements for the show to ensure safety (as aluded to at the end of the Wikipedia quote).\nIn the same way, scientists use visual cues to make rapid judgments about the quality and trustworthiness of research. A sloppy, unpolished plot acts like a brown M&M in the candy bowl; it’s a red flag that signals a potential lack of care that could extend to the data analysis itself (evenif the analysis is fine). A polished, professional plot does the opposite: it signals that the author is careful and trustworthy. It tells the reader they are in good hands.\n\n\n\n🎤 Public Talks: In presentations, you control the flow of information. A successful plot in a scientific talk:\n\nBuilds up figures slide by slide, to encourage audience understanding and engagement.\nUses large text, big points, and wide lines so that it is legible from the back of the room.\nFavors results and interpretation over details.\n\n\n\n\n\n\n\n\n🖼️ Posters:. Because your poster will be presented alongside tens or hundreds of others in a big room (Figure 3 A), your poster and its figures should be designed to draw viewers in and encourage them to engage. Posters are arguably the most difficult format in science communication, so here are some tips:\n\nMake it stand out: You’re often in a room with tens to hundreds of posters, and people are circulating casually, often with a coffee or drink in hand. Catch their eye. There is a time and place for charjunk and that time and place is a poster session.\n\nUse as little text as possible: Once you’ve removed unnecessary text, go back and remove even more.\nMake all graphical elements big: Text, points, labels—all of it.\n\n\n\n\n\n\n\n\n\n\nFigure 3: (A) A typical scientific poster session—crowded, fast-moving, and visually competitive. (B) A redesigned scatterplot of Iris data intended for a poster, using large points, direct species labels, and photographs of the flowers to make the plot visually engaging and immediately interpretable.\n\n\n\n\n\n\n\n💻 Digital Formats: Online formats allow for interactive elements, GIFs, and animations. These can help readers explore your data themselves, making the story more intuitive and engaging, but don’t overdo it. You should focus on helping readers engage with the plot to absorb the key results, not distract them from those results. As you can tell from my efforts in this book, this is my favorite context.\n\n\n\n\n\n\n\n\n\nFigure 4: Interactive iris plot with trendlines and direct labels. This interactive scatterplot shows Sepal Length versus Petal Length for three Iris species. Each species is plotted in a distinct color and shape, with accompanying trendlines and direct labels. Designed for digital formats, this figure allows for hover-based exploration, emphasizing user engagement and interpretability.\n\n\n\n\n\nSummary of the impact of medium on plot development.\n\n\n\n\n\n\n\n\nFormat\nKey Feature\nPrioritize\n\n\n\n\nPaper\nStatic, standalone\nClarity, context in figure\n\n\nTalk\nControlled pacing\nSimplicity, build-up, big elements\n\n\nPoster\nCompetitive and noisy setting\nVisual impact, minimal text\n\n\nDigital\nDynamic & user-driven\nInteractivity, clarity\n\n\n\n\n\n\n\nWe should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%\n\n— Donald Knuth\n\n\n\n\nPremature optimization is the root of all evil\nAlthough Donald Knuth was talking about optimizing computer programs, the same logic applies to figures. In research or publication contexts, refining the plot can be worth the time - but only after the scientific story is sound. Perfecting a plot before it is ready to be presented can be a massive time sink. I therefore recommend the following workflow to make great plots without wasting too much time:\n\nMake quick, rough plots to start exploring the stories in your data.\nMake additional plots and summaries to see if you fooled yourself and/or if an artifact can better explain your data than your hypothesis.\nMake a good “base plot” that clearly and honestly shows the point and circulate this to friends collaborators etc.\nSave the code for this plot and don’t lose it.\n🛑STOP🛑 working on the plot for a while. Do everything else to make your science good. Resist the urge to keep tweaking at this stage - your time is better spent elsewhere for now.\nOnce you are ready to present the work take steps towards further refining the plot following the concepts outlined in the rest of this chapter. Now is the time to make this figure shine!!! Do most of this in R - but feel free to spruce a quick thing up some other way (e.g. powerpoint / illustrator etc…) if it’s fast one-off.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Audience & Format"
    ]
  },
  {
    "objectID": "book_sections/data_viz/honest_plots.html",
    "href": "book_sections/data_viz/honest_plots.html",
    "title": "• 9. Honest plots",
    "section": "",
    "text": "Good Plots Are Honest\nPlots should clearly convey points without misleading or distorting the truth. A misleading exploratory plot can lead to confusion and wasted time, while a misleading explanatory plot can erode the reader’s trust. Honesty in plots builds credibility and helps ensure that both you and your audience stay on track.\nImportantly, honest people with good intentions can still create misleading plots - often due to default settings in R or other visualization tools. So, simply not intending to deceive isn’t enough. After we make a plot, we should take a step back and examine it - better yet, show it to naive peers to see what conclusions they draw from it. This practice can ensure that our plots are not unintentionally misleading.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Honest plots"
    ]
  },
  {
    "objectID": "book_sections/data_viz/honest_plots.html#good-plots-are-honest",
    "href": "book_sections/data_viz/honest_plots.html#good-plots-are-honest",
    "title": "• 9. Honest plots",
    "section": "",
    "text": "(Dis-)Honest Axes\n\n\n\n\n\n(Dis-)Honest Y-Axes\nDishonestY: truncating the axis. When people see a filled area in a plot, they naturally interpret it in terms of proportions. This can be a problem when the baseline of a filled bar is hidden or cropped, making modest differences look dramatic. Compare Figure 1, Figure 2 and Figure 3 in the three tabs below to see how a truncated y-axis can mislead a reader.\n\nFig. 1Fig. 2Fig. 3\n\n\n\n\n\n\n\n\n\n\nFigure 1: Misleading plot: Truncated y-axis with no visible axis labels. The differences in student–teacher ratios appear exaggerated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Still misleading: Adding axis labels to a truncated y-axis does not prevent our plot from misleading readers. The visual perception continues to overpower the text.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Honest plot: The y-axis starts at zero, providing an accurate visual impression of the true differences in student–teacher ratios.\n\n\n\n\n\n\n\n\n\n\nIf you don’t see any plots here, click on any of the tabs. Then browse through all of them by changing between tabs.\nFrom these plots we can see that:\n\nFigure 1 leads a reader to believe that student-to-teacher ratios in Africa are four times higher than in Asia, even though the actual difference is closer to two-fold.\nFigure 2 illustrates that adding y-labels does little to fix this, as our eyes are still driven to the magnitude of difference in bars, not the few letters that try to override this visual message.\nFigure 3 solves this by honestly displaying the full y-axis. \n\n\nNot all y-axes need to start at zero:\nTruncating the y-axis is most misleading for filled plots, but it’s not always necessary to start at zero.\n\nScatterplots: These don’t typically trick the eye the way bar plots do, so it’s less important to start the y-axis at zero. If you want to emphasize absolute differences, show the data as points and worry less about truncating the y-axis.\n\nNon-zero baselines: For variables like temperature, starting the y-axis at zero may be arbitrary.\n\n\n\nDishonestY: sneaky scales\nWe have previously discussed how common data transformations that can aid in modeling and visualizing our data. For example, log-transformation can be helpful for variables that grow exponentially. When such transformations are necessary be sure to clearly communicate the transformed scale. For example, if you use a log scale, but your readers don’t notice it, they will misinterpret the plot – a straight line on a log-log plot suggests a power-law relationship, while a straight line on a semi-log plot suggests exponential growth. A straight line on a log scale means exponential growth, not a steady increase.\nIf you’re displaying your data on a log scale, be loud about it: label the axes clearly. I particularly like the annotation_logticks() function to in the ggplot2 package to communicate the scale (as in Figure 4 B)\n\n\nCode for making a logscale two-panel figure and adding logticks.\nlibrary(patchwork)\n\na&lt;- ggplot(msleep, aes(bodywt, brainwt, label = name)) +\n    geom_point(na.rm = TRUE) +\n    scale_x_log10(\n        breaks = scales::trans_breaks(\"log10\", function(x) 10^x),\n        labels = scales::trans_format(\"log10\", scales::math_format(10^.x))\n    ) +\n    scale_y_log10(\n        breaks = scales::trans_breaks(\"log10\", function(x) 10^x),\n        labels = scales::trans_format(\"log10\", scales::math_format(10^.x))\n    )  \n\n\nb &lt;- a + \n  annotation_logticks() + \n  labs(title = \"Mammalian brain weight as a function of body weight\", \n       subtitle = \"Both axes are plotted on a log10 scale.\")\n\na &lt;- a + labs(title = \"Mammalian brain weight as a function of body weight\",\n              subtitle = \" \")\n\na+b\n\n\n\n\n\n\n\n\nFigure 4: Mammalian brain vs. body weight on a log scale. A the data on log-transformed axes with labeled axes. B adds log tick marks with annotation_logticks(), making the scale more visually explicit.\n\n\n\n\n\n\nDishonestY: broken axes\nSometimes extreme values on the y-axis make it hard to see meaningful variability for lower values. In some cases this situation is so extreme that the data cannot be plotted on a continuous without the bulk of the data being squished flat. One solution is a broken axis – a literal break to show the two different ranges of the data. I hesitate to recommend such an approach because it so often misleads the reader by distorting the relative distances. If you must use a “broken axis” be very explicit that you are doing so.\nA simple break as shown in Figure 5 A is insufficient. Rather something more extreme, like a large break marked by bright lines (e.g. Figure 5 B) is needed to ensure that the reader does not process the data without considering the axis break.\n\n\n\n\n\n\n\n\nFigure 5: Broken y-axes can be misleading if not clearly marked. Panel A includes a subtle y-axis break but risks misleading viewers. Panel B makes the break impossible to miss by using dashed lines and extra spacing. Always be loud and clear if you use a broken axis. Read more here.\n\n\n\n\n\n\nDishonestY: Unfair comparisons\nFewer babies are born in the U.S. on Sundays than any other day — with Saturdays close behind. When I first heard this I was amazed, but then I realized it makes sense – many births are scheduled C-sections, or induced in some other way, and doctors would prefer to take weekends off. But there is also a seasonality to births which cannot be explained by doctor’s schedules. Let’s look at this plot of the number of babies born each month of 2023 in Canada. While doing so, pay careful attention to the axes – as you will find that truncation isn’t the only way a y-axis can mislead\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nRather than me telling you what’s wrong here I want you to figure out what’s wrong and fix it. But you won’t be on your own, this chatbot tutor is here to help! Prompt available here if you want to make your own gem.\n\n\nHINT\n\nNot all months are have the same number of days.\n\n\n\nSOLUTION\n\n\nbabies |&gt;\n  mutate(days_per_month = (c(31,28,31,30,31,30,31,31,30,31,30,31)),\n  births_per_day  = births /days_per_month)|&gt;\n  ggplot(aes(x=month, y=births_per_day, group=1))+\n  geom_point(size = 3)+\n  geom_line(lty = 2)+\n  theme(axis.text.x = element_text(angle = 90, size=12))\n\n\n\n\n\n\n\n\n\n\n\nDishonestY: Y’s meaning depends on X: Sometimes values on the y-axis have different meanings for different values on the x-axis. For example, inflation tends to push prices up over time, and more people are born—or die—in larger populations. In these cases, showing raw values can mislead. It’s better to standardize the y-axis so it has a consistent meaning across x (like deaths per 1,000 people or inflation-adjusted cost), or to give viewers some point of reference so they can make fair comparisons.\n\n\n(Dis-)Honest X-Axes\nIt’s not just the y-axis that can mislead, there is plenty of opportunity for the x-axis to mislead as well. Common ways that the x-axis can mislead include:\n\nOrder that runs counter to expectations: Say we were plotting survival of Clarkia in four different temperatures – “Freezing”, “Cold”, “Warm”, and “Hot”. We would expect the x-axis to be in that order, but unless you tell it otherwise, R puts categorical variables in alphabetical order (i.e. “Cold”, “Freezing”, “Hot”, “Warm”). This will likely lead to patterns that surprise and confuse readers, as we explore in Figure 6.\nArbitrary spacing: Sometimes our categories suggest an order but not equal intervals — like “Control”, “Low”, “Medium”, and “Super High”. Plotting them on a linear x-axis makes the steps look evenly spaced, even if the treatment jump from “Medium” to “Super High” is much larger than from “Low” to “Medium”. This can trick readers into seeing a much sharper trend than is really there (e.g. Figure 7).\nInsufficient context: Seasonal ups and downs can be misused to make claims about long-term trends. For example, employment often drops in January as seasonal jobs disappear. But that doesn’t mean we should be reading headlines every year like “The year is off to a bad start.” That’s why it’s standard to present seasonally adjusted unemployment rates. Similar issues show up all over the biological world too - if you don’t consider seasonal or cyclical patterns, it’s easy to mislead or be misled.\n\nWith these ideas in mind, lets look at two plots (Figure 6 and Figure 7) showing trends in the temperature in Minnesota across the year:\n\nFig. 4Fig. 5\n\n\n\n\n\n\n\n\n\n\nFigure 6: (Dis-) Honest x-axis ordering: Average monthly high temperatures in Minnesota.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: (Dis-) Honest x-axis spacing: This plot uses only a subset of months (November, December, January, February, March, July) with a gap between March and July.\n\n\n\n\n\n\n\n\n\n\nIf you don’t see any plots here, click on any of the tabs. Then browse through all of them by changing between tabs.\nFrom these plots we can see that:\n\nBy having an order that runs counter to expectations, Figure 6 leads a reader to believe that temperature swings up and down many times dramatically across the year. This is because months are in alphabetical instead of sequential order.\n\nFix this by changing the order of categories (but this is harder than it sounds, we will work on that in the next chapter).\n\nBecause of the arbitrary spacing in Figure 7’s x-axis it looks like a sudden jump in July, but the jump is artificial - we’re just missing spring months.\n\nFix this by leaving a space on the x-axis where those categories should be (but make sure the missingness is not mistaken for zero).\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Airline passengers over time: This plot shows monthly passenger counts in the US from 1949 to 1960, revealing strong seasonal trends and long-term growth. The final months of 1960 (highlighted in red) appear as a decline only when removed from this full context.\n\n\n\n\n\nBecause it does not provide sufficient context Figure 8 misleads readers into thinking that the airline industry was crashing in late 1960.\n\nFix this by providing sufficient context – i.e. the year-over-year data in Figure 9 shows that Figure 8 is a predictable seasonal decline, not a not a sustained decline.\n\n\n\n\n\n\n\n\n\n\nFigure 9: Seasonal fluctuations in US air travel (1949-1960).\n\n\n\n\n\n\n\n\nHonest Bin Sizes\n\n\n\n\nThe video above explains how bin sizes can mislead. This issue comes up when using histograms to explore the shape of a distribution. The problem is that bin size is a smoothing decision, and smoothing decisions always involve trade-offs:\n\n\nSmoothing decisions aren’t unique to histograms - similar issues arise when choosing a kernel bandwidth in a density plot, set a bin width for a bar chart, or even a zoom in on a map.\n\nToo few bins oversmooths the data—you might miss real structure like bimodality or skew.\n\nToo many bins adds visual noise—random variation starts to look like meaningful bumps and wiggles.\n\nTo get a better sense for this, play with the bin number in the interactive salmon body size example above. Watch how the story changes as you try all values from 3 to 10, then try a few larger values. As you explore, ask yourself: Which bin size gives a clear picture without hiding or exaggerating the structure in the data? while recognizing there not always a single “right” answer.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| column: page-right\n#| standalone: true\n#| viewerHeight: 600\nlibrary(shiny)\nlibrary(munsell)\nlibrary(bslib)\nlibrary(readr)\n\n# Define UI for app that draws a histogram ----\n\nui &lt;- fluidPage(\n  titlePanel(\"Bin size can mislead!\"),\n  \n  fluidRow(\n    column(4),  # Empty column for spacing\n    column(4,\n           numericInput(\"bins\", \"Number of bins\", value = 3,\n                        min = 2, max = 200, step = 1,width = \"22%\")\n    ),\n    column(4)   # Empty column for spacing\n  ),\n  \n  fluidRow(\n    column(12,\n      plotOutput(\"plot\", width = \"100%\", height = \"430px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n      salmon &lt;- read.csv('https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/salmon_body_size.csv')\n  output$plot &lt;- renderPlot({\n    library(ggplot2)\n\n    # Create ggplot histogram\n    ggplot(salmon, aes(x = mass_kg)) +\n      geom_histogram(bins = as.numeric(input$bins)+2, \n                     fill = \"salmon\", \n                     color = \"black\", \n                     alpha = 0.7) +\n      labs(title = \"Distribution of Body Mass in Salmon\",\n           x = \"Body Mass (kg)\",\n           y = \"Count\") +\n      scale_x_continuous(limits = c(0.9,3.6))\n  }, res = 150)\n}\n# Create Shiny app ----\nshinyApp(ui = ui, server = server)\nAfter exploring this for a while try these short questions:\nQ1) Which bin number makes the reader think this distribution is unimodal and right-skewed? .\n\n\n\n\nQ1 Explanation\n\nWith 3 bins, the histogram oversmooths the data and there is no dip between peaks, but it’s not clear if the data are symmetric or skewed.\nThe histogram still oversmooths the data with 4 bins. But in this case the data appears more obviously right-skewed.\n\nQ2) Which is the smallest bin number that allows the reader to see that the data are clearly bimodal? .\n\n\n\n\nQ2 Explanation\n\nI accepted either six or seven. At six it seems like something is likely going on (counts seem to increase as x increases), and once you get to seven two obvious modes emerge.\n\nQ3) Which statement best describes the tradeoff in choosing the number of bins in a histogram? More bins always give more accurate results.Fewer bins always prevent overfitting.Fewer bins emphasize general patterns; more bins reveal detail.The number of bins doesn’t matter if the color is consistent.\n\nWhat about density plots?\nConcerns about bin size apply to smoothing in density plots. In ggplot, you can control the smoothing of a density plot using the adjust argument in the geom_density() function.\n\n\nAnother plotting option.\nIf no bin size seems to work well, you can display the cumulative frequency distribution using stat_ecdf(). This method avoids the binning issue altogether. The y-axis shows the proportion of data with values less than x, and bimodality is revealed by the two steep slopes in the plot. However, these plots can be harder for inexperienced readers to interpret.\n\n\n\n(Dis-)Honest color choice\nEven color can mislead. If colors imply an order (e.g., light to dark) but the categories don’t follow a logical sequence, viewers may misinterpret the pattern. Figure 10 shows temperature across America, but “warm” has a darker red than “Very hot”. This could mislead readers into thinking North Carolina is hotter than Texas. This is made even worse because “Warm” comes after “Very hot” in the legend. Fix this by making sure the order of colors (and color keys) make some sense and follow the reader’s expectations.\n\n\n\n\n\n\n\n\nFigure 10: U.S. states grouped by temperature category—Freezing, Cold, Mild, Warm, and Very Hot. Unfortunately, the colors do not follow a logical temperature progression - “Warm” is shown in a dark maroon and “Very Hot” is bright red, while “Freezing” and “Cold” are flipped on the blue scale. This non-intuitive color order (darker doesn’t always mean more extreme) makes the map unnecessarily confusing. What makes this even worse is that the color key also goes in this confusing order.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Honest plots"
    ]
  },
  {
    "objectID": "book_sections/data_viz/honest_plots.html#section-summary",
    "href": "book_sections/data_viz/honest_plots.html#section-summary",
    "title": "• 9. Honest plots",
    "section": "Section summary",
    "text": "Section summary\nEven honest people can make dishonest plots. Truncated y-axes, weird or uneven x-axes, misleading bin sizes, and unstandardized values can all distort what your audience sees—especially when viewed quickly or from a distance. A good plot doesn’t just show the data; it helps readers reach the right conclusion without extra mental gymnastics. After you make a plot, show it to someone—fast, far away, or with minimal labels—and ask what they think it says. If they walk away with a different takeaway than you intended, your plot needs work.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Honest plots"
    ]
  },
  {
    "objectID": "book_sections/data_viz/transparent_plots.html",
    "href": "book_sections/data_viz/transparent_plots.html",
    "title": "• 9. Transparent plots",
    "section": "",
    "text": "Good plots are transparent\nHere’s a motivating scenario and learning goals section that fits your tone and structure:\nTransparency is a great way to communicate honestly and encourage active engagement. Transparently presenting our data empowers our readers to evaluate our claims, critique them, test them for themselves, and even uncover new insights in our data. As such, showing the data is a critical step toward building trust with a skeptical audience and invites them to engage with the data themselves.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Transparent plots"
    ]
  },
  {
    "objectID": "book_sections/data_viz/transparent_plots.html#good-plots-are-transparent",
    "href": "book_sections/data_viz/transparent_plots.html#good-plots-are-transparent",
    "title": "• 9. Transparent plots",
    "section": "",
    "text": "Showing Your Data\nAs we saw in the datasauRus example (revisited in Figure 1), relying on summary statistics can obscure important patterns. Similarly, plots that only show summaries (e.g., barplots of means) fail to provide the full picture. Whenever possible, show all of your data.\n\n\n\n\n\n\n\n\nFigure 2: Always show your data! (A): A cartoon by allison_horst illustrating how summary statistics (e.g. means +/- error bars) can obscure interesting structure in the data. The raw data on the right reveal a bimodal distribution hidden by the simple summary. This figure is reformatted this for space here is the original. (B) The Meme-style reaction images of Nicholas Cage and Pedro Pascal from “The Unbearable Weight of Massive Talent” shows that a raw data accompanied by data summaries is generally preferred to a simple summary because the prior shows the data.\n\n\n\n\n\n\n\nBarplots aren’t inherently bad. While barplots should not be used to report means, they are effective for presenting proportions or count data.\n\nWhen I say: “Show your data”,\nI mean: “Show your F***NG data! All of it. Not a mean, not a trendline. SHOW ALL OF YOUR DATA!”\n\n\nIs it ever OK to not show all the data?\nDespite my emphatic cursing above, there are rare cases where showing all your data is actually less honest than showing a summary. This usually happens when overplotting becomes a problem - when there’s so much data, or so many points stacked at a single value, that showing every data point hides the overall pattern instead of revealing it. Below, we’ll walk through examples where showing the full data might be misleading and what to do instead.\n\n\n\nTransparency Avoids Overplotting\nSometimes, showing all your data can actually obscure patterns—a problem known as overplotting. Overplotting occurs when data points in a plot overlap or cluster so densely that it’s difficult or impossible to discern individual values or patterns in the data. This typically happens when you have a large number of data points, or when the range of data values is narrow, causing points to pile on top of each other. Overplotting can obscure the underlying distribution, relationships, or trends, making it hard to interpret the data accurately.\nFigure 3 shows several techniques (like jittering, using transparency, etc.), and alternative plots (e.g., density plots, box plots, or sina plots) that we can use to reveal patterns that would otherwise be hidden.\n\n\n\n\n\n\n\n\nFigure 3: Sometimes showing all the data hides patterns. (a) shows overplotting, where data points overlap and obscure the distribution of values. (b–i) demonstrate solutions for overplotting. The sina plot (f) is one of my favorites because it shows both the shape of the data and individual data points. After installing and loading the ggforce package, you can use geom_sina() to create a sina plot. Data from Beall (2006). Download the data here.\n\n\n\n\n\n\n\nTransparency Links Data, Code, and Results\nThe most transparent data are fully reproducible. Readers should be able to download your code and data, replicate your analysis, and understand the dataset well enough to perform their own analysis. As discussed in our previous sections on reproducible science, this level of transparency is becoming the standard in scientific research.\n\n\n\n\nBeall, C. M. (2006). Andean, Tibetan, and Ethiopian patterns of adaptation to high-altitude hypoxia. Integrative and Comparative Biology, 46(1), 18–24. https://doi.org/10.1093/icb/icj004",
    "crumbs": [
      "9. Better Figures",
      "• 9. Transparent plots"
    ]
  },
  {
    "objectID": "book_sections/data_viz/clear_plots.html",
    "href": "book_sections/data_viz/clear_plots.html",
    "title": "• 9. Clear plots",
    "section": "",
    "text": "Good Figures Are Clear\nGood plots are clear, with messages that stand out. To achieve clarity in a plot, we need to make labels informative and readable, minimize cognitive burden, make the point obvious, and avoid distractions.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Clear plots"
    ]
  },
  {
    "objectID": "book_sections/data_viz/clear_plots.html#good-figures-are-clear",
    "href": "book_sections/data_viz/clear_plots.html#good-figures-are-clear",
    "title": "• 9. Clear plots",
    "section": "",
    "text": "Have Informative and Readable Labels This should go without saying, but make sure that your labels can be read and that readers know what these labels mean. Sometimes labels are unclear because authors simply don’t look at the figures, other times they use shorthand that might be clear to experts or the team working on the project, but not to outsiders or non-experts.\nMinimize cognitive burden Two of my favorite books are Crime and Punishment and 100 Years of Solitude. While they’re great stories, I remember struggling when trying to track relationships between characters or remember that Raskolnikov and Rodya are the same person. Scientific communication is not \\(19^{th}\\) century Russian literature – as science communicators we strive to be consistent and to minimize how much your reader has to keep in their mind.\nMake points obvious A scientific figure should tell a story, but it shouldn’t be a mystery or a puzzle. Science is complicated enough, and people who read science are often busy - so the message of a plot should be clear. Readers should use their brains making sense of the implications of scientific findings, not figuring out what the finding is.\nAvoid distractions Readers should focus on your story, not on unnecessary visuals or effects.\n\n\nClear Plots Highlight Patterns\nGood plots do more than “show the data.” They help your reader see what matters, fast. That means intentionally designing plots to make important patterns and comparisons obvious - especially the ones that are central to your scientific story.\n\nBring Out Key Comparisons\nBecause data cannot speak, we can’t just “present the data.” Good plots are designed to help readers see and evaluate the patterns central to the story you’re telling. Just like in storytelling, emphasizing the wrong details can distract and mislead. This is important becase the same dataset can tell very different stories depending on how it is plotted. This often means guiding your reader’s eye with color, layout, or label placement.\nFigure 1 shows one such example:\n\nBecause most people at a given time do not have COVID-19 including the “non-covid” cases makes it hard to see any difference between the placebo and the mRNA vaccine (Figure 1 A-B).\nBy focusing on the proportion of severe vs. mild COVID cases in the placebo and vaccine group, Figure 1 C shows that vaccinated folks have proportionally fewer cases of extreme COVID among the vaccinated. But it does not provide information about the total number of infections.\nFigure 1 D-F show both the reduced number of cases and the lower proportion of severe COVID among vaccinated participants.\n\n\n\n\n\n\n\n\n\nFigure 1: Same data, different message: Various plots of the Moderna vaccine trial data tell different stories. (a) and (b) imply the vaccine isn’t effective by highlighting that most participants didn’t develop COVID. (c) compares the severity of cases by treatment but hides the vaccine’s effect on infection risk. (d) and (e) emphasize the severity of cases, while (f) highlights vaccine efficacy but makes it harder to compare severity. (f) is my favorite despite this.\n\n\n\n\n\n\n\nConsider How People Process Images\nWhen creating a plot, consider not just the data but how readers will interpret it. You don’t need to be a graphic designer to make a good plot. Try asking a friend what they see in your plot—can they easily spot the pattern or comparison you want to highlight?\nLet’s try this by looking at Figure 2 (you be the friend), which like Figure 1 shows different presentations of the same data. As you examine these plots consider which one allows you to best estimate the difference in X and Y.\n\n\n\n\n\n\n\n\n\nFigure 2: Facilitate comparisons – Which plot makes it easiest to compare X and Y? Image from slide 28 of this presentation by Karl Broman.\n\n\n\n\n\n\n\n\n\nClear Plots Use Informative and Readable Labels\nBeyond comparisons, clarity often breaks down when readers can’t read axis labels. Often labels for categorical variables are quite long, potentially running over each other and making them difficult to read. Poor plots like Figure 3 A, pollute the literature, and arise when authors don’t check how their plot labels render. Figure 3 b-d show alternatives:\n\nb Abbreviates to prevent labels from jumbling into each other. This definitely helps, but now the reader must think for a bit to connect the label to the actual variable.\n\nc Rotates the labels 90 degrees. This too is helpfull, but we rarely read at this angle.\n\nd Flips the axes. The last option is my favorite because it is the most natural to read.\n\n\n\n\n\n\n\n\n\nFigure 3: Different ways to handle long category labels. Plot A shows overlapping text that is difficult to read. Plot B uses abbreviations, C rotates the labels 90 degrees, and D flips the plot to use the y-axis for labels. Plot D is the clearest and most readable, making comparisons easy.\n\n\n\n\n\n\n\nClear Plots Are Consistent\nVisual consistency across figures can support storytelling, or undermine it when neglected. In a project with multiple plots, visual consistency helps readers follow your story. Unfortunately, R doesn’t remember color assignments unless you explicitly set them. This becomes a problem when you subset data - for example, plotting just two groups from an earlier plot that included six.\nIn Figure 4 a, South America and Asia appear in new default colors (blue and red), breaking the visual connection to earlier plots (e.g. Figure 3) where they were purple and brown. This disrupts the reader’s expectations. You can fix this by explicitly assigning colors, as shown in b, or go further and order the legend to match the plot layout, as in c.\n\n\n\n\n\n\n\n\nFigure 4: Maintaining consistency in color and legend order helps readers make comparisons across figures. (a) shows a mismatch in colors due to default reassignments. (b) fixes the color mapping to match earlier plots. (c) improves further by reordering the legend to match the visual flow.\n\n\n\n\n\n\nMaintaining consistency in color mapping, as shown in Figure @ref(fig-consistent)c, and sorting labels sensibly makes plots even easier to process.\n\n\n\nClear Plots Use Direct Labeling (When Helpful)\n\n\n\n\n\n\n\n\n\nFigure 5: Density plot showing student_ratio values for two regions (South America and Asia), with direct labels placed on the respective curves instead of using a separate legend. This use of direct labeling reduces the cognitive burden.\n\n\n\n\nFigure 5 improves on Figure Figure 4 C by using direct labeling instead of a legend. Readers no longer need to scan over to the legend, decode which color is which, and then return to the data to interpret what they’re seeing. Instead, the category labels are placed right on the plot, where the data are. This reduces cognitive load, and brings the readers’ attention where it belongs – the data.\n\n\nClear Plots Order Categories Sensibly\nHow you order categorical variables on an axis can dramatically affect a plot’s readability. Be deliberate about how you order categories on the x-axis (when categorical). Here are three guiding principles to help you:\n\nIf the data are ordinal (e.g., months of the year), place them in their natural order.\nFor nominal categories order by a meaningful statistic (e.g. decreasing mean or median) to make trends or extremes stand out.\n\nFor many low-frequency categories, consider grouping them into “Other” and placing it last.\n\nFigure 6 compares the default alphabetical order of continents (panel A) with the same plot reordered by the mean student–teacher ratio (panel B). The second version reveals the pattern more clearly and makes it easier to identify the lowest- and highest-ratio regions.\n\n\n\n\n\n\n\n\nFigure 6: Ordering categories by value makes patterns more interpretable. Panel A uses alphabetical order, while Panel B sorts by the mean student–teacher ratio, making differences easier to spot.\n\n\n\n\n\n\nBy default, R arranges categories alphabetically. This is rarely what you want. The forcats package gives you tools to arrange categorical variables more sensibly:\n\nfct_relevel() allows you to manually reorder factor levels in a specific order. This is great for ordinal variables like “high”, “medium”, “low”.\nfct_reorder() allows you to reorder factor levels based on a summary of another variable for when you want nominal categorical variables ordered by means, medians, etc. (as in Figure 6 B).\nfct_lump_*() allows you to group rare categories in “other”.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Clear plots"
    ]
  },
  {
    "objectID": "book_sections/data_viz/avoid_distractions.html",
    "href": "book_sections/data_viz/avoid_distractions.html",
    "title": "• 9. Avoid Distractions",
    "section": "",
    "text": "Good Figures Avoid Distractions\nFigure 1: Just because you can make a fancy plot doesn’t mean you should. A reminder from Jeff Goldblum as Dr. Ian Malcolm in Jurassic Park.\nBuckminster Fuller aimed to be “invisible,” letting his ideas, not his appearance, speak. This principle, which underlies the name of my favorite podcast on design, 99% Invisible. The same principle applies to figures A good figure calls attention to patterns in the data, not to itself.\nJust because you found an R package to make a Sankey diagram or a 3D bar chart doesn’t mean you should use it. Good data viz starts with a question and ends with a design that answers it, not the other way around. Before (or more realistically, halfway through) making an overly complex plot think, WWIMS (“What Would Ian Malcolm Say?” Figure 1), and ask yourself:\nBelow I illustrate this principle for common issues in data visualization.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Avoid Distractions"
    ]
  },
  {
    "objectID": "book_sections/data_viz/avoid_distractions.html#good-figures-avoid-distractions",
    "href": "book_sections/data_viz/avoid_distractions.html#good-figures-avoid-distractions",
    "title": "• 9. Avoid Distractions",
    "section": "",
    "text": "What is the viewer supposed to learn?\nIs this visual emphasizing the right relationships?\n\nIs there a simpler or more direct alternative?\n\n\n\nWhat the duck?\n\n\n\n\n\n\n\n\n\nFigure 2: The Big Duck was built in 1931 by duck farmer Martin Maurer and used as a shop to sell ducks, dairy, and duck eggs. This building inspired the architectural term later borrowed by Edward Tufte to critique overly stylized, decorative elements in data visualizations. In this metaphor, a “duck” prioritizes form over function. Image posted to Wikimedia commons by Mike Peel and shared under a CC-BY-SA-4.0 license.\n\n\n\n\n\nWhen a graphic is taken over by decorative forms or computer debris, when the data measures and structures become Design Elements, when the overall design purveys Graphical Style rather than quantitative information, then the graphic may be called a duck in honor of the duck-form store, “Big Duck.” For this building the whole structure is itself decoration, just as in the duck data graphic.\n\n— Edward Tufte\n\n\nTufte (1983) coined the term ‘duck’ to describe figures that showcase cleverness rather than data. An extreme example is the banana genome paper, where a banana drawing obscures the Venn diagram’s meaning (Figure 3). The image attempts to show gene families shared across three plant genomes, but fails because superimposing this over a cartoon banana is too distracting.\n\nResist the temptation to create flashy but ineffective visuals.\n\nRemember: visuals should prioritize clarity over aesthetics.\n\n\n\n\n\n\n\n\n\nFigure 3: Perhaps the ultimate data viz duck in comparative genomics. This plot is bananas. Figure 4 of the banana genome paper (D’Hont et al., 2012).\n\n\n\n\n\n\n\nDon’t use 3D or animation unnecessarily\n\n\n\n\n\n\n\n\n\nFigure 4: This rotating 3D pie chart demonstrates that an overcomplicated chart that looks flashy can be a problem.\n\n\n\n\n3D and animation are only helpful for specific purposes, like showing protein structures or time-lapse data. Unless the data our the audience calls for it, resist the urge to use them otherwise. More often than not, these flashy formats distract from your message and confuse your reader. They make your plot harder to read, harder to interpret, and much easier to ignore(as demonstrated in Figure 4).\n\n\nAvoid “glass slippers”\nA “glass slipper” (Figure 5) is when a visualization designed for one purpose is misapplied elsewhere, leading to confusion. Keep your visual tools fit for purpose. See this fun video from Calling Bullshit if you like.\n\n\n\n\nLately I've been getting all my best bullshit from promoted tweets. Here from @NexthinkNews, a classic \"glass slipper\" visualization (https://t.co/09curqq0tU), in which data is shoehorned into a highly specialized and entirely inappropriate format. pic.twitter.com/aYGxBRkHPG— Calling Bullshit (@callin_bull) March 14, 2019\n\n\n\nFigure 5: A tweet from the Calling Bullshit highlights a ‘glass slipper’. This ‘Periodic Table of IT Ops Tools’ mimics the structure of the chemical periodic table but does not reflect any real periodicity or organizational logic in the data it presents.\n\n\n\n\n\nThe examples above are cases of what Tufte called chartjunk – visual elaborations that are not needed to understand the information in the plot. Such additions can distract the viewer, increase interpretation time, or even mislead.\n\n\n\nIn defense of (occasional) chartjunk\nWe will soon consider the importance of considering audience when making plots. Such consideration can reveal that there are circumstances in which rules of data visualization are to be broken. Although much derided, chartjunk has numerous benefits:\n\nChartjunk can increase long-term memorability of the chart.\nChartjunk, in the form of semantically meaningful icons, can increase accessibility of charts for people with Intellectual and Developmental Disabilities.\n\nSo if you’re working on a serious plot for a scientific publication, avoid chartjunk. But if you’re aiming to get someone to come to your poster or remember your talk in a day full of seminars, you may find that limited and tasteful “chartjunk” is useful.\n\nWho is Tufte? I usually try to avoid centering famous individuals in science or statistics, because these fields are — in reality — massive collaborative efforts. But it’s worth knowing about Edward Tufte, not because he’s a singular genius, but because many widely cited “rules” of data visualization trace back to his book The Visual Display of Quantitative Information (Tufte (1983)).\nThis work shaped how people think about clutter, ink, and visual integrity and is the Bible of data visualization. But Tufte is a smart person with strong opinions, not a god. So we are allowed to disagree with him. I, for example, think there are times when a little chartjunk can actually help (see above).\n\n\n\n\n\nD’Hont, A., Denoeud, F., Aury, J.-M., Baurens, F.-C., Carreel, F., Garsmeur, O., Noel, B., Bocs, S., Droc, G., Rouard, M., Da Silva, C., Jabbari, K., Cardi, C., Poulain, J., Souquet, M., Labadie, K., Jourda, C., Lengellé, J., Rodier-Goud, M., … Wincker, P. (2012). The banana (musa acuminata) genome and the evolution of monocotyledonous plants. Nature, 488(7410), 213–217. https://doi.org/10.1038/nature11241\n\n\nTufte, E. R. (1983). The visual display of quantitative information (p. 197). pub-gp.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Avoid Distractions"
    ]
  },
  {
    "objectID": "book_sections/data_viz/accessible_plots.html",
    "href": "book_sections/data_viz/accessible_plots.html",
    "title": "• 9. Accessible Plots",
    "section": "",
    "text": "Good Figures Are Accessible\nFigure 1: See Section 508 of Amendment to the Rehabilitation Act.\nMaking figures accessible for all tends to make them better for everyone. Consider the diversity of people who may view your figure—this could include readers with color blindness, low vision, those who rely on screen readers, or even those who print your figure in black and white. A good figure should be interpretable by all of these individuals.\nWe have already highlighted several good practices. For example, describing the results of a figure in words can make it accessible to blind or visually impaired readers, while direct labeling can make the content clearer to readers with color vision deficiencies. These examples illustrate the benefits of universal design - they make figures better for all audiences, regardless of specific needs. ACCESIBILITY HELPS EVERYONE!",
    "crumbs": [
      "9. Better Figures",
      "• 9. Accessible Plots"
    ]
  },
  {
    "objectID": "book_sections/data_viz/accessible_plots.html#good-figures-are-accessible",
    "href": "book_sections/data_viz/accessible_plots.html#good-figures-are-accessible",
    "title": "• 9. Accessible Plots",
    "section": "",
    "text": "Color\n\n\n\n\n\n\n\n\n\nFigure 2: Ishihara colorblindness test plate: people with red-green colorblindness may not see the number 74.\n\n\n\n\nChoosing effective colors is a challenge. Ensure that your color choices are easy to distinguish, particularly if printed in grayscale or viewed by colorblind individuals. Many R tools can help with this, including the colorspace package. Toensure that your plots are accessible, I recommend:\n\nTesting your figures through a color vision deficiency emulator (like this one) to see how your plots appear to readers with color vision deficiencies.\n\nPrinting your plots in black and white, to see how it looks in that format.\n\nAnd, using redundant coding and direct labeling to increase accessibility.\n\n\nTry this color vision deficiency emulator at http://hclwizard.org/cvdemulator/ to see how other people might see your plot.\n\n\nRedundant coding - such as mapping shape, line type, or pattern in addition to color for the same variable provides readers with multiple ways to differentiate categories. This helps because - although color is a great way to differentiate variables, many readers have color vision deficiencies or may print your work in black and white.\n\n\n\nSize\nEnsure that all elements in your figure, including text, axis labels, and legends, are large enough to be easily read by people with poor eyesight. Always err on the side of larger text. Small text not only diminishes accessibility but can also make figures look cluttered and unclear.\n\n\n\n\n\n\n\n\nFigure 3: Bigger text is easier to read. Image from Advanced Data Science\n\n\n\n\n\n\nTest the size of elements in your figures by viewing them at reduced sizes or printing them. If the labels and details are still readable, they’re likely large enough.\n\n\n\nAlt Text for Figures\nWhen creating figures for digital use (e.g., websites, PDFs, or presentations), it’s important to include descriptive alt text for individuals who rely on screen readers. Alt text provides a textual description of the figure, ensuring that people who cannot see the image can still understand its content.\nGood alt text should describe the key information the figure conveys without unnecessary detail. It’s not enough to simply say “Figure showing data”; you need to explain what the reader should take away from the visual representation.\n\n\n\nAccessibility Checklist\nUse this to quickly review your figures before sharing them with the world:\n\nReadable text: Can all text (titles, axis labels, legends) be read at a glance—even when printed or viewed small?\nAlt text (if digital): Does your figure include a short but clear description for readers using screen readers?\nColor works for everyone:\n\nHave you tested your plot in grayscale or using a color vision deficiency simulator?\nIf color alone might not be distinguishable, have you used direct labeling (e.g., labels placed on the data) and/or redundant coding (e.g., shape, line type, pattern)?\n\nDirect labeling: When appropriate, are group labels placed directly on the plot (rather than in a separate legend)?\nRedundant coding: If color is doing work, are you also using shape, line type, or other cues to reinforce group differences?",
    "crumbs": [
      "9. Better Figures",
      "• 9. Accessible Plots"
    ]
  },
  {
    "objectID": "book_sections/data_viz/writing_about_figures.html",
    "href": "book_sections/data_viz/writing_about_figures.html",
    "title": "• 9. Writing about plots",
    "section": "",
    "text": "Writing About Figures\nGood figures should be clear enough to allow readers to interpret them on their own. Ideally, a reader should be able to examine your figure and draw a reasonable conclusion. But figures do not exist on their own, they exist in the in some broader context – a paper, a poster, a book, a presentation, etc… So, although good figures should stand alone, we can enhance the reader’s understanding by guiding their interpretation and emphasizing key takeaways.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Writing about plots"
    ]
  },
  {
    "objectID": "book_sections/data_viz/writing_about_figures.html#writing-about-figures",
    "href": "book_sections/data_viz/writing_about_figures.html#writing-about-figures",
    "title": "• 9. Writing about plots",
    "section": "",
    "text": "People read papers differently.\nSome readers barely glance at figures. Others skim the figures first, then dip into the text. Some go back and forth. Because we can’t control how people engage with our work, we need to make sure that both our figures and our writing about them are clear, complementary, and accessible to all readers.\n\n\nWriting Figure Captions\nAlthough well-designed figures should be interpretable without a figure captions, a good caption adds value by surfacing key takeaways and helpful context. A good caption does not merely restate the figure but rather presents additional context, support and background for the point clearly conveyed visually.\nFigure 1 shows an example of a bad (a) and good (b) figure and legend combination.\n\nFigure 1 A fails because it treats the legend as a crutch. The reader has to consult it just to figure out what the axes, groups, and colors mean - basic information that a figure should make obvious. This shortcoming forces readers to hold too much in memory, shifting their focus from interpreting the results to decoding the design.\nFigure 1 B succeeds because it adds helpful detail to a figure that is interpretable without a caption. Once you’ve taken in the visual message, the caption deepens your understanding by adding nuance and context.\n\n\n\n\n\n\n\n\n\nFigure 1: An example of bad (A), and good (B) figure / caption combinations. Readers should be able to understand a well-designed plot without reading the caption, this is not possible in A, but pretty obvious in B. A good caption adds context—not basic decoding.\n\n\n\n\n\n\n\nWriting “Alt text”\nAlt text should help readers who can’t see your figure still grasp its message. Think of alt text like narrating your figure to someone over the phone: what would you say so they could understand it? Below is an example of bad and good alt text for Figure 1 B:\n\n\n\n\n\n\n\n😞Bad alt text for Figure 1 B😞\n☺️Good alt text for Figure 1 B☺️\n\n\n\n\n“Stacked bar plot showing the the COVID vaccine works.”\n“Stacked bar plot showing COVID cases by severity for placebo and mRNA-1273 groups. The placebo group has many more cases, including severe ones (dark blue), while the vaccinated group has only a few mild cases (light blue).”\n\n\n\n\n\nWriting About Figures in Text\nWriting up results is one of the most important parts of doing science - if your work is not clearly communicated, it’s unlikely to make a lasting impact. When describing results, be explicit about what in a figure supports your conclusion. Rather than writing “Figure X shows that Group A grows faster,” aim for something more specific, like: “The steeper increase of Y in Group A than in Group B (Figure X) suggests that…” This style helps the reader connect your interpretation directly to visual evidence.\nWhen reading (or writing) text that discusses a figure, first look at the figure and think about its message. Then, consider:\n\nWhat features of the figure support the claims in the paper?\n\nAre there parts of the figure that challenge or complicate the interpretation in the paper?\n\nBelow is a comparison of weak vs. effective writing about the same figure. The second version goes beyond describing the visual and emphasizes the figure’s statistical and biological meaning.\n\n\n\n\n\n\n\n😞 A bad write-up Figure 1 B 😞\n☺️ A better write-up for Figure 1 B ☺️\n\n\n\n\n“Figure 1 B compares COVID cases and severity of these cases for treatments and controls.”\n“Figure 1 B shows a significant difference in COVID case incidence between the placebo group and those vaccinated with Moderna’s mRNA-1273 vaccine. Of the 15,000 individuals in the placebo group, 185 contracted COVID, while only 11 of the 15,000 vaccinated individuals did. Additionally, none of the vaccinated participants who became infected developed severe COVID, whereas 30 of the 185 infected placebo recipients had severe cases (compare the dark blue bar above the control group and its absence above the mRNA-1273 group).”",
    "crumbs": [
      "9. Better Figures",
      "• 9. Writing about plots"
    ]
  },
  {
    "objectID": "book_sections/data_viz/dataviz_summary.html",
    "href": "book_sections/data_viz/dataviz_summary.html",
    "title": "• 9. Dataviz Summary",
    "section": "",
    "text": "Chapter Summary\nLinks to: Summary. Chatbot tutor. Questions. Glossary. R functions. R packages. More resources.\n“Tall Infographics” cartoon from xkcd. Rollover text says: “Big data” does not just mean increasing font size. Explanation here.\nAn effective visualization allows you to rapidly communicate your key findings to your audience. The best visualizations are honest, transparent, clear, and accessible. They highlight important patterns while minimizing confusion or distraction, and they are thoughtfully tailored to both the audience and the format. Great figures avoid misleading elements and use design choices (e.g. captions, color, and labels) to guide interpretation.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Dataviz Summary"
    ]
  },
  {
    "objectID": "book_sections/data_viz/dataviz_summary.html#datviz_summary_chapter-summary",
    "href": "book_sections/data_viz/dataviz_summary.html#datviz_summary_chapter-summary",
    "title": "• 9. Dataviz Summary",
    "section": "",
    "text": "Chatbot tutor\n\nPlease interact with this custom chatbot (link here) I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you got what you needed from it.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Dataviz Summary"
    ]
  },
  {
    "objectID": "book_sections/data_viz/dataviz_summary.html#datviz_summary_practice-questions",
    "href": "book_sections/data_viz/dataviz_summary.html#datviz_summary_practice-questions",
    "title": "• 9. Dataviz Summary",
    "section": "Practice Questions",
    "text": "Practice Questions\nTry these questions!\n\n\n\n\n\n\n\n\nFigure 1: Hemoglobin levels of people native to different countries.\n\n\n\n\n\n\nQ1) Which of the plots in Figure 1 keep all information even if printed in black and white? a & ba & ca & db & cb & dc & d\nQ2) Which of the plots in Figure 1 is still somewhat useful but loses some information? abcd\n\n\n\n\n\n\n\n\n\nFigure 2: Canabalistic dads.\n\n\n\n\n\n\nQ3) Which plot in Figure 2 is better? ABIt depends…Don’t judge plots\nQ4) Your chose your answer, above, because the better plot Shows all the dataMakes patterns easy to seePresents data honestlyDraws graphics clearlyIs accesssibleI told you we dont judge plots, jeez\n\n\n\n\n\n\n\n\n\nFigure 3: Canabalistic dads, revisited.\n\n\n\n\n\n\nQ5) Which feature of Figure 3 is better than Figure 2? Shows all the dataMakes patterns easy to seePresents data honestlyDraws graphics clearlyIs accesssible\nQ6) Which feature of Figure 2 is better than Figure 3? Shows all the dataMakes patterns easy to seePresents data honestlyDraws graphics clearlyIs accesssible\n\n\n\n\n\n\n\n\n\nFigure 4: Canabalistic dads, revisited (again).\n\n\n\n\n\n\nQ7) What is the biggest problem with Figure 4? It does not show all the dataIt does not make patterns easy to seeIt does not display patterns honestlyIt does not draw graphics clearly\n\n\nExplanation\n\nThe y-axis is labeled \"count,\" so unlike Figure 2 and Figure 3, this plot shows only the number of cannibalism cases—not the proportion. As a result, a reader might misinterpret Figure 4 and incorrectly conclude that broods with only one father are the most susceptible to cannibalism.\n\nQ8) Which of the figure above do you like the best and why?\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Data viz test from an online advertisement.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Dataviz Summary"
    ]
  },
  {
    "objectID": "book_sections/data_viz/dataviz_summary.html#datviz_summary_glossary-of-terms",
    "href": "book_sections/data_viz/dataviz_summary.html#datviz_summary_glossary-of-terms",
    "title": "• 9. Dataviz Summary",
    "section": "📊 Glossary of Terms",
    "text": "📊 Glossary of Terms\n\n\n🏷 1. ️ Figure Elements & Interpretation\n\nLegend: A guide that explains the meaning of colors, symbols, or line types in a plot. Helpful when symbols are ambiguous, but often unnecessary when direct labeling is used.\nCaption: Text beneath a figure that highlights the main point and guides the reader’s interpretation. A good caption doesn’t just restate what’s shown—it helps make sense of it.\nDirect Labeling: Placing labels directly on or near data elements (e.g., lines, points, bars), so viewers don’t have to cross-reference with a legend. Especially useful in talks and posters.\nRedundant Coding: Encoding the same variable multiple ways (e.g., using both color and shape for species). Can increase accessibility but should be used carefully to avoid clutter.\n\n\n\n♿ 2. Accessibility & Universal Design\n\nAlt Text: A textual description of a figure, written for people who cannot see it. Good alt text conveys the message of the figure, not just its parts.\nAccessibility: Designing figures so they can be understood by people with diverse abilities (e.g., colorblindness, low vision, screen reader users). Often overlaps with universal design.\nColorblindness: A common visual condition that affects how people perceive color. Plots should use color palettes and redundancy (e.g., line types) to remain interpretable without relying on color alone.\nUniversal Design: The principle of creating products and experiences—like data visualizations—that work well for as many people as possible, regardless of ability.\n\n\n\n💥 3. Visual Clarity & Distraction\n\nOverplotting: When data points are so densely packed they obscure patterns or hide important features. Common with large datasets; solutions include transparency, jittering, or summarizing.\nChartjunk: Any visual element in a plot that doesn’t help convey the data—like heavy gridlines, excessive shading, or unnecessary 3D effects. Coined by Edward Tufte.\nData Viz “Duck”: A graphic with unnecessary visual decoration (named after a duck-shaped building in Long Island). A plot that prioritizes aesthetics or novelty over clarity.\nCognitive Burden: The mental effort required to interpret a figure. Good visualizations reduce cognitive burden by being clear, consistent, and well-structured.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Dataviz Summary"
    ]
  },
  {
    "objectID": "book_sections/data_viz/dataviz_summary.html#datviz_summary_key-r-functions",
    "href": "book_sections/data_viz/dataviz_summary.html#datviz_summary_key-r-functions",
    "title": "• 9. Dataviz Summary",
    "section": "Key R Functions",
    "text": "Key R Functions\n\nThis section did not focus on R, but rather concepts for data visualization.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Dataviz Summary"
    ]
  },
  {
    "objectID": "book_sections/data_viz/dataviz_summary.html#r-packages-introduced",
    "href": "book_sections/data_viz/dataviz_summary.html#r-packages-introduced",
    "title": "• 9. Dataviz Summary",
    "section": "R Packages Introduced",
    "text": "R Packages Introduced\n\nThis section did not focus on R, but rather concepts for data visualization.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Dataviz Summary"
    ]
  },
  {
    "objectID": "book_sections/data_viz/dataviz_summary.html#datviz_summary_r-packages-introduced",
    "href": "book_sections/data_viz/dataviz_summary.html#datviz_summary_r-packages-introduced",
    "title": "• 9. Dataviz Summary",
    "section": "Additional resources",
    "text": "Additional resources\n\nOther web resources:\n\nFundamentals of Data Visualization (Wilke (2019)): A free online book about best practices in data viz. Also available in physical form.\n\nStorytelling with Data: The work of Cole Nussbaumer Knaflic. This website links to her books, a usefull chart guide and more (see links to her most relevant podcast episodes below).\n\nThoughts on poster presentations: NPR article on a brief trend in minimal poster design. A critique of this ide in Forbes.\nAxes of evil: How to lie with graphs: This short blogpost goes over some classic dishonest graphs.\n\nVideos:\n\nThe Art of Data Visualization | Off Book | PBS Digital Studios.\nCalling Bullshit: Misleading axes, Manipulating bin size Data viz ducks and Duck hunting, Glass slippers, The Principle of Proportional Ink.\n\nCorrelation and Causation: “Correlations are often used to make claims about causation. Be careful about the direction in which causality goes. For example: do food stamps cause poverty?”\n\nWhat are Correlations? :“Jevin providers an informal introduction to linear correlations.”\n\nSpurious Correlations?: “We look at Tyler Vigen’s silly examples of quantities appear to be correlated over time), and note that scientific studies may accidentally pick up on similarly meaningless relationships.”\n\nCorrelation Exercise” “When is correlation all you need, and causation is beside the point? Can you figure out which way causality goes for each of several correlations?”\nCommon Causes: “We explain how common causes can generate correlations between otherwise unrelated variables, and look at the correlational evidence that storks bring babies. We look at the need to think about multiple contributing causes. The fallacy of post hoc propter ergo hoc: the mistaken belief that if two events happen sequentially, the first must have caused the second.”\n\nManipulative Experiments: “We look at how manipulative experiments can be used to work out the direction of causation in correlated variables, and sum up the questions one should ask when presented with a correlation.\n\n\nPodcasts:\n\nStorytelling with data: Here are some episodes that I think best complement this chapter #4 it depends…., #4 it depends…., #8 the many myths of data visualization, #10 right place, right graph, #17 which graph should I use?, #43 misleading graphs, #64 Beginner mistakes in data viz.\n\nSocial:\n\nGraph Crimes.\n\n\n\n\n\n\n\nWilke, C. O. (2019). Fundamentals of data visualization: A primer on making informative and compelling figures. O’Reilly Media.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Dataviz Summary"
    ]
  },
  {
    "objectID": "book_sections/betteR_plots.html",
    "href": "book_sections/betteR_plots.html",
    "title": "10. Better Figures in R",
    "section": "",
    "text": "Creating Engaging, Attractive Plots in R\nYou’ve already been introduced to the basics of ggplot and explored the key elements that make for effective figures. But at this point, you might be feeling a bit frustrated. You know how to generate plots in R, and you understand what makes a plot good, yet creating polished, impactful visuals in R still seems challenging. My advice is threefold:",
    "crumbs": [
      "10. Better Figures in R"
    ]
  },
  {
    "objectID": "book_sections/betteR_plots.html#creating-engaging-attractive-plots-in-r",
    "href": "book_sections/betteR_plots.html#creating-engaging-attractive-plots-in-r",
    "title": "10. Better Figures in R",
    "section": "",
    "text": "Start with the right plot for your data. For most exploratory plots—which will make up about 95% of what you create—this is key. Often, a well-chosen plot paired with a couple of quick R tricks can make your visuals clear and informative.\nThis chapter is here to guide you through the rest. For more advanced plotting and customization, take a look at these excellent resources: The R Graphics Cookbook (Chang, 2020), ggplot2: Elegant Graphics for Data Analysis (Wickham, 2016), Data Visualization: A Practical Introduction (Healy, 2018), and Modern Data Visualization with R (Kabacoff, 2024).\n\n\n\n\n\n\n\nThe right (gg)plot for your data (click to expand)\n\n\n\n\n\n\n\n\nData Type\nSuggested Plot(s)\nLikely geom\nWhat It Shows\n\n\n\n\nOne numeric variable\nHistogram, Density plot\ngeom_histogram, geom_density\nDistribution shape, spread, skew, outliers\n\n\nOne categorical + one numeric variable\nBoxplot, Violin plot, Sina plot\ngeom_boxplot, geom_violin, geom_sina\nGroup comparisons, spread, outliers\n\n\nTwo numeric variables\nScatterplot\ngeom_point + geom_smooth\nTrends, clusters, correlation, outliers\n\n\nCategorical counts or proportions\nBar plot, Stacked bar plot, Mosaic plot\ngeom_bar, geom_col, geom_mosaic\nFrequencies, relative proportions\n\n\nTime series (numeric over time)\nLine plot\ngeom_line\nTrends over time",
    "crumbs": [
      "10. Better Figures in R"
    ]
  },
  {
    "objectID": "book_sections/betteR_plots.html#review-what-makes-a-good-plot",
    "href": "book_sections/betteR_plots.html#review-what-makes-a-good-plot",
    "title": "10. Better Figures in R",
    "section": "Review: What Makes a Good Plot",
    "text": "Review: What Makes a Good Plot\n\nGood plots tell the story of the data.\n\nGood plots are tailored to the audience and the method of presentation.\n\nGood plots are Honest, Transparent, Clear, and Accessible.\n\nWe’ll explore these concepts in this chapter, with a particular focus on creating honest, transparent, and clear plots, as this is where R offers the most opportunities for customization.",
    "crumbs": [
      "10. Better Figures in R"
    ]
  },
  {
    "objectID": "book_sections/betteR_plots.html#review-avoiding-data-viz-time-sinks",
    "href": "book_sections/betteR_plots.html#review-avoiding-data-viz-time-sinks",
    "title": "10. Better Figures in R",
    "section": "Review: Avoiding data viz time sinks",
    "text": "Review: Avoiding data viz time sinks\n\nMaking and critiquing plots is one of my favorite parts of science — I absolutely love it! However, I know it can be a major time sink, and we want to avoid that. We already discussed this once.\n\nHere are my tips for preventing yourself from getting bogged down by every figure:\n\nKnow your goal: Determine whether you’re creating an exploratory or explanatory figure. Don’t waste time perfecting an exploratory plot — it is meant for quick insights, not for publication.\nStandardize your process: Develop a few go-to themes and color schemes that you use frequently. Save and reuse these templates so you can produce attractive plots without customizing each one from scratch.\nMaster the basics: Get comfortable with the most common tasks you’ll perform in ggplot2. Keep the ggplot cheat sheet handy, and bookmark additional resources that suit your workflow.\nPremature optimization is the root of all evil: Save detailed customizations (e.g., annotations, special formatting) for last. This way, you can focus on the essential elements of the plot first without getting bogged down in complex code prematurely.\n\nGet help: Reach out to friends, use Google, consult books, or turn to Generative AI and other resources to solve problems quickly. Remember, the more specific your question, the better the help you’ll receive!",
    "crumbs": [
      "10. Better Figures in R"
    ]
  },
  {
    "objectID": "book_sections/betteR_plots.html#whats-ahead-and-how-to-thrive",
    "href": "book_sections/betteR_plots.html#whats-ahead-and-how-to-thrive",
    "title": "10. Better Figures in R",
    "section": "What’s ahead and how to thrive",
    "text": "What’s ahead and how to thrive\nIn this chapter, we’ll tackle the challenge of creating great plots by breaking it down into three key parts.\n\nFirst, in Tools for BetteR Plots, we’ll build a problem-solving toolkit. You’ll learn how to find answers, use help files, and leverage modern resources to help you make the plots you want to make.\nNext, in Making cleaR Plots, we’ll put that toolkit to use in a detailed, step-by-step ‘makeover’ of a messy plot, transforming it into a clear, publication-ready figure.\nFinally, in Plots for the Medium, we’ll learn how to adapt our visualizations for specific contexts like scientific papers, talks, and posters, because a great plot is always tailored to its audience.\n\nAss usual, we conclude with a chapter summary.\n\n\n\n\nChang, W. (2020). R graphics cookbook: Practical recipes for visualizing data. https://r-graphics.org/\n\n\nHealy, K. (2018). Data visualization: A practical introduction. Princeton University Press.\n\n\nKabacoff, R. (2024). Modern data visualization with r. CRC Press.\n\n\nWickham, H. (2016). ggplot2: Elegant graphics for data analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org",
    "crumbs": [
      "10. Better Figures in R"
    ]
  },
  {
    "objectID": "book_sections/betteR_plots/plotting_tools.html",
    "href": "book_sections/betteR_plots/plotting_tools.html",
    "title": "• 10. Tools for BetteR plots",
    "section": "",
    "text": "Motivating Scenario:\nYou’ve successfully created a basic ggplot, but now you want to make it look good. You know exactly what you want: the legend moved to the bottom, larger axis labels, a custom color palette, etc. but can’t figure out how to do it. Rather than spending hours in pain you ask: Which resources can I leverage to make this plot nice, and how can I use them?\nLearning Goals: By the end of this subchapter, you should be able to:\n\nAdopt a problem-solving mindset for coding by:\n\nFocusing on strategies for finding answers rather than knowing everything.\nIdentifying the part of your code you need.\n\nNavigate and use key resources to find solutions by:\n\nQuickly finding relevant examples in books and blogs.\nReading R’s built-in help() files.\n\nLeverage modern tools to accelerate your workflow by:\n\nUsing graphical tools like the ggThemeAssist RStudio add-in to generate theme code automatically.\nKnowing how and when to effectively google for help.\nUsing Generative AI (e.g., ChatGPT) responsibly to explain concepts, debug code, and suggest solutions.\n\nKnow how and when to ask people for help by preparing a clear, concise question with a minimal reproducible example.\n\n\n\n\nHow to get good at making nice figures\nMaking nice figures can be super fun, and getting good at this is a great skill… but it’s also somewhat “encyclopedic” – more about memorization and knowing your options than creativity. So I think there are two mindsets we can take toward learning how to get good at making plots:\n\n\nThis section is relevant for all coding in all languages This is simply the first time in the book I found it relevant.\n\n🤮 Rote memorization of the “encyclopedic” stuff is boring and frustrating. No one wants to memorize weird ggplot things, and it feels shitty when you don’t know a specific trick.\n🫶 Building a problem-solving toolkit while making nice plots is super empowering.\n\nI therefore focus on the latter strategy – rather than spending time memorizing all the ways to bend ggplot to your will, there are a bunch of resources (below) that help us learn by doing (or at least learn as we are doing). To me, this is the best way to learn.\n\nA key to getting help is figuring out what you actually need help with. Regardless of which tool you use (AI, a friend, Google, or a book) the first step is getting clear on what part you can do, and what part you’re stuck on. That separation makes it way easier to ask a good question, get a useful answer, and move forward.\n\n\n\nNo need to memorize anything: We just need to get good at using the available tools and knowing when to use which. The even better news is that by using these tools regularly and effectively, you’ll actually get better at making good plots on your own!\n\n\nBooks & Blogs\nIn my view, a good book or blogpost is the best way to learn (This is why I am writing this book, after all, and why I include additional resources in each chapter). Authors intentionally and patiently walk through the details of how to do something, and provide concepts and context to understand how and why it works. You don’t need to read any of these cover-to-cover. Think of them more like cookbooks or survival guides: flip to the bit you need (or use the search feature), get the idea, and move on. As noted above, a key is knowing what you need!\nA problem with a book is that it may not have exactly the thing you need right now, and might it not get to the point quickly. In theory if you master the material in a book you are likely to be able to do more complex stuff, but we don’t always have time for that. Sometimes we want the answer fast!\n\nFor more advanced plotting and customization, take a look at these excellent resources:.\n\nThe R Graphics Cookbook (Chang, 2020).\n\nggplot2: Elegant Graphics for Data Analysis (Wickham, 2016).\n\nData Visualization: A Practical Introduction (Healy, 2018).\nModern Data Visualization with R (Kabacoff, 2024).\n\n\n\n\n\nHelpfiles\nThe help() function in R can provide fast information on how to use a specific function. For me helpfiles are incredibly useful, but they take some expertise to use effectively:\n\nFirst you must know the function you need help with. This isn’t always easy, as if you knew the function you might not need help. If you don’t know the function you need, or you can’t make sense of the helpfile, try google!\nSecond, even if you know the function you need help with, helpfiles can be hard to read. Reading helpfiles is actually a skill. See this nice resource to walk you through using a helpfile.\n\nI suggest skimming the helpfile and pay the most attention to\n\nDescription: What the function does.\n\nUsage: How to run the function.\n\nArguments: What you give the function.\n\nExamples: Some examples of using the function.\n\n\nBetween a help file and ChatGPT lies a sweet spot: custom RAGs – language models trained on specific documentation. For example, the top right of https://ggplot2.tidyverse.org has a button labeled “Ask AI”. Clicking it brings you to a language model trained specifically on the help files for dplyr, ggplot2, and tidyr. It gives you answers grounded in the actual docs - without the pain of learning how to read them.\n\n\n\n\n\nThe ggplot website has an embedded RAG trained on dplyr, tidyr, and ggplot2 documentation.\n\n\n\n\nOr paste the help() output into your favorite LLM and ask it to help you understand how to read the helpfile and use the function.\n\n\n\nGoogle and Stackoverflow\nThere is a lot of information on the internet, and Google is your friend. If you don’t know how to do something, try googling it. Often Google searches lead to answers on stackoverflow a question and answer website for computer science.\n\n\nDon’t ask questions on stackoverflow Instead find answers there. Stackoverflow is not a particularly friendly place, they are not gentle with noobs, and get upset when a question you asked could be answered by anything written in the history of stackoverflow.\nGoogling is trickier than it sounds. Like making sense of help, knowing the appropriate search term, separating helpful from unhelpful answers and identifying where the useful information in a webpage is are all skills. These skills require practice and basic knowledge of R. Therefore you will see these tools become more valuable as you get more confident with R.\n\n\n\n\n\n\nGoogle the error message (click to expand)\n\n\n\n\n\nSometimes R doesn’t do what you want and spits out an error message\n\nggplot(aes(x = pull(iris,Sepal.Length), \n           y = pull(iris,Petal.Length))) +\n     geom_point()\n\nError in `fortify()`:\n! `data` must be a &lt;data.frame&gt;, or an object coercible by `fortify()`,\n  or a valid &lt;data.frame&gt;-like object coercible by `as.data.frame()`, not a\n  &lt;ggplot2::mapping&gt; object.\nℹ Did you accidentally pass `aes()` to the `data` argument?\n\n\n\n\n\n\n\nGoogling the error message can help find the answer!\n\n\n\n\n\n\n\n\n\nGUIs\nThe ggThemeAssist package provides a graphical user interface (GUI) that allows you to point and click your way to the desired figure. It then generates the corresponding R code for you (see Figure 1). I learned everything I know about the theme() function (a way to change font size, plot color etc…) from ggThemeAssist.\n\n\nAs of this writing ggplot changed the syntax for placing legends in a plot, so ggThemeAssist’s advice is wrong in that instance.\nTo use ggThemeAssist:\n\nInstall and load the package.\nCreate a ggplot in your R script.\nSelect ggplot Theme Assistant from the add-ins drop-down menu.\nA GUI will appear. Point and click your way through the options, and the corresponding R code will be inserted into your script.\n\n\n\n\n\n\n\n\n\nFigure 1: An example of how to use the ggThemeAssist package from the ggThemeAssist website. Do yourself a favor and use this package.\n\n\n\n\n\n\n\nPeople\nFriends, peers, mentors etc are the most valuable and useful help you can ask for - they often know where you’re coming from, and have had experiences similar to you. The problem is that people have limited bandwidth, limited patience, and don’t know everything. As in all cases in which we seek help, the clearer we can state our goal, and the more we can isolate our challenge the more useful the help we get will be.\n\n\nGenerative AI\n\n\nI wrote this in June 2025 GenAI is rapidly evolving, and ?might? get better/change by the time you read this. We currently don’t know the future impact of LLMs and coding / stats / jobs etc… LLMs are trained on a huge amount of public code and documentation But training doesn’t always reflect the most up-to-date info. In my experience LLMs are particularly bad with obscure R packages. In these cases LLMs often guess plausible but incorrect syntax.\nWhen a friend isn’t available, there’s generative AI (e.g., Claude, ChatGPT, and the like). Unlike friends, GenAI tools have infinite patience and access to way more information. I encourage you to use them to improve your figures when necessary - but I also feel compelled to offer a few warnings and bits of guidance.\n\nGenAI is most useful when you already know what you’re doing.\nI’m pretty good at coding in R. When I ask ChatGPT or Claude for help, it’s amazing. It shows me useful ways to solve problems. When the answer isn’t quite right (and it often isn’t), I can fix it or build from it.\nBy contrast, I know next to nothing about JavaScript. If I ask ChatGPT a JavaScript question and the answer is perfect, I’m in luck. But if it’s even a little off, I end up in a 90-minute debugging session with an LLM—and I’m no closer than when I started.\n\nDon’t trust generative AI to be correct These things are bullshitters. They don’t understand thing. They are written to please us. I could go on about the limitations of LLM’s - but I love them for making plots because it is so easy to see if they worked (does the plot look how I wanted it too) or not.\n\nKnow when to quit (or at least change prompts)\nIf you are going in circles with your favorite LLM about how to make your plot how you want and it’s just not working, take a step back. Think. Is this something GenAI cannot solve? Do I need to change my prompt? Do I need to think differently about the problem? etc.. Don’t waste hours here.\n\nAsking ChatGPT a question and copy-pasting the code it gives you might work. But if you want to actually learn—and avoid painful bugs—try this:\n\nRead the code.\nRead the explanation it gives you.\nRun the code in R and see what happens. PAY ATTENTION. Is this what you wanted?\n\nIf the code doesn’t work as expected, tweak the code to have it do something slightly different. This helps make sure you understand what it’s doing.\n\nIf the code doesn’t work as expected, go back to ChatGPT, books, or Google. Iterate until you understand what’s going on.\n\n\n\nGenAI is really good with error messages When we Googled the error message above, the best answer was the automatic one from generative AI. Googling error messages can work, but often this sends us in the wrong direction because the error message does not light up the right keywords.\n\n\nDo not share any data with a generative AI tool (e.g., ChatGPT, Claude) that you wouldn’t be comfortable posting publicly. Once your data is submitted to an LLM, it’s not uniquely yours anymore. If you’re working with sensitive or private data, do not paste them into a standard chatbot.\n\nUse fake data with the same structure.\n\nUse a built-in R dataset (like penguins, iris, or mtcars) with a similar shape.\n\n\n\n\n\n\nChang, W. (2020). R graphics cookbook: Practical recipes for visualizing data. https://r-graphics.org/\n\n\nHealy, K. (2018). Data visualization: A practical introduction. Princeton University Press.\n\n\nKabacoff, R. (2024). Modern data visualization with r. CRC Press.\n\n\nWickham, H. (2016). ggplot2: Elegant graphics for data analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org",
    "crumbs": [
      "10. Better Figures in R",
      "• 10. Tools for BetteR plots"
    ]
  },
  {
    "objectID": "book_sections/betteR_plots/cleaR_plots.html",
    "href": "book_sections/betteR_plots/cleaR_plots.html",
    "title": "• 10. Making cleaR plots",
    "section": "",
    "text": "Making Clear Plots in R\nIn the previous chapter we discussed that clear plots (1) Have Informative and Readable Labels (2) Minimize cognitive burden, (3) Make points obvious, and (4) Avoid distractions. In this subsection, we focus on how to accomplish these goals in ggplot.\nTo do so, we initially focus on a truly heinous plot, which aims to compare petal area across field sites and subspecies. We can see that Figure 1 is basically unreadable:\nSo, we give it a “makeover” to turn it into a solid explanatory plot.\nLoading and formatting hybrid zone data\nlibrary(stringr)\nhz_pheno_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_hz_phenotypes.csv\"\n\nhz_phenos &lt;- read_csv(hz_pheno_link) |&gt;\n  filter(replicate == \"N\")           |&gt;\n  select(site, ssp =subspecies, prot = avg_protandry, herk = avg_herkogamy, area = avg_petal_area, lat, lon) |&gt;\n  mutate(site_ssp = paste(site, ssp),\n         site_ssp = str_replace(string = site_ssp , pattern = \" X\\\\?\",replacement = \" uncertain\"),\n         site_ssp = str_replace(string = site_ssp , pattern = \" X\",replacement = \" xantiana\"),\n         site_ssp = str_replace(string = site_ssp , pattern = \" P\",replacement = \" parviflora\"))\nggplot(hz_phenos, aes(x = site_ssp, y = area)) +\n  geom_jitter(width = 1, height =1)\n\n\n\n\n\n\n\nFigure 1: Our starting plot. The x-axis labels are unreadable, and the legend labels are unclear, data points are all over the place.\nAfter improving this plot and considering alternatives, we conclude by introducing a few other data sets to cover additional topics in how to go from a solid exploratory plot to a good explanatory plot!",
    "crumbs": [
      "10. Better Figures in R",
      "• 10. Making cleaR plots"
    ]
  },
  {
    "objectID": "book_sections/betteR_plots/cleaR_plots.html#making-clear-plots-in-r",
    "href": "book_sections/betteR_plots/cleaR_plots.html#making-clear-plots-in-r",
    "title": "• 10. Making cleaR plots",
    "section": "",
    "text": "We can’t tell which data point is associated with which category.\nThe x-axis labels bump into each other, so we can’t read them anyway.\n\nHow are there negative values for area?\n\nThe meaning of area site_ssp, ssp, P, X, and X? are unclear.\n\nIt’s hard to follow patterns (but there are some bigger things and lower things)!\n\n\n\n\n\n\nEnsuring Labels Are Readable and Informative\n\nStep 1: Making Labels Readable by Flipping Coordinates\nThe first problem to solve is the overlapping text. There are two possible solutions:\n\nFlipping the x and y axes is my favorite solution because so the long labels have room to breathe on the y-axis (Panel: Switch x & y).\n\nRotating the labels on the x-axis is also acceptable, but can be a pain in the neck (Panel: Rotate X label).\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nSwitch x & yRotate x labels\n\n\nTo learn how to swap x and y axes, let’s start with the code from Figure 1 here.\n\nFirst run the code to make sure it works.\n\nIt should look like Figure 1.\n\nThen switch x and y and see what’s changed.\n\nIt should look like Figure 2.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nHere’s the answer if you can’t figure it out.\n\n\nggplot(hz_phenos, aes(x = area, y = site_ssp)) +\n  geom_jitter(width = 1, height =1)\n\n\n\n\n\n\n\nFigure 2: Our starting plot - now with flipped axes. The legend labels are unclear, data points are all over the place, but now we can read the categories, so that’s something.\n\n\n\n\n\n\n\n\nHere is the alternative solution in which we can rotate the x-axis labels, which we accomplish through the theme function:\n\nggplot(hz_phenos, aes(x =  site_ssp, y = area)) +\n  geom_jitter(width = 1, height =1)+\n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\nFigure 3: Our starting plot - now with rotated x-labels. The legend labels are unclear, data points are all over the place, but now we can read the categories, so that’s something.\n\n\n\n\n\n\n\n\n\n\n\nStep 2: Making Labels Informative by Changing Labels\nSpreadsheets and datasets often use shorthand for column names or categories. Such shorthand can make data analysis more efficient, but makes figures unclear to an outside audience. We could maybe guess that area referred to petal area, and that site_ssp meant the combination of site and species, but that’s not fully clear.\nReplace &lt;ADD A GOOD X LABEL HERE&gt; and &lt;ADD A GOOD Y LABEL HERE&gt; in the labs() function of the code below to make a clearly labelled figure (See my answer in Figure 4).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nYaniv’s code for clearer labels.\n\n\nggplot(hz_phenos, aes(x = area, y = site_ssp)) +\n  geom_jitter(width = 1, height =1)+\n  labs(x = \"Petal area (mm^2)\", y = \"Site and subspecies combination\")\n\n\n\n\n\n\n\nFigure 4: Our starting plot - now with flipped axesand better labels. The legend labels are unclear, data points are all over the place, but now we can read the categories and know what X and Y mean, so that’s something.\n\n\n\n\n\n\n\n\nStep 3: Picking Colors to Make Labels Informative\nAlthough the Y axis (now) should provide enough information to understand the plot, associating color with a variable can make patterns stick out.\nFigure 5 (in Panel: Default colors) does this by mapping subspecies onto color.\nFigure 6 (in Panel: Color choice + better labels + choose order) takes further control by picking colors ourselves or using a fun and informative color palette.\n\nDefault colorsColor choice + better labels + choose order\n\n\n\nggplot(hz_phenos, aes(x = area, y = site_ssp, color = ssp)) +\n  geom_jitter(width = 1, height =1)+\n  labs(x = \"Petal area (mm^2)\", \n       y = \"Site and subspecies combination\", \n       color = \"subspecies\")\n\n\n\n\n\n\n\nFigure 5: This plot improves on previous figures by using color to show which data point came from which subspecies.\n\n\n\n\n\n\n\nHere we have taken control of defaults, using scale_color_manual() to rename the categories within the legend.\n\nvalues = c(...) sets the colors for the categories.\n\nbreaks = c(\"X?\", \"X\", \"P\") specifies the original shorthand values from the data and sets the order they should appear in the legend.\n\nlabels = c(\"uncertain\", \"xantiana\", \"parviflora\") provides the new, descriptive labels that correspond to the items listed in breaks.\n\n\nggplot(hz_phenos, aes(x = area, y = site_ssp, color = ssp)) +\n  geom_jitter(width = 1, height =1)+\n  labs(x = \"Petal area (mm^2)\", \n       y = \"Site and subspecies combination\", \n       color = \"subspecies\")+\n  scale_color_manual(values = c(\"yellow\", \"red3\", \"cornflowerblue\"),\n                     breaks = c(\"X?\", \"X\", \"P\"), \n                     labels = c(\"uncertain\", \"xantiana\", \"parviflora\"))\n\n\n\n\n\n\n\nFigure 6: This plot improves on previous figures by using color to show which data point came from which subspecies. Colors are chosen intentionally and default category names are replaced with legible names.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChoosing Your ggplot2 Colors\n\n\n\n\n\nThere are many “color palettes” available in R to add some fun to you figures. Check out these options, but be sure to check for accessibility (the colorblindcheck package can help).\n\nRColorBrewer: This option comes with ggplot2. Use scale_fill_brewer() or scale_color_brewer() for a wide range of well-designed sequential, qualitative, and diverging palettes.\nviridis: The most commonly used palette for scientific plots is also built into ggplot2. Its palettes are perceptually uniform and friendly to viewers with color vision deficiency. Use scale_color_viridis_d() (for discrete data) or scale_color_viridis_c() (for continuous data). Change color to fill as necessary.\nThemed & Fun Palettes: Add personality to your plots with packages like wesanderson, or the artistically-inspired MetBrewer. These typically provide a vector of colors to use with scale_color_manual(). See this link for an extensive list of options.\nThe colorspace package: This package is great for creating your own high-quality, color-blind safe custom palettes (based on perceptually-uniform color models).\n\n\n\n\n\n\n\nMaking Patterns Clear\nWe’ve come a long way from Figure 1 – Figure 6 is much improved, and we can now see the xantiana likely has larger petals than parviflora. But it’s still hard to make much sense of these data. Let’s further clarify this plot.\n\nStep 4: Choosing the Appropriate jitter\nA huge problem with this plot are that data points are spread all over the place, because we used the geom_jitter() function. At times jittering points is a good way to prevent over-plotting - but it can be a problem when jittered points change our data or make patterns unclear. In our case jittering introduces both issues:\n\nBecause of the large jitter height, data points aren’t lined up with their category.\n\nBecause of the large jitter width, data points are wrong (notice the negative values for petal area.)\n\nThere are two solutions:\n1. Use geom_point(): I always use geom_point when x, and y are continuous variables. In such cases using jitter actually changes our data, and should be avoided.\n2. Choose appropriate jitter sizes: When an axis is categorical, jittering points along the axis makes sense, but\n\nBe sure that points don’t run across categories (jitter should be small) for the categorical variable.\nBe sure that points aren’t jittered for the axis with the continuous variable.\n\n\nggplot(hz_phenos, aes(x = area, y = site_ssp, color = ssp)) +\n  geom_jitter(width = 0, height =.25, size=3, slpha = .7)+\n  labs(x = \"Petal area (mm^2)\", \n       y = \"Site and subspecies combination\", \n       color = \"subspecies\")+\n  scale_color_manual(values = c(\"yellow\", \"red3\", \"cornflowerblue\"),\n                     breaks = c(\"X?\", \"X\", \"P\"), \n                     labels = c(\"uncertain\", \"xantiana\", \"parviflora\"))\n\n\n\n\n\n\n\nFigure 7: This plot improves on previous figures by using color to show which data point came from which subspecies. Colors are chosen intentionally and default category names are replaced with legible names. We can now see the true petal area, and unambiguously determine which category a datpoint came from (while avoiding overplotting)\n\n\n\n\n\n\n\nStep 5: Showing Data Summaries\nWe are really getting there! The previous plot shows the raw data clearly, but it’s still hard to precisely estimate the mean petal area for each group or see the uncertainty in that estimate. Summary statistics can guide the reader’s eye and make the main patterns more obvious.\nThe stat_summary() function computes summaries for us and add them to our plot. We’ll explore two common approaches:\n\nAdding bars to show the mean (Panel: Adding a bar).\n\nAdding points and error bars to show the mean and its uncertainty.(Panel: Adding errorbars).\n\n\nAdding a barAdding Errorbars\n\n\nBars allow for effective and rapid estimation of group means, and differences among groups. But adding bars to a plot without care can cover up our raw data. Three tricks to avoid this are:\n\nAdd the stat_summary() layer before geom_jitter(). to ensures the raw data points are plotted on top of the bars.\nMaking bars semi-transparent (via the alpha argument).\n\nMaking the bars a different color than the data points (e.g. fill = \"black\").\n\n\nggplot(hz_phenos, aes(x = area, y = site_ssp, color = ssp)) +\n  stat_summary(geom = \"bar\",alpha = .1)+\n  geom_jitter(width = 0, height =.25, size=3, alpha = .7)+\n  labs(x = \"Petal area (mm^2)\", \n       y = \"Site and subspecies combination\", \n       color = \"subspecies\")+\n  scale_color_manual(values = c(\"yellow\", \"red3\", \"cornflowerblue\"),\n                     breaks = c(\"X?\", \"X\", \"P\"), \n                     labels = c(\"uncertain\", \"xantiana\", \"parviflora\"))\n\n\n\n\n\n\n\nFigure 8: This plot improves on previous figures by adding a bar going from zero to each sample’s mean.\n\n\n\n\n\n\n\nAn alternative to bars is to show the mean and its uncertainty with a point and error bars. Here, we use stat_summary() again, we need to make some additional choices:\n\nWhat the bars should show I usually choose 95% Confidence intervals (more on that in a later chapter) withfun.data = \"mean_cl_normal\".\n\nNOTE: Standard errors,standard deviations, 95% confidence intervals and the like all different, and can be shown with bars. So you must communicate what the bars represent. I usually do this in the figure legend.\n\n\nHow to display the uncertainty I usually choose error bars geom = \"errorbar\" of modest width (width = 0.25), but geom = pointrange can work too.\n\n\nggplot(hz_phenos, aes(x = area, y = site_ssp, color = ssp)) +\n  stat_summary(fun = \"mean\", geom = \"bar\", alpha = 0.2) +\n  geom_jitter(width = 0.0, height = 0.1, size = 3, alpha = 0.7) +\n  stat_summary(fun.data = \"mean_cl_normal\", geom = \"errorbar\", \n               color = \"black\", width = 0.25, \n               position = position_nudge(x = 0, y=.35))+\n  labs(x = \"Petal area (mm^2)\", \n       y = \"Site and subspecies combination\", \n       color = \"subspecies\")+\n  scale_color_manual(values = c(\"yellow\", \"red3\", \"cornflowerblue\"),\n                     breaks = c(\"X?\", \"X\", \"P\"), \n                     labels = c(\"uncertain\", \"xantiana\", \"parviflora\"))\n\n\n\n\n\n\n\nFigure 9: This plot improves on previous figures by showing both means and 95% confidence intervals for each category.\n\n\n\n\n\n\n\n\n\n\n\nFacilitate Key Comparisons\nWe have previously seen that the way we arrange our data can highlight key comparisons and make trends obvious.\n\nStep 6: Arrange Categories In A Sensible Order\nBy default, R orders categorical variables alphabetically, which is rarely the most insightful arrangement. To make patterns stand out, you should order categories based on a meaningful value. Two such meaningful values are:\n\nThe order of categories If categories are ordinal show them in their natural order. (e.g. Months should go in order). Some things aren’t exactly ordinal but they may have an order that makes trends clear – for example our Clarkia field sites go (roughly) from south to north, so that order makes sense.\n\nThe order of values If categories cannot be sensibly arranged by something about them, it often helps to arrange them by a summary statistic, like the mean or median of the numeric response variable you are plotting. This makes patterns easiest to spot.\n\nWe can achieve either of these aims with functions in the forcats package. This pdf explains all the functions in the package, but most often I use:\n\n\nNOTE There is no connection between the order categories appear in a tibble and the order they are displayed in a plot. Changing the order of factors in a tibble will not change the way they are displayed in the tibble, and reordering observations in a tibble (e.g. with arrange()) will not change their order in a plot.\nLet’s give this a shot in our Clarkia hybrid zone dataset.\n\nFirst, let’s reorder “by hand” with fct_relevel().\nThen, let’s reorder by some value with fct_reorder().\n\nFinally, let’s reorder first by subspecies, and then by latitude with fct_reorder2().\n\n\nOrder “by hand”Order by a variableOrder by two things\n\n\nWe can use fct_relevel() to reorder categories “by hand.”\nBelow, I place \"S22 uncertain\" last (i.e. at the top). I do this by listing all variables in the order I want them. But if you just want to move one variable (as in this case), we can alternatively use the after argument:\n\nTo place it first \"MYVAR\", after = 0\nTo place it last \"MYVAR\", after = Inf\n\nChallenge: Change the code to place \"S22 uncertain\" first (i.e. at the bottom as in Figure 10).\nNote: Due to space considerations, this plot does not include all the best practices from above. Feel free to add them!\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nHere’s how to put ‘S22 uncertain’ first\n\nTo place S22 uncertain first, use fct_relevel(site_ssp, \"S22 uncertain\", after = 0)\n\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(ggplot2)\n\n# Reorder site_ssp placing S22 uncertain first\nhz_phenos &lt;- hz_phenos |&gt;\n    mutate(site_ssp = fct_relevel(site_ssp, \"S22 uncertain\", after = 0))\n\n# Plot the reordered data\nggplot(hz_phenos, aes(x = area, \n                          y = site_ssp, \n                          color = ssp)) +\n  stat_summary(fun = \"mean\", \n               geom = \"bar\", \n               alpha = 0.2) +\n  geom_jitter(width = 0.0, height = 0.1, \n              size = 3, alpha = 0.7) +\n  labs(y = \"Site & Subspecies (Ordered by Area)\", x = \"Petal Area\")\n\n\n\n\n\n\n\nFigure 10: A plot showing site and subspecies combinations with S22 uncertain last.\n\n\n\n\n\n\n\n\nWe can use fct_reorder() to reorder categories by the area of some variable. Below, I include the code to reoder from smallest to largest petal area. To get better with this approach, try the following challenges:\n\nReorder from biggest to smallest petal area by including .desc = TRUE in fct_reorder().\n\nSolution in Figure 11.\n\n\nReorder from smallest to biggest longitude (lon). .\n\nSolution in Figure 12.\n\n\nNote: Due to space considerations, this plot does not include all the best practices from above. Feel free to add them!\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nHere’s how to order by Descending Petal Area\n\nTo reorder the categories from the largest mean petal area to the smallest, we use fct_reorder() and set the .desc = TRUE argument. This flips the default ascending order.\n\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(ggplot2)\n\n# Reorder site_ssp by area, in descending order\nhz_phenos &lt;- hz_phenos |&gt;\n  filter(!is.na(area))|&gt;\n  mutate(site_ssp = fct_reorder(site_ssp, area, .desc = TRUE,.na_rm = TRUE))\n\n\n# Plot the reordered data\nggplot(hz_phenos, aes(x = area, \n                          y = site_ssp, \n                          color = ssp)) +\n  stat_summary(fun = \"mean\", \n               geom = \"bar\", \n               alpha = 0.2) +\n  geom_jitter(width = 0.0, height = 0.1, \n              size = 3, alpha = 0.7) +\n  labs(y = \"Site & Subspecies (Ordered by Area)\", x = \"Petal Area\")\n\n\n\n\n\n\n\nFigure 11: A plot showing site and subspecies combinations ordered by mean petal area, from largest (bottom) to smallest (top).\n\n\n\n\n\n\n\n\nHere’s how to order from smallest to biggest longitude\n\nTo reorder by longitude, let’s put that variable in!\n\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(ggplot2)\n\n# Reorder site_ssp by area, in descending order\nhz_phenos &lt;- hz_phenos |&gt;\n  mutate(site_ssp = fct_reorder(site_ssp, lon))\n\n# Plot the reordered data\nggplot(hz_phenos, aes(x = area,\n                      y = site_ssp, \n                      color = ssp)) +\n  stat_summary(fun = \"mean\", \n               geom = \"bar\", \n               alpha = 0.2) +\n  geom_jitter(width = 0.0, height = 0.1, \n              size = 3, alpha = 0.7) +\n  labs(y = \"Site & Subspecies (Ordered by Area)\", x = \"Petal Area\")\n\n\n\n\n\n\n\nFigure 12: A plot showing site and subspecies combinations ordered by mean longitude, from smallest (bottom) to largest (top).\n\n\n\n\n\n\n\n\nWe can order by more than one thing with fct_reorder2(). Below I order, first by longitude and then by subspecies, but strangely to do so, we type ssp first and then lon.\nChallenge: Change the code order first by subspecies and then by longitude..\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\nHere’s how to order by subspecies and then longitude\n\nTo reorder by subspecies and the longitude, try fct_reorder2(site_ssp, lon, ssp).\n\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(ggplot2)\n\n# Reorder site_ssp by subspecies and then by longitude.\nhz_phenos  &lt;- hz_phenos |&gt;\n  mutate(site_ssp = fct_reorder2(site_ssp, lon, ssp))\n\n# PLOT **Don't change this**  \nggplot(hz_phenos, aes(x = area, \n                      y = site_ssp, \n                      color = ssp)) +\n  stat_summary(fun = \"mean\", \n               geom = \"bar\", \n               alpha = 0.2) +\n  geom_jitter(width = 0.0, height = 0.1, \n              size = 3, alpha = 0.7) \n\n\n\n\n\n\n\nFigure 13: A plot showing site and subspecies combinations ordered by subspecies and the mean longitude.\n\n\n\n\n\n\n\n\n\nSummary Improving a Plot\nWe’ve come a long way from that first “heinous” plot! Let’s take a moment to appreciate the journey. We started with a plot that was confusing and basically unreadable. Step-by-step, we identified problems and applied targeted fixes:\n\nWe made labels readable by flipping the axes.\n\nWe made them informative by replacing shorthand with clear names.\n\nWe controlled the jitter to present the data’s position honestly.\n\nWe added summary bars and error bars to guide the reader’s eye to the key patterns.\n\nWe reordered the categories to make the comparison between groups clear and intuitive.\n\nThe big takeaway is that making a great explanatory plot is an iterative process. You don’t have to get it perfect on the first try. The key is to critically look at your plot, identify what’s confusing or unclear, and then use the tools at your disposal to fix it. Our final plot isn’t just “prettier”, it’s more honest, more informative, and a clearer story.\n\n\n\nBonus: Explore Alternative Visualizations\nIt’s always worthwhile to consider alternative visualizations of the same dataset to see which best reveals the key patterns in the data. I usually do this earlier in the figure-making process, but better late than never!\nHere, let’s use “small multiples” - a series of small plots that use the same scales and axes to explore two additional approaches to gaining insight from these data. In my view both of these represent improvements over their analogues in the previous plots because the facets separate the data to clearly highlight specific comparisons of interest.\nOPTION 1 Facet by site\nThe plot below “facets” data by site. I really like Figure 14 because it allows us to visually compare the petal area of different subspecies when they are found at the same site. This makes it easy to see that the difference in petal area between subspecies is largest at “Site 22” and smallest at “Site 6”.\n\nggplot(hz_phenos, aes(x = ssp, y = area, color = ssp)) +\n  stat_summary(fun = \"mean\", geom = \"bar\", alpha = 0.2) +\n  geom_jitter(width = 0.1, height = 0.0, size = 3, alpha = 0.7) +\n  stat_summary(fun.data = \"mean_cl_normal\", geom = \"errorbar\", \n               color = \"black\", width = 0.25, \n               position = position_nudge(x = .35, y=0))+\n  labs(y = \"Petal area (mm^2)\", \n       x = \"Site and subspecies combination\", \n       color = \"subspecies\")+\n  facet_wrap(~site, nrow = 1, labeller = \"label_both\")+\n  scale_color_manual(values = c(\"yellow\", \"red3\", \"cornflowerblue\"),\n                     breaks = c(\"X?\", \"X\", \"P\"), \n                     labels = c(\"uncertain\", \"xantiana\", \"parviflora\"))+\n  theme(axis.text = element_text(size = 12), \n        axis.title = element_text(size = 12),\n        strip.text = element_text(size = 12))\n\n\n\n\n\n\n\n\n\nFigure 14: A faceted plot showing the petal area of each subspecies, broken down by site. Each panel represents a different field site, allowing for a direct comparison of subspecies within that site. This highlights the differences in petal area between subspecies across sites.\n\n\n\n\n\nOPTION 2 Facet by subspecies\nThe plot below “facets” data by subspecies. I really like Figure 15 because it allows us to visually compare how the petal area for a given subspecies changes across sites. This makes it easy to see that, for example, parviflora plants have their largest petals at Site 6, while xantiana plants have their largest at Site 22 and smallest at Site 6.\n\nggplot(hz_phenos, aes(x = site, y = area, color = site)) +\n  stat_summary(fun = \"mean\", geom = \"bar\", alpha = 0.2) +\n  geom_jitter(width = 0.1, height = 0.0, size = 3, alpha = 0.7) +\n  stat_summary(fun.data = \"mean_cl_normal\", geom = \"errorbar\", \n               color = \"black\", width = 0.25, \n               position = position_nudge(x = .35, y=0))+\n  labs(y = \"Petal area (mm^2)\", \n       x = \"Site and subspecies combination\", \n       color = \"subspecies\")+\n  facet_wrap(~ssp, nrow = 1, labeller = \"label_both\")+\n  theme(axis.text = element_text(size = 12), \n        axis.title = element_text(size = 12),\n        strip.text = element_text(size = 12),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nFigure 15: A faceted plot comparing petal area across sites, with each panel dedicated to a single subspecies. This view makes it easy to assess how the petal area of a specific subspecies changes from one geographic site to another.\n\n\n\n\n\n\n\n\nBONUS: Direct labeling\nSometimes, a legend can feel like a detour for your reader’s eyes. Forcing them to look back and forth between the data and the key adds cognitive load. A great alternative is direct labeling, where you place labels right next to the data they describe.\nThere are two main tools for this in ggplot2:\n\nMethod 1: The “ggplot Way” with geom_label(): This approach uses the same aes() aesthetic mapping you’re already familiar with. You can map variables from your data to the label, x, and y aesthetics. It’s best when the position of your label depends on the data itself (e.g., placing a label at the mean of a group).\n\nIn the example below, we calculate the mean position for each penguin species on the fly and use that to place the labels.\n\nggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm, color = species)) +\n    geom_point(alpha = 0.5) +\n    # Add labels using a summarized data frame\n    geom_label(data = penguins |&gt;\n                 group_by(species) |&gt;\n                 summarise_at(c(\"bill_depth_mm\", \"bill_length_mm\"), mean, na.rm = TRUE),\n               aes(label = species), fontface = \"bold\", size = 4, alpha=.6) +\n    # Remove the redundant legend\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigure 16: A scatter plot of penguin bill dimensions that uses direct labeling. The geom_label() layer calculates the mean position for each species and places the label directly on the plot, making it easier to identify the groups without a legend.\n\n\n\n\n\n\nMethod 2: The “Manual Way” with annotate().\n\nThe annotate() function is for adding “one-off” plot elements. It does not use aesthetic mappings. Instead, you give it the exact coordinates and attributes for the thing you want to add.\nThis gives you precise control over label placement, but it comes at a price: it’s not linked to your data and won’t update automatically. It’s best for adding a single title, an arrow, or manually placing a few labels where the position is fixed. I often choose this at the very last step of making an explanatory plot when there is a specific space I can see is best for such labels.\n\nggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm, color = species)) +\n    geom_point(alpha = 0.5) +\n    # Add labels using a summarized data frame\n    annotate(geom = \"label\", label = c(\"Gentoo\", \"Chinstrap\", \"Adelie\"), \n             x = c(14, 18.5,20), y = c(55,55,34), \n             color = c(\"blue\",\"forestgreen\",\"red\"), \n             fontface = \"bold\", size = 5, alpha=.6)+\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigure 17: This plot demonstrates direct labeling using the annotate() function. This method provides precise control by requiring the user to manually specify the exact coordinates, text, and color for each label, independent of the data mapping.",
    "crumbs": [
      "10. Better Figures in R",
      "• 10. Making cleaR plots"
    ]
  },
  {
    "objectID": "book_sections/betteR_plots/plots_foR_medium.html",
    "href": "book_sections/betteR_plots/plots_foR_medium.html",
    "title": "[• 10. Plots for the medium]{#plots_foR_medium) .quarto-section-identifier}",
    "section": "",
    "text": "Motivating Scenario:\nYou’ve just completed a big analysis and created a clear, honest ggplot figure that perfectly shows your main result. Now you realize you have to give a talk, write a paper, and present a poster. You’re thinking about just using the same plot for each case, but realize that, forexample, no one will be able to read your axes labels if you present your plots in a talk. So now you want to customize your plot for the medium of presentation.\n\nLearning Goals: By the end of this subchapter, you should be able to:\n\nAdopt a strategic mindset for data visualization by recognizing that the “best” plot depends on its context and tailoring your design to the specific demands of the medium (e.g., talk, poster, paper).\nPrepare publication-quality figures for scientific papers by:\n\nCombining multiple plots into a single, cohesive multi-panel figure using patchwork.\nApplying clean, professional themes like theme_bw() or theme_classic().\nFormatting labels with mathematical symbols or italics using expression() and/or ggtext.\n\nDesign effective plots for live presentations like talks and posters by:\n\nDramatically increasing text size for readability from a distance using theme().\nUsing attention-grabbing visuals (like icons or isotype plots) to stand out in a crowded poster hall.\n\nEnhance plots for digital mediums by:\n\nMaking plots interactive with packages like plotly and highcharter.\nDesigning clear, compelling infographic components that tell a single, powerful story.\n\nKnow when to be pragmatic by deciding between a pure-R solution and using graphics software for final, one-off annotations and design touches.\n\n\n\nTailoring Plots for Your Medium\nThere are many ways to tell a story. You could share it informally with a friend, write it in a book, produce it as a play, or perform it live on stage (like on The Moth podcast). Even if the core story is the same, you would tailor your approach for each medium.\nThe same is true when presenting data visually. We’ve already discussed that a good plot is designed for its specific audience and purpose. You might present your findings in a scientific talk, on a poster, in a manuscript for a peer-reviewed journal, a digital document, or perhaps “clickbait” or an infographic for a broader audience. Each of these mediums has different demands.\nHere, we’ll go over how to customize your plots in R for a few of these common formats.\n\nMake your life easier by knowing when you can skip doing it in R.\nR is great because it is reproducible and dependable. What’s more, once you can do something in R, it’s pretty straightforward to apply that skill to a new dataset. This comes in handy when you’ll be doing the same task a bunch. But figuring out how to do a very specific, one-off tweak in R can be a huge time sink, and sometimes it makes more sense to make final touches in powerpoint, illustrator, or photoshop than to figure it our in R. Before you spend hours on a minor customization, ask yourself a few questions. Here is the guidance I use when making this decision:\n\nFrequency: Will I do this sort of thing more than a handful of times? If so, I try to figure it out in R. If it’s a one-off task, I often don’t.\n\nTime Investment: How long will this take to figure out in R versus doing it “by hand” in e.g. PowerPoint? If a manual edit in PowerPoint takes five minutes, that’s often smarter than spending two hours wrestling with code for a minor annotation.\nMotivation: Why do I want to do this in R? Is it for convenience, utility, and/or growth, or am I motivated by pride? Don’t do it for pride!\nMedium of Presentation: What is my final output? There is an expectation that figures for published papers are highly reproducible, so I use R exclusively for such figures. But for posters and talks, I am more likely to do a final touch-up “by hand.”\n\n\n\n\nPlots for a scientific talk\n\nUnfolding the narrative\nWe previously saw that talks offer a great opportunity to walk your audience through a plot as you build it up one step at a time. We also saw that as you do so you can provide the audience with plausible alternative outcomes and how we would interpret them biologically. You can achieve this by working through your ggplot code slowly (e.g. initially exclude geom_point() etc) and using annotate() or even Powerpoint to add alternative outcomes.\n\n\nMaking things BIG (literally)\n\n\nLoading and formatting hybrid zone data\nlibrary(dplyr)\nlibrary(stringr)\nhz_pheno_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_hz_phenotypes.csv\"\n\nhz_phenos &lt;- read_csv(hz_pheno_link) |&gt;\n  filter(replicate == \"N\", !is.na(avg_petal_area))           |&gt;\n  select(site, ssp =subspecies, prot = avg_protandry, herk = avg_herkogamy, area = avg_petal_area,lat, lon) |&gt;\n  mutate(site_ssp = paste(site, ssp),\n         site_ssp = str_replace(string = site_ssp , pattern = \" X\\\\?\",replacement = \" uncertain\"),\n         site_ssp = str_replace(string = site_ssp , pattern = \" X\",replacement = \" xantiana\"),\n         site_ssp = str_replace(string = site_ssp , pattern = \" P\",replacement = \" parviflora\"))\n\n\ninitial_plot &lt;- hz_phenos |&gt; \n  filter(ssp != \"X?\")      |&gt;\n  mutate(ssp = case_when(ssp == \"P\"~ \"parviflora\",\n                         ssp == \"X\"~ \"xantiana\"))|&gt;\n  ggplot(aes(x = site, y = area, color = site)) +\n  geom_jitter(width = 0.1, height = 0.0, size = 3, alpha = 0.7) +\n  stat_summary(fun.data = \"mean_cl_normal\", \n               geom = \"errorbar\", \n               width = 0.25, \n               position = position_nudge(x = .35, y=0))+\n  labs(x = \"Site\", \n       y = \"Petal area (mm^2)\",\n       color = \"subspecies\")+\n  facet_wrap(~ssp, nrow = 1)\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nPlots in scientific talks require large text! We achieve this largely through the theme() function where we use element_text() to customize a specific feature of our plot.\nBelow is an interactive R session to get you started on this!\n\nFirst, run the code as is to see the initial_plot with its default text sizes.\nNext, copy and paste the code from the margin on the right into the session to see how theme() can change the text size.\nChallenge: You will notice the y-axis tick mark labels are still small. Add a line of code inside theme() to make the axis.text.y larger as well.\nFinally, feel free to add any other elaborations / explorations.\n\n\n\ninitial_plot +\n   theme_linedraw()+\n  theme(axis.text.x =  element_text(size = 18), \n        axis.title.x = element_text(size = 25),\n        axis.title.y = element_text(size = 25),\n        strip.text = element_text(size = 25),\n        legend.position = \"none\")\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nRemember ggthemeassist can really help with these tasks, and is a great way to learn!\n\nggplot2 has several built-in themes (like theme_linedraw() I added above) built into ggplot2. Read more here. Additional packages like ggthemes or hrbrthemes have even more options.\nAs we did above, we can customize these themes - but be sure to do this after running the theme. For example, the code below makes a large x-axis title with theme_linedraw.\n\ninitial_plot + \n  theme_linedraw() +\n  theme(axis.title.x = element_text(size = 25))\n\nBy contrast, this next bit of code makes a standard x-axis title size with theme_linedraw.\n\ninitial_plot +\n  theme(axis.title.x = element_text(size = 25) ) + \n  theme_linedraw()\n\nRevisit the code above to see this for yourself.\n\n\n\n\nPlots for a poster presentation\nLike a plot for a talk, a plot for a poster must be readable from a distance. In fact, for a poster, this is even more critical.\n\nText should be huge\n\nUnlike a talk, a scientific poster does not get you a “captive audience.” Most poster presentations take place in large, crowded halls. So, I recommend having something about your poster that makes passersby want to look more closely. To achieve this, I often tend to break some of the standard plotting rules. For example, some carefully chosen “distractions” or “chartjunk” can get you attention. If you can do this while keeping your plot honest and interpretable, you’re in great shape.\n\nBelow I show how you can add images as data points with the ggimage package.\nIn the previous edition of this book, I showed how you could make bargraphs with pictures (so called “isotype plots”) with the ggtextures package. Read my previous text here.\n\n\n\nUsing ggimage\nlibrary(ggimage)\n\nplot_data &lt;- hz_phenos |&gt; \n    filter(ssp != \"X?\")   \n\nmean_area &lt;- plot_data |&gt;\n    group_by(site,ssp)|&gt;\n    summarise(area= mean(area))\n\n\nggplot(plot_data,aes(x = ssp, y = area)) +\n    geom_image( \n      data  = mean_area,\n      image = \"https://github.com/ybrandvain/datasets/blob/master/clarkia_petal.png?raw=true\",\n      size  = .1) +\n    geom_jitter( aes(color = ssp), size = 2, alpha = .2, width = .4, height = 0)+\n    facet_wrap(~site, ncol=4)+\n    scale_y_continuous(limits = c(0,2.5))+\n    stat_summary(aes(color = ssp),\n                 fun.data = \"mean_cl_normal\", \n                 geom = \"errorbar\", \n                 width = 0.25, \n                 position = position_nudge(x = .5, y=0))+\n  labs(x = \"Subspecies\", \n       y = \"Petal area (mm^2)\",\n       color = \"subspecies\")+\n  theme_test()+\n  theme(axis.text.x =  element_text(size = 18), \n        axis.title.x = element_text(size = 25),\n        axis.title.y = element_text(size = 25),\n        strip.text = element_text(size = 25),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\nDigital documents\nFor online documents. Digital formats open up opportunities to engage readers with interactive graphs and animations, making data visualization more dynamic and accessible. Here are some powerful tools to consider:\n\nThe Shiny package allows you to build interactive web applications that let users update graphs, tables, and other visual outputs in real-time, facilitating exploratory data analysis and the creation of interactive dashboards.\ngganimate enables the creation of animations, bringing your data to life and capturing your audience’s attention more effectively. I used this package to make all of the gifs throughout this book.\nOther packages, such as plotly, ggiraph, rbokeh, and highcharter, offer additional capabilities for creating interactive and visually engaging graphs. These tools make it easy for users to explore your data in a more hands-on way. For example, Figure Figure 1 shows an interactive plot that focuses on a specific penguin species when you hover over its points.\n\nFor more information, check out the Interactive Graphs chapter in Modern Data Visualization with R (Kabacoff, 2024). For deeper exploration, see the book Interactive Web-Based Data Visualization with R, plotly, and shiny (Sievert, 2020).\n\nlibrary(highcharter)\n\nhchart(penguins, \"scatter\", hcaes(x = flipper_length_mm, \n                                  y = bill_length_mm, \n                                  group = species, \n                                  color = species))\n\n\n\n\n\n\n\nFigure 1: Example interactive plot using highcharter\n\n\n\n\n\n\nFor Scientific Papers: Polishing for Publication\nScientific papers have specific conventions, and your plots should meet them to look professional and be clearly understood. As discussed in the previous section (read the bit on Brown M&Ms) scientists often judge a paper based on features of figures. So be sure to invest time in cleaning up the theme, precisely formatting labels with italics or mathematical symbols, and combining several plots into a single, multi-panel figure.\n\n\nCombining Plots into a Multi-Panel Figure\nOften, a single figure in a paper needs to tell a complex story by showing multiple views of the data. The patchwork package is the best tool for this job. It lets you combine separate ggplot objects into a single, publication-ready figure with panel labels (A, B, C, etc.).\nIn Figure 2 we create three different plots from our Clarkia hybrid zone data and then combine them into one figure. Doing so, I highlight some tricks of patchwork:\n\nThe + sign combines plots right to left (but sometimes you need to give it further directions).\n\nThe / sign combines plots up and down (but sometimes you need to give it further directions).\n\nplot_layout(guides = \"collect\") combines like legends to minimize redundancy and prevent wasted space.\n\nplot_annotation(tag_levels = 'A') adds letters to plots.\n\nRead the documentation for more information.\n\n\nMaking mutli-panel plots with patchwork\n# Create plot 1: A density plot of petal area\np1 &lt;- ggplot(hz_phenos, aes(x = herk, fill = ssp)) +\n  geom_density(alpha = 0.7) +\n  theme_bw() +\n  labs(subtitle = \"Distribution of Herkogamy\",  x =  \"Herkogamy\")\n\n# Create plot 2: A density plot of protandry\np2 &lt;- ggplot(hz_phenos, aes(x = prot, fill = ssp)) +\n  geom_density(alpha = 0.7) +\n  theme_bw() +\n  labs(subtitle = \"Distribution of Protandry\", x = \"Protandry\")\n\n# Create plot 3: A scatter plot of two traits\np3 &lt;- ggplot(hz_phenos, aes(x = prot, y = herk, color = ssp)) +\n  geom_point(alpha = 0.7, size = 3, show.legend = FALSE) +\n  theme_bw() +\n  labs(subtitle = \"Trait Correlation\", x = \"Protandry\", y = \"Herkogamy\")\n\n\n\n# Combine them with patchwork\n(p1 + p2)/p3 +\n  plot_layout(guides = \"collect\") + # Collect legends into one\n  plot_annotation(tag_levels = 'A')   # Add \"A\", \"B\", and \"C\" panel labels\n\n\n\n\n\n\n\n\nFigure 2: A multi-panel figure created with the patchwork package to provide a comprehensive view of two floral traits in Clarkia subspecies. (A) A density plot showing the distribution of Herkogamy, revealing that subspecies ‘P’ (red) has much lower values than ‘X’ (green). (B) A similar density plot for Protandry. (C) A scatter plot revealing the positive correlation between the two traits. This combined view effectively shows both the individual trait distributions and their relationship in a single figure.\n\n\n\n\n\n\n\n\nAdding Mathematical and Styled Text\nScientific papers require precision in labels. This can mean adding mathematical symbols for units (like mm²) or using italics for species names, as is standard in biology. These goals can be reached in more than one way. Two standard approaches are:\n\nUsing the ggtext package which allows you to write in html. OR\n\nUsing the expression() function, which is more “classic R” but a bit of a pain.\n\n\n\nhtml help I don’t know how to write greek letter or whatever in html. Luckily you can consult these resources (for html math symbols and html for Greek letters ) or ask a chatbot to help!\nUnfold the code below to see the ggtext approach I used to make Figure 3. Note however, that you will likely run into code doing it a different way elsewhere.\n\n\nItalics and math\n# You may need to install ggtext: install.packages(\"ggtext\")\nlibrary(ggtext)\n\n# First, create proper species names for the labels\nhz_phenos_formatted &lt;- hz_phenos |&gt;\n  mutate(ssp_name = case_when(\n    ssp == \"P\" ~ \"*C. x. parviflora*\", # Use Markdown for italics\n    ssp == \"X\" ~ \"*C. x. xantiana*\",\n    ssp == \"X?\" ~ \"Uncertain\")) |&gt;\n  filter(ssp_name != \"Uncertain\")\n\nggplot(hz_phenos_formatted, aes(x = area, fill = ssp_name)) +\n  geom_histogram(bins = 15, color = \"white\", alpha = 0.8) +\n  facet_wrap(~ssp_name) +\n  theme_linedraw() +\n  labs(y = \"Frequency\",\n       x = \"Petal Area  (in mm&lt;sup&gt;2&lt;/sup&gt;)\",\n       fill = \"Subspecies\") +\n  theme(strip.text = element_markdown(size =18), # Use element_markdown() to render italics\n        legend.text = element_markdown(),# Use element_markdown() to render italics\n        axis.title = element_markdown(size = 15),\n        legend.position = \"bottom\")  \n\n\n\n\n\n\n\n\nFigure 3: Faceted histograms comparing the distribution of petal area for two subspecies. This plot italicizes subspecies names and uses a super-script to show that a value is squared.\n\n\n\n\n\n\n\n\nUsing Publication-Ready Themes\nThe default ggplot2 theme with its grey background is great for data exploration, but it’s not the standard for formal publications. Part of this goes back to the “brown M&M thing” - readers trust authors who make conscious decisions rather than defaulting to pre-set options. So, a standard step in making “publication-ready” plot is to apply a cleaner, simpler theme. As shown in Figure 4, functions like theme_bw(), theme_classic(), or theme_minimal() can help! (see above).\n\n\nUsing publication style themes\nlibrary(ggplot2)\nlibrary(patchwork) \n\n# A basic plot with the default theme\np_default &lt;- ggplot(hz_phenos, aes(x = prot, y = herk, color = ssp)) +\n  geom_point(alpha = 0.7) +\n  labs(title = \"Default ggplot2 Theme\")+\n  theme(legend.position = \"bottom\")\n\n# The same plot with theme_classic()\np_classic &lt;- ggplot(hz_phenos, aes(x = prot, y = herk, color = ssp)) +\n  geom_point(alpha = 0.7) +\n  theme_classic() + # Apply the new theme\n  labs(title = \"theme_classic()\")+\n  theme(legend.position = \"bottom\")\n\n# Display side-by-side for comparison\np_default + p_classic\n\n\n\n\n\n\n\n\nFigure 4: A comparison of ggplot2 themes for a scatter plot of Clarkia traits. (Left) The default theme_grey() has a grey background and grid lines, excellent for data exploration. (Right) Applying theme_classic() removes the background and grid, creating a cleaner look often preferred for scientific publications.\n\n\n\n\n\n\nBe sure to use a consistent theme throughout the manuscript.\n\n\n\n\n\nFrom Chart to Infographic: Telling a Visual Story\nSo far, we have focused on making excellent charts for scientific communication. But what if you need to explain your findings to the public, on a website, or in a report for a general audience? That’s when you move from a chart to an infographic.\nWhile a scientific plot must show the full, transparent distribution of data, an infographic often simplifies the view to make a single message more powerful and clear. An infographic is much like poster presentation but with fewer details – it should grab attention and make a clear point and assume the audience trusts us. This means shifting your priorities from showing comprehensive data to communicating one single, powerful message. While we don’t lie with infographics, our goal is to make the point clear, and not worry as much about transparency.\nLet’s take our Clarkia hybrid zone data and create one component for an infographic. Our story is simple: one subspecies has a larger petal area than the other. Notice how the code below differs from our previous plots. We summarize the data before plotting, remove all axes, and use a bold title to state the main finding directly. Figure 5 is designed to be a self-contained story.\n\n\nMaking an infographic\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales) # For number() formatting\nlibrary(ggtext)\n# 1. Summarize the data to get the key takeaway\nsummary_data &lt;- hz_phenos |&gt;\n  filter(ssp %in% c(\"P\", \"X\")) |&gt;\n  group_by(ssp,site) |&gt;\n  summarise(mean_area = mean(area, na.rm = TRUE)) |&gt;\n  mutate(ssp_name = if_else(ssp == \"P\", \"parviflora\", \"xantiana\"))\n\n# 2. Build the infographic-style plot\nggplot(summary_data, aes(x = ssp_name, y = mean_area, fill = ssp_name, size = mean_area)) +\n    geom_col(width = 0.6, show.legend = FALSE) +\n    # Add direct labels with formatted numbers\n    geom_text(\n        y=0,\n        aes(label = number(mean_area, accuracy = 0.01)),\n        vjust = -0.2, # Nudge text just above the bar\n      #  size = 7,\n        fontface = \"bold\",\n      show.legend = FALSE\n    ) +\n  scale_size_continuous(range = c(2, 15))+\n    # Use a deliberate, simple color palette\n  scale_fill_manual(values = c(\"parviflora\" = \"gray60\", \"xantiana\" = \"#56B4E9\"))+\n  scale_x_discrete(labels =  c(\n  parviflora= \"&lt;br&gt;parviflora&lt;br&gt;&lt;br&gt;&lt;img src='https://www.calflora.org/app/up/entry/57/17193.jpg'\n    width='100' /&gt;\",\n  xantiana = \"&lt;br&gt;xantiana&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;img src='https://www.calflora.org/app/up/io/132/io39763-0.jpg'\n    width='100' /&gt;&lt;br&gt;\")) +\n    # Add a strong title and subtitle that tell the story\n    theme_void() +\n    labs(\n        title = \"Clarkia xantiana Has Larger Petals\",\n        subtitle = \"Average petal area for two Clarkia subspecies across all sites.\",\n        x = NULL, y = \"petal area\" # We don't need axis titles\n    ) +\n    # Start with a completely blank theme and add back only what we need\n    theme(\n        plot.title = element_text(size = 22, face = \"bold\", hjust = 0.5, margin = margin(b = 10)),\n        plot.subtitle = element_text(size = 18, hjust = 0.5, margin = margin(b = 20)),\n        axis.text.x = element_markdown(size = 18, face = \"bold\"),\n        axis.title.y = element_text(size = 22, face = \"bold\",angle = 90),\n        strip.text =  element_text(size = 18, angle = 270),\n        # Add a buffer around the plot\n        plot.margin = margin(15, 15, 15, 15),\n        panel.border = element_rect(color = \"pink\", fill = NA, size = 1))+\n    facet_grid(site~., labeller = \"label_both\")\n\n\n\n\n\n\n\n\nFigure 5: An example of an infographic component created from the Clarkia dataset. The plot uses horizontal bars and direct labels to clearly show that xantiana has a larger average petal area than parviflora at each of the four field sites. Including photographs of the flowers makes the comparison more tangible and engaging for a general audience.\n\n\n\n\n\n\n\n\n\nKabacoff, R. (2024). Modern data visualization with r. CRC Press.\n\n\nSievert, C. (2020). Interactive web-based data visualization with r, plotly, and shiny. Chapman; Hall/CRC.",
    "crumbs": [
      "10. Better Figures in R",
      "• 10. Plots for the medium"
    ]
  },
  {
    "objectID": "book_sections/betteR_plots/betteR_plots_summary.html",
    "href": "book_sections/betteR_plots/betteR_plots_summary.html",
    "title": "• 10. Better ggplots summary",
    "section": "",
    "text": "Chapter summary\nLinks to: Summary. Chatbot tutor. Questions. Glossary. R functions. R packages. More resources.\nPolishing a ggplot plot is not about running a single command, but is an iterative process of refinement, moving from a default chart to a polished, explanatory figure.\nWe do this by layering components. We start with a basic plot and add functions to clarify labels (with the labs() function and the ggtext package) and control aesthetics like color and shape (with scale_*_manual()). We then adjust non-data elements with the theme() function, arrange categories logically with the forcats package, and combine plots into larger narratives with patchwork.\nThroughout this process, we also make sure to consider the presentation format, tailoring our choices for each specific medium. A working understaing of ggplot is essential, but luckily you don’t need to have all this memorized—knowing how to use books, friends, chatbots, GUIs like ggThemeAssist, and other resources can help!",
    "crumbs": [
      "10. Better Figures in R",
      "• 10. Better ggplots summary"
    ]
  },
  {
    "objectID": "book_sections/betteR_plots/betteR_plots_summary.html#better_plots_summary_chapter-summary",
    "href": "book_sections/betteR_plots/betteR_plots_summary.html#better_plots_summary_chapter-summary",
    "title": "• 10. Better ggplots summary",
    "section": "",
    "text": "Chatbot tutor\n\nPlease interact with this custom chatbot (link here) I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you got what you needed from it.",
    "crumbs": [
      "10. Better Figures in R",
      "• 10. Better ggplots summary"
    ]
  },
  {
    "objectID": "book_sections/betteR_plots/betteR_plots_summary.html#better_plots_summary_practice-questions",
    "href": "book_sections/betteR_plots/betteR_plots_summary.html#better_plots_summary_practice-questions",
    "title": "• 10. Better ggplots summary",
    "section": "Practice Questions",
    "text": "Practice Questions\nThe following questions will walk you through the iterative process of refining a plot, from a messy default to a polished, clear visualization.\nTo start, let’s create a basic plot from the palmerpenguins dataset. It shows the distribution of flipper lengths for each species. As you can see, it has several problems we need to fix!\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\n# The base plot we will improve upon\nbase_plot &lt;- ggplot(penguins, aes(x = species, y = flipper_length_mm)) +\n  geom_point()\n\nbase_plot\n\n\n\n\n\n\n\nFigure 1: Our starting point: a messy plot with overlapping points.\n\n\n\n\n\n\nQ1) The plot above suffers from severe overplotting, making it hard to see the distribution of points. Which geom_* function is specifically designed to fix this by adding a small amount of random noise to the points’ positions? geom_jitter()geom_point(position = ‘dodge’)geom_smooth()geom_bin2d()\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ2) Let’s fix the overplotting. In the R chunk below, replace geom_point() with the correct function from the previous question. To keep the data honest, make sure all y-values stay the same, and that x values are clearly associated with a category.\nAfter running the corrected code, what is the best description of the resulting plot?\n\n The points form a single, undifferentiated cloud. The plot now shows three horizontal bars instead of points. The points for each species are now spread out horizontally in distinct vertical columns. The plot is unchanged from the original.\n\n\n\nHint\n\nIn the geom_*_ Set the height argument to 0 and providing a small width (e.g., 0.2)\n\n\nQ3) Great! Now look at the x-axis. ggplot2 defaults to alphabetical order (Adelie, Chinstrap, Gentoo). Use fct_reorder() from the forcats package to reorder the species factor from largest to smallest median flipper length. Now which species now appears first (leftmost) on the x-axis? AdelieChinstrapGentooThe order remains alphabetical\n\n\nHint 1\n\nAdd a mutate() call before ggplot()\n\n\n\nHint 2\n\nAdd a .na_rm =TRUE to ignore NA values and .desc = TRUE to go from greatest to smallest.\n\n\n\nCode\n\n\nlibrary(forcats)\n\npenguins |&gt;\n  mutate(species = fct_reorder(species, flipper_length_mm, median, .na_rm =TRUE, .desc = TRUE))|&gt; \n  ggplot(aes(x = species, y = flipper_length_mm)) +\n  geom_jitter(width = 0.2, height = 0)\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nQ4) Our plot is now well-organized, but the labels are not publication-ready. Add a labs() layer to the good_start plot above to achieve the following:\n\nSet the title to “Penguin Flipper Lengths”\n\nSet the x axis label to “Species”\n\nSet the y axis label to “Flipper Length (mm)”\n\nThe labs() function is used to change titles and axis labels. If you also wanted to change the title of the color legend, which argument would you add inside labs()? legend.title = ‘My Title’color = ‘My Title’fill = ‘My Title’legend = ‘My Title’\n\nQ5) Now, imagine you need to put this plot on a slide for a presentation. The text is far too small. Add a theme() layer to the code below to make the axis titles (axis.title) size 20.\nInside theme(), the element_text() function has many arguments besides size. Which argument would you use to change the font from normal to bold? style = ‘bold’font = ‘bold’face = ‘bold’bold = TRUE\n\nQ6) Now you want the species names (e.g. Gentoo, Chinstrap, etc) and flipper lengths (e.g. 170, 210, etc.) to be large (size = 16). What would you add to the theme to do this?\n\n axis.text(size = 16) axis.text = element_text(size = 16) axis.names = element_text(size = 16) axis.ticks = element_text(size = 16)\n\n\nQ7) Now you want the species names (e.g. Gentoo, Chinstrap, etc) but not flipper lengths (e.g. 170, 210, etc.) to be italicized. What would you add to the theme to do this? (NOTE we did this a different ay in the chapter)\n\n element_text(size = 16, face = 'italic') axis.text.x = element_text(size = 16, face = 'italis'), axis.text.y = element_text(size = 16)\n\n\nQ8) The plot above shows the raw data well. Now, let’s add a summary statistic. In the webr chunk below, add a stat_summary() layer to display the mean value for each species as a large, black point (size = 5, color = \"black\"). Hint: You’ll need to specify fun = \"mean\" and geom = \"point\" inside stat_summary().\nAfter successfully adding the summary layer, what new visual element appears on your plot?\n\n A single large black point for each species, located at its mean value. A black horizontal line showing the mean for each species. All the jittered points for each species turn black. A black bar appears behind the points for each species.\n\n\nQ9) Faceting is a powerful way to create “small multiples.” In the chunk above, add a facet_wrap() layer to the scatter plot to create separate panels for each island.\nAfter adding the facet layer correctly, which Islands have Chinstrap penguins? (select all correct) BiscoeDreamTorgersennone of them\n\n\n\n📊 Glossary of Terms\n\n\n🎨 1. Core Visualization Concepts\n\nAesthetic Mapping: The process of connecting variables in the data to visual properties (aesthetics) of the plot, such as x/y position, color, shape, or size.\nGeom (Geometric Object): The visual shape used to represent data, such as a point, a bar, or a line.\nLayering: The process of building a plot by starting with a base and sequentially adding new visual elements on top of each other.\nCognitive Load: The mental effort required to interpret a plot. A well-designed figure reduces cognitive load by being clear and intuitive (e.g., using direct labels instead of a legend), allowing the reader to focus their brainpower on the data’s story, not on trying to figure out the plot itself.\n\n\n\n\n📊 2. Representing and Arranging Data\n\nJittering: Adding a small amount of random noise to the position of data points to prevent them from overlapping perfectly, making it easier to see the distribution of dense data.\nData Summary: A statistical value (like a mean or confidence interval) calculated from raw data and added to a plot to help guide the reader’s eye to a key pattern.\nCategorical Ordering: The deliberate arrangement of categorical data on an axis in a logical way (e.g., by size, by geographic location) to make trends more obvious, rather than using the default alphabetical order.\nRedundant Coding: The practice of mapping a single variable to multiple aesthetics (e.g., mapping a category to both color and shape). This improves clarity and accessibility.\n\n\n\n\n📝 3. Annotation and Polishing\n\nDirect Labeling: The practice of placing text labels directly on or next to data elements, rather than in a separate legend, to make a plot easier to read.\nMathematical Notation: The use of specially formatted text in labels to represent mathematical symbols, such as superscripts (for units like mm²), subscripts, or Greek letters.\nPlot Theme: The collection of all non-data elements of a plot that control its overall look and feel, such as the background color, grid lines, and font styles.\nColor Palette: A set of colors chosen to represent data in a plot. Palettes can be chosen for clarity (qualitative), to show a gradient (sequential), or for aesthetic style.\n\n\n\n\n🖼️ 4. Advanced Figure Types\n\nSmall Multiples (Faceting): A series of small plots that use the same scales and axes, with each plot showing a different subset of the data. This technique is used to make comparisons across groups.\nMulti-Panel Figure: A single figure that combines several individual plots into a larger, organized layout, often used in scientific papers to tell a complex story in a compact space.\nIsotype Plot: A type of chart that uses a repeated icon or image to represent quantity, often used in posters and infographics to be more engaging than a standard bar chart.\nInfographic: A visual representation of information that blends data visualization, graphic design, and text to tell a compelling narrative, typically for a general audience.\nInteractive Plot: A plot, typically for a digital medium, that allows the user to engage with the data by hovering, clicking, zooming, or filtering to explore the data themselves.\n\n\n\n\n✨ 5. Principles & Philosophy\n\nThe “Brown M&M” Principle: A reference to the band Van Halen, who used a “no brown M&Ms” clause in their contract as a quick test for attention to detail. In data visualization, it refers to a small flaw in a plot (like a typo or misalignment) that signals a potential lack of care in the more critical underlying analysis, eroding audience trust.\n\n\n\n\n\n\nKey R Functions\n*All functions are in the ggplot2 package unless otherwise stated.\n\n\ngeom_image() ([ggimage]): Adds images to a plot, often used to represent data points or summaries.\n\n\n📝 1. Labels & Annotations\n\nlabs(): The primary function for setting the plot’s title, subtitle, caption, and the labels for each axis and legend.\ngeom_label() / geom_text(): Adds text-based labels to a plot, mapping data variables to the label aesthetic. geom_label() adds a background box to the text.\nannotate(): Adds a single, “one-off” annotation (like a piece of text or a rectangle) to a plot at specific, manually-defined coordinates.\nelement_markdown() In the ggtext package: Used inside theme() to render plot text (like axis labels or facet titles) that contains Markdown or HTML for styling.\n\n\n\n\n🧮 2. Summaries & Ordering\n\nstat_summary(): Calculates summary statistics (like means or confidence intervals) on the fly and adds them to the plot as a new layer (e.g., as bars or errorbars). This in gplot2, but required the Hmisc package.\nfct_reorder() In the forcats package: Reorders the levels of a categorical variable (a factor) based on a summary of another variable (e.g., order sites by their mean petal area).\nfct_relevel() In the forcats package: Reorders the levels of a factor “by hand” into a specific, manually-defined order.\n\n\n\n\n🎨 3. Controlling Aesthetics (Colors, Shapes, etc.)\n\nscale_color_manual() / scale_fill_manual(): Manually sets the specific colors or fill colors for each level of a categorical variable.\nscale_color_brewer() / scale_fill_brewer(): Applies pre-made, high-quality color palettes from the RColorBrewer package.\nscale_color_viridis_d() / scale_color_viridis_c(): Applies perceptually uniform and colorblind-friendly palettes from viridis. Use _d for discrete data and _c for continuous data.\n\n\n\n\n🖼️ 4. Arranging & Combining Plots\n\nfacet_wrap(): Creates “small multiples” by splitting a plot into a series of panels based on the levels of a categorical variable.\nplot_annotation() In the forcats package: Adds overall titles and panel tags (e.g., A, B, C) to a combined figure.\nplot_layout() In the forcats package: Controls the layout of a combined figure, such as collecting all legends into a single area.\n\n\n\n\n⚙️ 5. Theming & Final Touches\n\ntheme(): The master function for modifying all non-data elements of the plot, such as backgrounds, grid lines, and text styles.\nelement_text(): Used inside theme() to specify the properties of text elements, like size, color, and face (e.g., “bold”).\ntheme_classic() / theme_bw(): Applies a complete, pre-made theme to a plot with a single command, often for a cleaner, publication-ready look.\n\n\n\n\n\n\nR Packages Introduced\n\n\n📦 Core Tidyverse & Plotting.\n\nggplot2: The core package we use for all plotting, based on the Grammar of Graphics.\ndplyr: Used for data manipulation and wrangling, like filter() and mutate().\nforcats: The essential tool for working with categorical variables (factors), especially for reordering them in a logical way.\n\n\n\n\n🖼️ Arranging, Annotating & Polishing Plots.\n\npatchwork: A tool for combining separate ggplot objects into a single, multi-panel figure.\nggtext: A powerful package that allows you to use rich text formatting (like Markdown and HTML) in plot labels for effects like italics and custom colors.\nHmisc: A general-purpose package that contains many useful functions, including mean_cl_normal for calculating confidence intervals in stat_summary().\nscales: Provides tools for controlling the formatting of numbers and labels on plot axes and legends.\n\n\n\n\n✨ AFlair (Images, Animations & Themes)\n\nggimage: Used to add images as data points or layers in your plots.\nggtextures: Allows you to create isotype plots (bar graphs made of images).\ngganimate: Brings your static ggplots to life by creating animations.\n[ggthemes]https://github.com/jrnold/ggthemes): Provides a collection of additional plot themes, including styles from publications like The Economist and The Wall Street Journal.\n\n\n\n\n🖱️ Interactivity\n\nplotly: A powerful package for creating interactive web-based graphics. Its ggplotly() function can make almost any ggplot interactive with one line of code.\nhighcharter: Another popular and powerful package for creating a wide variety of interactive charts.\nShiny: R’s framework for building full, interactive web applications and dashboards directly from your R code.\n\n\n\n\n🎨 Themed Color Palettes\n\nMetBrewer: Provides beautiful and accessible color palettes inspired by artworks from the Metropolitan Museum of Art.\nwesanderson: A fun package that provides color palettes inspired by the films of director Wes Anderson.\n\n\n\n\n🛠️ Helper Tools\n\nggThemeAssist: An RStudio add-in that provides a graphical user interface (GUI) for editing theme() elements, helping you learn how to customize your plot’s appearance.\n\n\n\n\n\nAdditional Resources\n\nInteractive Cookbooks & Galleries:\n\nThe R Graph Gallery: An extensive, searchable gallery of almost every chart type imaginable, created with ggplot2 and other R tools. Each example comes with the full, reproducible R code.\nFrom Data to Viz: A fantastic tool that helps you choose the right type of plot for your data. It provides a decision tree that leads to ggplot2 code for each chart.\nThe ggplot2 Extensions Gallery: The official showcase of packages that extend the power of ggplot2 with new geoms, themes, and capabilities. A goodt place to discover new visualization tools in the R ecosystem.\nA ggplot2 Tutorial for Beautiful Plotting in R Many fantastic examples. This is great! Have a look!\n\nOnline Books:\n\nR Graphics Cookbook, 2nd Edition: A great “how-to” manual by Winston Chang. It is a collection of practical, problem-oriented “recipes” for solving common plotting tasks in ggplot2.\nData Visualization: A Practical Introduction: Kieran Healy’s blends data visualization theory with practical ggplot2 code.\nInteractive web-based data visualization with R, plotly, and shiny: A guide by Carson Sievert for turning your ggplot2 plots into interactive graphics and building web applications. This is a bit dated, but still useful.\nData Visualization with R: A fantastic course on data visualization by Andrew Heiss.\n\nVideos & Community:\n\nThe #TidyTuesday Project: A weekly data project from the R for Data Science community. It is the best place to see hundreds of creative and inspiring examples of what’s possible with ggplot2 and to practice your own skills.\nData visualization with R’s tidyverse and allied packages A collection of videos by Pat Schloss includes maps, community microbiolgy and more.",
    "crumbs": [
      "10. Better Figures in R",
      "• 10. Better ggplots summary"
    ]
  },
  {
    "objectID": "book_sections/stats_foundation_index.html",
    "href": "book_sections/stats_foundation_index.html",
    "title": "Section III: Stats Foundations",
    "section": "",
    "text": "The major goals of statistics are to: (1) Summarize data, (2) Estimate uncertainty, (3) Test hypotheses, (4) Infer cause, and (5) Build models. Now that we can do things in R, and we can summarize data (and even build simple models), we are ready to dive into the second and third goals of statistics – to Estimate with Uncertainty and Test Hypotheses.\n\nAs we summarized data in the previous section we could precisely (and correctly) calculate a mean or a covariance or whatever. However, these calculations only described the data we collected, which is not exactly what we care about. We don’t just want to know if the specific small-flowered Clarkia plants in our sample set fewer hybrid seeds than the large-flowered plants we observed. Instead, we want to know if, in general, Clarkia plants with smaller flowers set fewer hybrid seeds than those with large flowers \\(^*\\).\n\n\\(^*\\) We actually often want to know if petal size causes a shift in hybrid seed set, but that issue of causal inference is reserved for a later section.\n\n\n\n\n\n\n\n\n\nFigure 1: A pretty scene of Clarkia’s home showing the world we get to summarize.\n\n\n\n\n\n\n\nSamples and Populations\nThe difference between describing our sample versus describing the population is the major logical gap we aim to fill with statistics. Filling this gap requires a few conceptual tools, and these tools make up the core of this part of the book:\n\nWe must first understand the process of sampling, including how and why a sample may differ from a population, which is the subject of Chapter 11.\nChapter 12 introduces how we can add a measure of humility to our estimates by noting the uncertainty around them, and introduces bootstrapping as a technique to think more clearly about and quantify this uncertainty.\nFinally, we aim to evaluate if two samples plausibly come from the same “population.” Chapter 13 lays out the foundational idea of Null Hypothesis Significance Testing (NHST), while Chapter 14 introduces permutation as a method to both clarify NHST and perform such a test.",
    "crumbs": [
      "Section III: Stats  Foundations"
    ]
  },
  {
    "objectID": "book_sections/sampling_intro.html",
    "href": "book_sections/sampling_intro.html",
    "title": "11. Intro to Sampling",
    "section": "",
    "text": "Sampling gone wrong\nRather than collecting and measuring each flower, we take a sample – a subset of a population. We characterize a sample by taking an estimate of population parameters. So far we have been calculating estimates for samples (e.g. average petal area), not parameters from populations.\nSo a major goal of a statistical analysis is how to go from conclusions about a sample that we can measure and observe, to the population(s) we care about. In doing so we must worry about:",
    "crumbs": [
      "11. Intro to Sampling"
    ]
  },
  {
    "objectID": "book_sections/sampling_intro.html#whats-ahead",
    "href": "book_sections/sampling_intro.html#whats-ahead",
    "title": "11. Intro to Sampling",
    "section": "What’s ahead",
    "text": "What’s ahead\nI cannot overstate how important the concept of sampling is to Statistics - understanding this chapter is key to understanding what statistics is all about.\nIn this section, we work through fundamental ideas of sampling.\n\nWe start with the idea of sampling,\n\nWe then consider what goes wrong in sampling – sampling error, sampling bias, and non-independent sampling.\n\nWe then consider how to sample better before concluding with a summary, a chatbot tutor, practice questions, a glossary, and additional resources.",
    "crumbs": [
      "11. Intro to Sampling"
    ]
  },
  {
    "objectID": "book_sections/sampling/sampling.html",
    "href": "book_sections/sampling/sampling.html",
    "title": "• 11. Sampling",
    "section": "",
    "text": "Estimate population parameters by sampling\nOf course, if we already had a well characterized population, there would be no need to sample – we would just know actual parameters, and there would be no need to do statistics. It’s usually too much work, costs too much money, or takes too much time to characterize an entire population and calculate its parameters. So we take estimates from a sample of the population.\nAs such, being able to imagine the process of sampling – how we sample, what can go wrong in sampling is perhaps the most important part of being good at statistics. I recommend walking around and imagining sampling in your free time.",
    "crumbs": [
      "11. Intro to Sampling",
      "• 11. Sampling"
    ]
  },
  {
    "objectID": "book_sections/sampling/sampling.html#random-sampling-example",
    "href": "book_sections/sampling/sampling.html#random-sampling-example",
    "title": "• 11. Sampling",
    "section": "Random sampling example",
    "text": "Random sampling example\nIn the real world, taking a random sample from a population can be a challenge (see above). But in R this is very easy. I work through an example to better wrap our heads around sampling.\nFor this example, let us pretend that our hybrid zone data from SAW is a population (i.e. that we sampled every individual). I note this is for demonstration purposes only – if we had information for an entire population we would not sample. But this should help us think about sampling! We use the slice_sample() function, with the arguments being the tibble of interest, n the size of our sample. The output are the individuals we sampled!\n\n\nLoading and formatting saw hybrid zone data.\nhz_pheno_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_hz_phenotypes.csv\"\n\nsaw_hz_pop &lt;- read_csv(hz_pheno_link)|&gt; \n  filter(site == \"SAW\") |&gt;\n  select(id,subspecies, node, protandry = avg_protandry, herkogamy = avg_herkogamy, petal_area = avg_petal_area)|&gt;\n  mutate(petal_area = round(petal_area, digits = 2))\n\n\n\n\nsample_size &lt;- 10\n\n# make a sample \nsaw_hz_sample &lt;- saw_hz_pop|&gt; \n slice_sample(n=sample_size)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Comparing parameters from our ‘population’ to estimates from our ‘sample’ for numerous traits.\n\n\n\n\n\n\nParameters versus estimates\nWe can now compare our estimates from our sample of size ten to our “true” parameter (remember this is not our true parameter we are just pretending that our initial sample is the entire population so we can learn this stuff better).\nFigure 2 clearly shows that our sample estimates differ from the true population parameters. This chance deviation is sampling error in action. While some estimates are close, others are not, highlighting the fundamental challenge for statisticians: how do we quantify our uncertainty about the true parameter when we only have one sample?\n\n\nCode\nlong_estimate &lt;- estimates |&gt; mutate(measure = \"estimate\")|&gt;pivot_longer(-measure)\nlong_param &lt;- parameters |&gt; mutate(measure = \"parameter\")|&gt;pivot_longer(-measure)\nbind_rows(long_estimate, long_param) |&gt;\n  rename(trait= name)|&gt;\n  ggplot(aes(y = measure, x = value, fill = trait, alpha = measure))+\n  geom_col()+\n  labs(y=\"\")+\n  facet_wrap(~trait, scales = \"free_x\",ncol = 1)+\n  scale_alpha_discrete(range = c(1,.5))+\n  theme(legend.position = \"none\",\n        axis.text = element_text(size = 20),\n        axis.title = element_text(size = 20),\n        strip.text =  element_text(size = 20))",
    "crumbs": [
      "11. Intro to Sampling",
      "• 11. Sampling"
    ]
  },
  {
    "objectID": "book_sections/sampling/sampling_error.html",
    "href": "book_sections/sampling/sampling_error.html",
    "title": "• 11. Sampling Error",
    "section": "",
    "text": "Sampling error is unavoidable\nWe concluded the previous subsection on sampling by taking a sample from what we pretended was an entire population. We then compared the estimates from this sample to the true parameter. Repeating this exercise (Figure 1) reveals that sample estimates differ not just from population parameters, but also from each other.\nEstimates from samples (e.g. the opaque bars in Figure 1) will differ from population parameters (e.g. the semi-transparent bars in Figure 1) due to chance. This chance deviation is called sampling error. But make no mistake — sampling error cannot be avoided.\nLarger samples and more precise measurements can reduce sampling error, but it will always exist because we take our samples by chance. In fact, I would call it the rule of sampling rather than sampling error.\nMuch of the material in this chapter — and about half of the content for the rest of this term — focuses on how to handle the law of sampling error. Sampling error is the focus of many statistical methods.\nThe interactive document below works us through the idea of sampling and sampling error. Throughout we are pretending that the “sawmill” hybrid zone data represents a complete census of a population. We then “sample” for illustrative purposes.",
    "crumbs": [
      "11. Intro to Sampling",
      "• 11. Sampling Error"
    ]
  },
  {
    "objectID": "book_sections/sampling/sampling_error.html#quantifying-uncertainty-due-to-sampling-error",
    "href": "book_sections/sampling/sampling_error.html#quantifying-uncertainty-due-to-sampling-error",
    "title": "• 11. Sampling Error",
    "section": "Quantifying uncertainty due to sampling error",
    "text": "Quantifying uncertainty due to sampling error\nThe most common summary of uncertainty is the standard error.\n\nThe standard error quantifies the expected variability in estimates as the standard deviation of the sampling distribution. If we had a sampling distribution in hand we could find this in R as sd(my_sampling_dist).\n\n\nWe almost never have a population characterized (after all that’s why we are sampling), so we never know the sampling distribution. In the real world, we use mathematical or computational tricks to guess a sampling distribution given the distribution of values in our sample.",
    "crumbs": [
      "11. Intro to Sampling",
      "• 11. Sampling Error"
    ]
  },
  {
    "objectID": "book_sections/sampling/sampling_error.html#minimizing-sampling-error",
    "href": "book_sections/sampling/sampling_error.html#minimizing-sampling-error",
    "title": "• 11. Sampling Error",
    "section": "Minimizing sampling error",
    "text": "Minimizing sampling error\nWe cannot eliminate sampling error, but we can do things to decrease it. Here are two ways we can reduce sampling error:\n\nDecrease the standard deviation in a sample. We only have so much control over this, because nature is variable, but more precise measurements, more homogeneous experimental conditions, and the like can decrease the variability in a sample.\nIncrease the sample size. As the sample size increases, our sample estimate gets closer and closer to the true population parameter. This is known as the law of large numbers. Remember that changing the sample size will not decrease the variability in our sample, it will simply decrease the expected difference between the sample estimate and the population mean.\n\nReturn to our web exercise to explore how sample size (\\(n\\)) influences the extent of sampling error. To do so, simply change sample_size to a small (e.g. 3) and large (e.g. 30) number and compare the difference between estimates and parameters.You will need to rerun all three R bits, in sequential order, and its probably best to do so a small handful of times.",
    "crumbs": [
      "11. Intro to Sampling",
      "• 11. Sampling Error"
    ]
  },
  {
    "objectID": "book_sections/sampling/sampling_error.html#be-wary-of-exceptional-results-from-small-samples",
    "href": "book_sections/sampling/sampling_error.html#be-wary-of-exceptional-results-from-small-samples",
    "title": "• 11. Sampling Error",
    "section": "Be Wary of Exceptional Results from Small Samples",
    "text": "Be Wary of Exceptional Results from Small Samples\nBecause sampling error is most pronounced in small samples, estimates from small samples can easily mislead us. Figure 3 compares the sampling distributions for the proportion of Chinstrap penguins in samples of size five, thirty, and one hundred. About one-third of samples of size five have exactly zero Chinstrap penguins. Seeing no Chinstrap penguins in such a sample would be unsurprising but could lead to misinterpretation. Imagine the headlines:\n\n“Chinstrap penguins have disappeared, and may be extinct!…”\n\n— Some unscrupulous newspaper, probably.\n\n\nThe very same sampling procedure from that same population (with a sample size of five) could occasionally result in an extreme case where more than half the penguins are Chinstrap penguins (this happens in about 6% of samples of size five). Such a result would yield a quite different headline:\n\n“Chinstrap penguins on the rise — could they be replacing other penguin species?”\n\n— Some unscrupulous newspaper, probably.\n\n\nA sample of size thirty is much less likely to mislead—it will only result in a sample with zero or a majority of Chinstrap penguins about once in a thousand times.\nThe numbers I provided above are correct and somewhat alarming. But it gets worse—since unremarkable numbers are hardly worth reporting (illustrated by the light grey coloring of unremarkable values in Figure 3), we’ll rarely see accurate headlines like this:\n\n“A survey of penguins shows an unremarkable proportion of three well-studied penguin species…”\n\n— A responsible, but quite boring newspaper.\n\n\n\n\n\n\n\n\n\n\nFigure 3: Comparing the sampling distribution of chinstrap penguins in samples of size five, thirty, and one hundred. The true population proportion is 0.198. Bars are colored by whether they are likely to be reported (less than 5% or more than 39%), with unremarkable observations in dark green.\n\n\n\n\n\nIn summary – whenever you see an exceptional claim, be sure to look at the sample size and measures of uncertainty. For a deeper dive into this issue, check out this optional reading: The Most Dangerous Equation (Wainer, 2007).",
    "crumbs": [
      "11. Intro to Sampling",
      "• 11. Sampling Error"
    ]
  },
  {
    "objectID": "book_sections/sampling/sampling_error.html#small-samples-overestimation-and-the-file-drawer-problem",
    "href": "book_sections/sampling/sampling_error.html#small-samples-overestimation-and-the-file-drawer-problem",
    "title": "• 11. Sampling Error",
    "section": "Small Samples, Overestimation, and the File Drawer Problem",
    "text": "Small Samples, Overestimation, and the File Drawer Problem\nLet’s say you have a new and exciting idea—maybe a pharmaceutical intervention to cure a deadly cancer. Before you commit to a large-scale study, you might do a small pilot project with a limited sample size. This is a necessary step before getting the funding, permits, and time needed for a bigger study.\n\nWhat if you found an amazing result? The drug worked even better than you expected! You would likely shout it from the rooftops—issue a press release, etc.\nWhat if you found something subtle? The drug might have helped, but the result is inconclusive. You might keep working on it, but more likely, you’d move on to a more promising target.\n\nAfter reading this section, you know that both of these outcomes could happen for two drugs with the exact same effect (see Figure 3). This combination of sampling and human nature has the unfortunate consequence that reported results are often biased toward extreme outcomes.\nThis issue, known as the file drawer problem (because underwhelming results are kept in a drawer somewhere, waiting for a mythical day when we have time to publish them), means that reported results are often overestimated, modest effects are under-reported, and follow-up studies tend to show weaker effects than the original studies. Importantly, this happens even when experiments are performed without bias, and insisting on statistical significance doesn’t solve the problem. It is therefore exceptionally important to report all results—even boring, negative ones.\n\n\n\n\nWainer, H. (2007). The most dangerous equation. American Scientist, 95(3), 249.",
    "crumbs": [
      "11. Intro to Sampling",
      "• 11. Sampling Error"
    ]
  },
  {
    "objectID": "book_sections/sampling/sampling_bias.html",
    "href": "book_sections/sampling/sampling_bias.html",
    "title": "• 11. Sampling Bias",
    "section": "",
    "text": "(Avoiding) Sampling Bias\nWe previously introduced sampling error - the unavoidable, random chance that makes our estimates differ from the truth. We noted that sampling error does not mean you did anything wrong, and showed that there are established tools to acknowledge and quantify sampling error.\nNow we look into a scarier problem: sampling bias. Sampling bias describes “bug” in sample collection that makes the sample unrepresentative of the population (for example volunteer bias in Figure 1). Unlike random sampling error, you can’t fix bias by collecting more data - a larger biased sample just provides more certainty in the wrong answer. Thinking hard about a study design, how it could result in bias, and what can be done to prevent such bias is one of the most important skills in applied statistics.\nBelow, we look into potential issues in sampling bias, largely building off of an imagined attempt to sample the GC Clarkia xantiana hybrid zone.\nSay we we are interested in knowing the proportion of Clarkia xantiana plants that are hybrids between the two subspecies, parviflora and xantiana. To make this problem a bit easier, lets focus our attention on one field site, GC. Here we change our question from “what proportion of Clarkia xantiana plants are hybrids between the subspecies?” to the more precise “what proportion of Clarkia xantiana plants at GC are hybrids between the subspecies?”\nLucky for us, Dave is a very diligent researcher, and this population is not too large. So he censused the population – finding each plant and inferring its subspecies identity by genomic analyses (Figure 2). But, what if the population was too large to census, or what if we weren’t as motivated as Dave? How could we randomly sample the population to get this estimate and how could things go wrong if we do not sample randomly? Below I lay out some potential problems that could lead to a biased sample.\nFigure 2: A map of the GC (Clarkia) hybrid zone. Each triangle represents an individual plant, colored according to its genetic makeup (Genetic PCA 1). The right panel provides a zoomed-in view of the area with the most hybrids.",
    "crumbs": [
      "11. Intro to Sampling",
      "• 11. Sampling Bias"
    ]
  },
  {
    "objectID": "book_sections/sampling/sampling_bias.html#avoiding-sampling-bias",
    "href": "book_sections/sampling/sampling_bias.html#avoiding-sampling-bias",
    "title": "• 11. Sampling Bias",
    "section": "",
    "text": "A sample of convenience\nImagine we were driving along the main road and stopped when we saw a large patch of Clarkia plants, roughly in the “zoomed in” region of Figure 2. If we collected all of those plants, we would mistakenly conclude that the population was roughly 1/4 hybrid, 1/4 xantiana, and 1/2 parviflora. In fact, the population is roughly 85% xantiana, 10% parviflora, and 5% hybrid. It’s not clear if this bias is due to luck (the hybrids and parviflora) just happen to be near the road, or something more predictable (e.g. hybrids and parviflora) do better in roadsides, it is clear that limiting our sample to the roadside would skew our view of the population. Of course. Here is an edited version of that section.\n\n\n“Volunteer” Bias\nSay that instead of sampling by the roadside, we walked around the entire hillside looking for Clarkia plants. Because xantiana flowers are larger, brighter, and more conspicuous, we might subconsciously sample them more often, biasing our sample towards xantiana.\nThis is a form of “volunteer” bias. As shown in Figure 1, this problem is especially common in surveys where the people who choose to respond are rarely representative of the entire population. The same principle applies in other contexts: it’s likely easier to catch a slow, docile mouse than a fast, skittish one, which would lead to a biased sample of animal behavior. et etc etc.\n\n\nA Bias in Timing\nOur Clarkia example highlights another potential issue: the timing of your sampling can matter. In general, parviflora flowers before xantiana does, so sampling early in the season will generate a different sample than sampling late in the season.\nI point out this specific challenge of this system to highlight a broader principle: every study system has its own biological quirks that can frustrate efforts to get a truly random sample. It’s your job to consider these biological issues before collecting data.\n\n\nSurvivorship bias\nSometimes, bias is introduced because the individuals we sample are only the “survivors” of some process. For example, in our Clarkia data it is reasonable to assume that hybrids are less likely to survive to flowering than parents. If this is the case, we would underestimate the proportion of hybrids in our sample.\n\n\n\n\n\n\n\n\n\nFigure 3: Hypothetical damage pattern on a WW2 bomber. Loosely based on data from an unillustrated report by Abraham Wald (1943), showing that a similar plane survived a single hit to the engine 60% of the time, but a hit to the fuselage or fuel system closer to 95% of the time. Picture is from wikipedia, uploaded by Martin Grandjean and shared under a Creative Commons Attribution-Share Alike 4.0 International license.\n\n\n\n\nThe classic example of survivorship bias comes from the location of bullet holes in WWII planes (Figure 3). Planes often returned with bullet holes in the wings and tails, suggesting to some in the army that these areas need better protection. The statistician Abraham Wald pointed out that it might be that planes shot in the engine and cockpit might be less likely to return than those shot in the tail - so it is the areas without bullet holes which require additional protection. This example has been elevated to a meme – a rapid shorthand for much survivorship bias.\n\nSometimes, you can address sampling bias by carefully reframing your research question to match what you actually sampled. For example, if we said, we were estimating the proportion of flowering Clarkia that were hybrids our bias would go away. However, this is not always the best solution – we want our statistics to answer biological questions of interest.\nChanging the question so it is statistically valid may help us feel better, but leaves the inital question unanswered. For example, we want to know how likely planes are to get hit in th cockpit, making the question about wings does not solve this.\n\n\n\nBias in Experiments\nWhile we’ve focused on sampling in observational studies, bias can also creep into experiments. Human psychology is powerful, and our expectations can easily influence results. For example, a doctor describing a new drug or a patient taking it might pay more attention to (or even imagine) signs of improvement. Similarly, a researcher might “notice” more pollinators visiting plants they spiked with extra sugar, simply because they are subconsciously looking for that outcome.\n\n\nRandomized study: Participants are randomly assigned to either the treatment or control group to ensure both groups are as similar as possible from the start.  Double-Blind study: This means that neither the participants nor the researchers assessing the outcomes know who is in the treatment group and who is in the control group. This setup prevents the expectations of either group from influencing the results.\nBias also arises if individuals aren’t randomly assigned to treatment groups. For example, if only the sickest patients are given a new medication, while the kinda sick patients are in the control group, we may see more success for the treatment because the sicker people had more room to improve. Whether the bias comes from how groups are assigned or from human expectations, the solution is the same – the gold standard for many experiments is the randomized, double-blind study.",
    "crumbs": [
      "11. Intro to Sampling",
      "• 11. Sampling Bias"
    ]
  },
  {
    "objectID": "book_sections/sampling/nonindependence.html",
    "href": "book_sections/sampling/nonindependence.html",
    "title": "• 11. Non-independence",
    "section": "",
    "text": "Motivating Scenario:\nYou are designing your study or critically evaluating someone else’s research. You know that a sample should be a random draw from a population, and that sampling bias can be a problem, but random sampling is a pain, and you are itching for more efficient ways to sample. You think you can (or the authors did) avoid sampling bias. What else should you worry about?\nLearning Goals: By the end of this subsection, you should be able to:\n\nDefine statistical independence and non-independence.\nExplain why non-independence is a problem.\nIdentify common sources of non-independence in biological data, such as genetic relatedness, spatial clustering, and experimental design flaws (e.g.pseudoreplication).\n\n\n\n\nNon-independence and a False Sense of Confidence\nWe have seen that statistics has tools to deal with sampling error - random differences between the sample and the population, for example by quantifying uncertainty as the standard error. However, these tools assume that samples are independent.\n\nA sample is independent - i.e. if knowing something about one observation gives you zero information about any other.\nA sample is non-independent if observations are related or clustered.\n\nMost of the statistics we do in this course relies on the assumption of independence. When we violate it, we can become wildly overconfident in our results, because our sample size isn’t what we think it is.\nBelow we walk through common sources of Non-Independence, anchoring most of this in the Clarkia System.\n\nLater in the term, we will learn two approaches to mitigate non-independence: adding covariates to models and how to build mixed effect models. However, such statistical solutions increase uncertainty and can add additional complications, so it’s best to collect independent samples whenever possible.\n\n\n\n\nNon-independence of measurements\nI’m not sure if I said this already, but phenotypes from our Clarkia hybrid zones are actually not those of the plants we collected. Rather these data come from seeds collected from these plants.\n\nThe issue: At site GC we measured up to three offspring of each maternal plant. So we ended up with 239 data points from 87 moms. Because siblings resemble one another these data are non-independent.\n\n\n\nWe aimed to measure three kids per mom, but in some cases only one or two survived.\n\nThe consequence: If we naively used all 239 observations to generate a sampling distribution we would be overconfident in our estimates. \nPotential solutions: So far, I’ve made these data independent by randomly choosing one kid per mom. An alternative approach would be to take the mean phenotype of the offspring, or two build a fancier model. For now I go with the random subsample because it is the most straightforward approach.\n\n\n\nPseudoreplication in Experimental Design\nLet’s switch from our hybrid zone study to our RIL experiment. Recall that some RILs were pink-flowered and some were white-flowered. We planted the RILs at four field sites, and watched pollinator visitation and genotyped seeds to estimate the proportion of seeds on a flower that were hybrids.\nCrucially we planted all RILs at all sites, but assume we didn’t. What would have happened if we planted pink-flowered plants at one site and white flowered plants at another?\nLet’s imagine that we planted pink flowered-plants only at “US” and white-flowered plants only at “LB”. As Figure 1 shows, in that case we would incorrectly and confidently conclude that white-flowered plants are more likely to set hybrid seed than are pink-flowered plants. This is why it is best to put spread treatments evenly across some background enviroenmnet when possible.\n\n\nCode for data processesing and plotting.\nril_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_rils.csv\"\nril_data &lt;-  read_csv(ril_link) |&gt;\n  filter(!is.na(prop_hybrid), !is.na(petal_color))|&gt;\n  select(location, prop_hybrid, petal_color)\n\nril_data |&gt; \n    filter((location == \"US\" & petal_color == \"pink\")|\n               (location == \"LB\" & petal_color == \"white\"))|&gt;\n    ggplot(aes(x = petal_color, y = prop_hybrid, color = petal_color, fill = petal_color))+\n    geom_jitter(height =0, width = .3, size= 3.5, alpha = .8)+\n    stat_summary(fun.y = \"mean\", geom= \"bar\", alpha = .3)+\n    stat_summary(fun.data  = mean_cl_boot, geom= \"errorbar\",\n                 width = .2, color = \"black\",linewidth =1)+\n    scale_color_manual(values = c(\"pink\",\"white\"))+\n    scale_fill_manual(values = c(\"pink\",\"white\"))+\n    theme_dark()+\n    theme(legend.position = \"none\",\n          axis.text = element_text(size = 20),\n          axis.title = element_text(size = 25))\n\n\n\n\n\n\n\n\nFigure 1: The proportion of hybrid offspring produced by pink- and white-flowered Clarkia plants. Because the pink-flowered and white-flowered plants were evaluated at different locations (“US” and “LG”, respectively), data are non-independent. This also creates a confounding bias.Error bars reflect 95% confidence intervals (see next chapter).\n\n\n\n\n\n\nPsedudoreplication or sampling bias? It’s not clear to me if the example above is a case of pseudoreplication or sampling bias, it’s probably both.\nThis is clearly pseudoreplication because unearned confidence in our estimated hybrid seed proportion comes from the fact that our repeated measurements come from one location for each flower color. Thus our estimate of hybrid seed proportion is shielded from the variation across locations.\nThis is also a case of bias because the locations had different things going on which systematically altered the probability of hybrid seed set.\n\n\n\nSpatial non-independence\n\n\n\n\n\n\n\n\n\nFigure 2: Former postdocs Shelley Sianta and Lauren Carley sampling Clarkia with a quadrat!\n\n\n\n\nReturning to our survey of the natural hybrid zone at “GC” say that rather than conveniently sampling near the main road (as we did in the sampling bias section) you decide to do something that you hope will generate a random sample. You decide to use your computer to randomly select a location in the field site. You center a 1x1 meter quadrat and then sample every single plant inside it. You do this repeatedly (making minor adjustments in the case that the quadrat overlaps a previously sampled region to avoid double counting). Unfortunately, you are still sampling non-independently.\n\nThe Issue: This is a form of cluster sampling. As above, the plants within that one quadrat are not independent. They share the same microhabitat and are more likely to be the same subspecies or hybrid status than to plants 100 meters away.\nThe consequence: We would be more certain in our conclusions than we deserve. \nPotential solutions: Just sample the plant in the grid closest to the randomly chosen point.\n\n\nNon-independence generates a false sense of certainty. The core problem is that with non-independence your effective sample size — the number of independent pieces of information you have — is much smaller than the number of plants you measured.",
    "crumbs": [
      "11. Intro to Sampling",
      "• 11. Non-independence"
    ]
  },
  {
    "objectID": "book_sections/sampling/better_sampling.html",
    "href": "book_sections/sampling/better_sampling.html",
    "title": "• 11. Sampling Better",
    "section": "",
    "text": "Motivating Scenario: We’ve seen that at best – because chance in inherent in sampling we cannot avoid sampling error. We’ve also seen that in the real world sampling bias and non-independence, can further complicate our aims of estimation and hypothesis testing. In this section I go over sampling strategies to generate precise and accurate estimates.\nLearning Goals: By the end of this chapter, you should be able to:\n\nIdentify the two key features of an ideal sample (large and random) and explain what problems they solve.\n\nKnow how to generate a random sample.\nHave some familiarity with good enough sampling approaches when a random sample cannot be obtains.\nList the key principles of robust experimental design, including blinding and avoiding pseudoreplication, and what problems they solve.\n\n\n\nLater, we will spend time considering experimental design, but given that we just worked through how sampling can go wrong, let’s consider how sampling and experimental design can go right.\nThe best sample is a large random sample.\n\nA large sample minimizes the extent of sampling error. Sampling error is unavoidable, but it can be minimized – the larger the sample the less sampling error and the larger the sample the less sampling error (see (#sampling_error)). Of course, larger samples come at a cost of time and energy, and each additional sample becomes less useful as our sample size gets large. So later in the term we will consider how to balance these issues in planning for a good sample size.\n\n\n\nAccording to the law of large numbers the average of a large random sample converges to the true population parameter.\n\nA random sample prevents non-independence and bias If you can, use a random number generator (e.g. the sample() or runif() functions in R) to select coordinates, individuals from a numbered list, or experimental plots to ensure that every individual has an equal and independent chance of being chosen.\n\nOf course, while getting a random sample is ideal, it’s not always plausible. Your best alternatives include Systematic Sampling e.g. stretching a 100-foot transect line across the hillside and sampling the single plant closest to the line (and withing x square feet) every 5 feet. This disciplined approach prevents you from only sampling the convenient spots or the big, showy patches of flowers.\n\nBest practice for experiments\n\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\nThere are also things we can do to minimize bias and non-independence in experiments. As stated previously, when conducting human trials a double blind study is best. Similarly, when working in non-humans (plants, animals, bacteria etc) its best if the observer is “blind” to the treatment. However, in some cases “blinding” is impossible or impractical – e.g. how can you watch pollinators and no see petal color? (see also Figure 1).\nFinally, it is best to avoid pseudoreplication by spreading treatments across conditions however this is not always possible. For example, because pesticides often “drift” we usually need to put distance between a pesticide and no pesticide treatment. Later in the term we will learn how to model such non-independence with random effect models.",
    "crumbs": [
      "11. Intro to Sampling",
      "• 11. Sampling Better"
    ]
  },
  {
    "objectID": "book_sections/sampling/sampling_summary.html",
    "href": "book_sections/sampling/sampling_summary.html",
    "title": "• 11. Sampling summary",
    "section": "",
    "text": "Links to: Summary. Chatbot tutor. Questions. Glossary. R functions. More resources.",
    "crumbs": [
      "11. Intro to Sampling",
      "• 11. Sampling summary"
    ]
  },
  {
    "objectID": "book_sections/sampling/sampling_summary.html#sampling_summary_chapter-summary",
    "href": "book_sections/sampling/sampling_summary.html#sampling_summary_chapter-summary",
    "title": "• 11. Sampling summary",
    "section": "Chapter summary",
    "text": "Chapter summary\nWe almost never know the truth (the population parameter), but we estimate it from a sample. Random chance (aka sampling error) ensures that our estimate will deviate from the true parameter. If sampling error is our only issue we can envision the “sampling distribution” – a histogram of what we would see if we repeated our study many times to characterize uncertainty. We quantify uncertainty as the “standard error” i.e. the standard deviation of the sampling distribution. We can decrease sampling error by increasing sample size. When samples are non-independent, we tend to underestimate our sampling error. When samples are not chosen at random, “sampling bias” can generate a systematic deviation between estimates and true population parameters. Random sampling is our best protection against non-independence and sampling bias.\n\nChatbot tutor\n\nPlease interact with this custom chatbot (link here) I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you got what you needed from it.",
    "crumbs": [
      "11. Intro to Sampling",
      "• 11. Sampling summary"
    ]
  },
  {
    "objectID": "book_sections/sampling/sampling_summary.html#sampling_summary_practice-questions",
    "href": "book_sections/sampling/sampling_summary.html#sampling_summary_practice-questions",
    "title": "• 11. Sampling summary",
    "section": "Practice Questions",
    "text": "Practice Questions\nTry these questions! By using the R environment you can work without leaving this “book”. To help you jump right into thinking and analysis.\n\nQ1) The ___ describes the variability among individual observations in a sample (or population). In other words, the ____ quantifies how far we expect individuals to deviate from the sample estimate. Standard errorStandard deviationError function.\nQ2) The ___ describes the variability among estimates (of a fixed size) from a population. In other words, the ____ quantifies how far we expect sample estimates to deviate from the population parameter. Standard errorStandard deviationError function.\n\n\n\n\n\n\n\n\n\nFigure 2: A histogram showing the distribution of lengths for all human genes. Most are very small, the median is less than 3kb!\n\n\n\n\n\n\nQ3) As seen in Figure 2, the distribution of human gene lengths is ___ (find all correct)\n\n Unimodal Bimodal Symmetric Right Skewed\n\n.\nQ4) Figure 2 displays ___\n\n the sampling distribution the population distribution\n\n\nSampling wrong: I tried to create a sampling distribution for median human gene length by randomly selecting nucleotides from the human exome, finding the gene they were in, noting its length, and removing it from my list until I had the lengths of fifty genes in the human genome. I did this one thousand times to get medians from one thousand samples of size fifty (Figure 3).\n\n\n\n\n\n\n\n\nFigure 3: The distribution of the estimated mean length of human genes. Here is how I sampled to get this distribution: (1) A base pair is randomly selected from the human genome. (2) If it falls within a gene, that gene’s length is recorded. If not, another base pair is chosen at random. (3) This process continues until 50 genes have been sampled.\n\n\n\n\n\n\nQ5) The difference between the true population mean (red line) and my estimates from samples of size fifty (bars in histogram) are most likely explained by\n\n Sampling bias Non-independence Sampling Error The unreliability of the law of large numbers\n\n.\nQ6)\n\nQ7) Why is the range of values so much smaller in Figure 3 than in Figure 2?\n\n Sampling bias Non-independence Sampling error Parameter estimates are usualy have a tighter distribution than all observed values.\n\n\nFor the following questions, refer to Figure 4 (right).\n\n\n\n\n\n\n\n\n\nFigure 4: The effect of sample size on the sampling distribution of the mean human gene length. Each panel shows a histogram of 1,000 sample means, with each mean calculated from a random sample of n genes.\n\n\n\n\n\nQ8) What aspect of sampling is responsible for the difference between the plots in Figure 4?\n\n There is less sampling bias in larger samples There is less non-independence in larger samples There is less sampling error in larger samples\n\nQ9) Which sample size is associated with the smallest standard error?\n\n 5 30 500 They all have approximately the same standard error\n\nQ10) For the sampling distribution for samples of size n in Figure 4, approximately what proportion samples have a mean greater than the population mean?\n\nn = 5: Way less than 0.01about 0.02about 0.25about 0.50.\n\nn = 500: Way less than 0.01about 0.02about 0.25about 0.50.\n\nQ11) For the sampling distribution for samples of size n in Figure 4, approximately what proportion samples have a mean greater than 3.8 kb?\n\nn = 30: Way less than 0.01about 0.02about 0.25about 0.50.\n\nn = 500: Way less than 0.01about 0.02about 0.25about 0.50.\n\n\nREFRESHER For the questions below summarize the gene lengths data set. If there is not an answer, type NA.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ12) The mean gene length is .\nQ13) The standard deviation in gene length .\nQ14) The standard error in estimated gene length .\n\n\nClick for answer\n\nThis is the entire populations. We have a parameter known withoutsampling error, so there is not standard error.",
    "crumbs": [
      "11. Intro to Sampling",
      "• 11. Sampling summary"
    ]
  },
  {
    "objectID": "book_sections/sampling/sampling_summary.html#sampling_summary_glossary-of-terms",
    "href": "book_sections/sampling/sampling_summary.html#sampling_summary_glossary-of-terms",
    "title": "• 11. Sampling summary",
    "section": "📊 Glossary of Terms",
    "text": "📊 Glossary of Terms\n\nConvenience sampling (aka haphazard sampling): Sampling whatever is easiest or closest.\nIndependent sample A sample where knowing something about one observation tells you nothing about the others.\nNon-independent sample A sample where some observations are related to each other. This messes with your uncertainty estimates unless accounted for.\nParameter: A number that describes the truth about a population (e.g. the actual mean petal area of all Clarkia xantiana plants on Earth).\nPseudoreplication: Using repeated but non-independent measurements as if they were independent. This can lead to overconfidence and misleading results.\nRandom sample: A sample where each individual has an equal chance of being selected.\nSampling: Selecting a subset of individuals from a population.\nSampling Bias: Any process that causes a sample to be systematically unrepresentative of the population.\nSampling Distribution: A histogram of what you’d get if you repeated your study over and over, taking a new sample each time and recording the resulting estimate.\nSampling Error: The random difference between an estimate from a sample and the true population parameter.\nStandard Error (SE): The standard deviation of the sampling distribution. Measures the expected variability in your estimates due to sampling error.\nSurvivorship bias: A kind of sampling bias where you only observe individuals that survive some process (e.g., returning warplanes), which can give a misleading picture of the whole population.",
    "crumbs": [
      "11. Intro to Sampling",
      "• 11. Sampling summary"
    ]
  },
  {
    "objectID": "book_sections/sampling/sampling_summary.html#sampling_summary_key-r-functions",
    "href": "book_sections/sampling/sampling_summary.html#sampling_summary_key-r-functions",
    "title": "• 11. Sampling summary",
    "section": "Key R Functions",
    "text": "Key R Functions\n\n\nsample(): Samples values from a vector.\n\nslice_sample() (dplyr): Samples rows from a tibble.",
    "crumbs": [
      "11. Intro to Sampling",
      "• 11. Sampling summary"
    ]
  },
  {
    "objectID": "book_sections/sampling/sampling_summary.html#sampling_summary_additional-resources",
    "href": "book_sections/sampling/sampling_summary.html#sampling_summary_additional-resources",
    "title": "• 11. Sampling summary",
    "section": "Additional resources",
    "text": "Additional resources\n\nReadings:\n\nModule: Standard Error from Teacups Giraffes and Statistics, by Hasse Walum and Desirée De Leon.\nThe Most Dangerous Equation (Wainer, 2007).\nChapter 7 Sampling from (Ismay & Kim, 2019).\nPages 104-112 and 126-133 about sampling bias from Bergstrom & West (2020).\nInterleaf 2: Pseudo-replication from Whitlock & Schluter (2020).\n\n\n\n\n\n\nBergstrom, C. T., & West, J. D. (2020). Calling bullshit: The art of skepticism in a data-driven world. Random House.\n\n\nIsmay, C., & Kim, A. Y. (2019). Statistical inference via data science: A ModernDive into r and the tidyverse. CRC Press.\n\n\nWainer, H. (2007). The most dangerous equation. American Scientist, 95(3), 249.\n\n\nWhitlock, M. C., & Schluter, D. (2020). The analysis of biological data (Third). Macmillan.",
    "crumbs": [
      "11. Intro to Sampling",
      "• 11. Sampling summary"
    ]
  },
  {
    "objectID": "book_sections/uncertainty.html",
    "href": "book_sections/uncertainty.html",
    "title": "12. Uncertainty",
    "section": "",
    "text": "Review: We estimate parameters from samples\nIn the real world we have a small number (usually one) of sample(s) and we want to learn about the population. This is a major challenge of statistics.",
    "crumbs": [
      "12. Uncertainty"
    ]
  },
  {
    "objectID": "book_sections/uncertainty.html#review-we-estimate-parameters-from-samples",
    "href": "book_sections/uncertainty.html#review-we-estimate-parameters-from-samples",
    "title": "12. Uncertainty",
    "section": "",
    "text": "In the previous chapter we sampled from a population many times to build a sampling distribution. But, if we had data for an entire population, we would know population parameters, so there would be no reason to calculate estimates from a sample.\n\n\n\n\nReview: Populations have parameters\nWe conceptualize populations as truth with true parameters that are “out there in the world”.\nSampling involves chance: Because (a good) sample is taken from a population at random, a sample estimate is influenced by chance (aka sampling error).",
    "crumbs": [
      "12. Uncertainty"
    ]
  },
  {
    "objectID": "book_sections/uncertainty.html#review-the-sampling-distribution",
    "href": "book_sections/uncertainty.html#review-the-sampling-distribution",
    "title": "12. Uncertainty",
    "section": "Review: The sampling distribution",
    "text": "Review: The sampling distribution\nThe sampling distribution – a histogram of sample estimates we would get by repeatedly sampling from a population – allows us to think about the chance deviation between a sample estimate and population parameter induced by sampling error.",
    "crumbs": [
      "12. Uncertainty"
    ]
  },
  {
    "objectID": "book_sections/uncertainty.html#estimation-with-uncertainty",
    "href": "book_sections/uncertainty.html#estimation-with-uncertainty",
    "title": "12. Uncertainty",
    "section": "Estimation with uncertainty",
    "text": "Estimation with uncertainty\nBecause estimates from samples take their values by chance, it is irresponsible and misleading to present an estimate without describing our uncertainty in it, by reporting the standard error or other measures of uncertainty.\n\nAfter this chapter you will be able to quantify uncertainty. From now on I WILL NOT LET YOU report point estimates without a measure of uncertainty. Point estimates without uncertainty are difficult to interpret.\n\n\nReview: The standard error\nThe standard error quantifies the uncertainty in our estimate due to sampling error as the standard deviation of the sampling distribution.\n\nReflect on the sentence above. It’s full of stats words and concepts that are the foundation of what we are doing here.\n\nWould this have made sense to you before the term?\n\nDoes it make sense now? If not, take a moment, talk to a friend, a chatbot, a professor or TA, whatever you need. This is important stuff and should not be glossed over.",
    "crumbs": [
      "12. Uncertainty"
    ]
  },
  {
    "objectID": "book_sections/uncertainty.html#generating-a-sampling-distribution",
    "href": "book_sections/uncertainty.html#generating-a-sampling-distribution",
    "title": "12. Uncertainty",
    "section": "Generating a sampling distribution",
    "text": "Generating a sampling distribution\nQuestion: We usually have one sample, not a population, so how do we generate a sampling distribution?\nAnswer: With our imagination!!! (Figure 1).\n\n\n\n\n\n\n\n\nFigure 1: We use our imagination to build a sampling distribution by math, simulation, or bootstrapping.\n\n\n\n\n\nWhat tools can our imagination access?\n\nWe can use math tricks that allow us to connect the variability in our sample to the uncertainty in our estimate.\n\nWe can simulate the process that we think generated our data.\n\nWe can resample from our sample by bootstrapping (see the rest of the chapter!).\n\nDon’t worry about the first two too much – we revisit them throughout the course. For now, just know that whenever you see this, we are imagining an appropriate sampling distribution. Here we focus on bootstrapping.\n\nHere we focus on estimating uncertainty first by bootstrapping. This is not the only way to estimate uncertainty.\nLater in the book, we use standard sampling distributions (e.g. the t-distribution) to estimate uncertainty.\nThis progression differs from many texts, which introduce bootstrapping as a “special topic” later in the book. I switch this order because:\n\nI think it is better pedagogy. I, for one, have an easier time conceptualizing the idea of sampling and uncertainty when these ideas are connected to actual sampling, rather than mathematical formulas.\nIn many cases, bootstrapped estimates of uncertainty are more robust to violations of assumptions than approaches using mathematical formulas (but see the final section).\nWe can often bootstrap when there is not a mathematical approach. For example, branches on phylogenetic trees often have “bootstrap support values” corresponding to the proportion of bootstrap replicates (in which variable loci are resampled with replacement) in which that particular branch appears.\n\nThese arguments are made very clearly by my colleague John Fieberg in his fantastic paper: “Resampling-based methods for biologists” (Fieberg et al. (2020)).\n\n\n\n\n\nFieberg, J. R., Vitense, K., & Johnson, D. H. (2020). Resampling-based methods for biologists. PeerJ, 8, e9089. https://doi.org/10.7717/peerj.9089",
    "crumbs": [
      "12. Uncertainty"
    ]
  },
  {
    "objectID": "book_sections/uncertainty/bootstrap.html",
    "href": "book_sections/uncertainty/bootstrap.html",
    "title": "• 12. Bootstrap",
    "section": "",
    "text": "Introducing the bootstrap\nA major question in evolution is how often to recently diverged species hybridize. If they almost never hybridize we don’t need to consider how gene flow impacts the process of speciation. If hybridization is frequent, we must wrestle with how populations can maintain their divergence despite ongoing hybridization.\nHow frequent is hybridization? It differs by species-pair, field site, individual phenotypes etc. For example \\(\\approx 15\\%\\) of seeds on Clarkia xantiana ssp parviflora RILs plants at site GC were actually hybrids. This estimate comes from averaging across all individuals in our sample. We know by now that we could have sampled other individuals from the population, and we must quantify uncertainty in this estimate.\nIf you don’t know anything about statistics, learn the bootstrap. Seriously, think of it as a “statistical pain reliever”!! My new favorite term for the approach that underlies almost everything I do... https://t.co/jCHozq80oM— Ryan Hernandez (@rdhernand) March 9, 2019\nWhen we take a statistical sample from a population, we do so without replacement. That, is we do not select the same individual twice – rather we sample n distinct individuals. This is how we get our sample. Conceptually we build sampling distributions by repeating this amny times (but of course in the real world we usually only have one sample).\nBootstrapping allows us to approximate the sampling distribution by choosing n individuals from our sample with replacement (i.e. we might pick the same sample more than once) as follows:\nAfter completing this you have one bootstrap replicate! We simply repeat this a bunch of times to get the bootstrap distribution.\nLet’s get started by generating one bootstrap replicate from our GC data! As in the sampling chapter, we use dplyr’s slice_sample() function, but this time we set prop (it defaults to our sample size) and we do need to specify replace = TRUE to sample with replacement:\nboot_rep1 &lt;- slice_sample(gc_rils, prop = 1, replace = TRUE)\nWe can now estimate the mean proportion hybrid from this first bootstrap replicate:\nboot_rep1 |&gt;\n    summarise(mean(prop_hybrid),\n              sample_size = n())\n\n# A tibble: 1 × 2\n  `mean(prop_hybrid)` sample_size\n                &lt;dbl&gt;       &lt;int&gt;\n1               0.138         101\nThis new mean of 0.136 is different from our original 0.151. Figure 2 shows that in this bootstrap replicate we happened to only select one of the three moms with the greatest proportion hybrid seed. This difference is the essence of sampling variability. The bootstrap allows us to recreate this variability computationally to see how much our estimate might have changed by chance.\nBy repeating this process thousands of times, we can see the full range of estimates we might have gotten, which is the key to measuring our uncertainty.\nCode for making the histogram below\nlibrary(patchwork)\nplot_count &lt;- ggplot(boot_rep1, aes(x = as.numeric(id), fill = id))+\n  geom_bar(show.legend = FALSE)+\n  labs(title = \"Representation of moms in bootstrap\",x = \"mom id\", y = \"# of occurrences\")+\n  theme(axis.title.x = element_text(size = 14),\n        axis.text.x = element_text(size = 14),\n        title =  element_text(size = 14),\n        axis.title.y = element_text(size = 14),\n        axis.text.y = element_text(size = 14))\n\nplot_hist &lt;- ggplot(boot_rep1 , aes(x = prop_hybrid,fill = id))+\n  geom_histogram(breaks = seq(0,1,length.out=10), \n                 color = \"white\",show.legend = FALSE)+\n  labs(title = \"Histogram of proportion hybrid in the first bootstrap\",x = \"Proportion hybrid (by mom)\")+\n  theme(axis.title.x = element_text(size = 14),\n        axis.text.x = element_text(size = 14),\n        title =  element_text(size = 14),\n        axis.title.y = element_blank(),\n        axis.text.y = element_blank())\n\nplot_count + plot_hist  + plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\n\n\nFigure 2: A visualization of a single bootstrap replicate derived from the original sample. (A) The result of sampling with replacement – The y-axis represents the number of times each original mother (identified by mom id and a unique color) was selected for the bootstrap sample. Many mothers from the original data were not selected, while others were selected two or more times. (B) A histogram of the proportion hybrid variable for this new bootstrap sample. The colors correspond to the individuals in Panel A, illustrating the composition of the new sample. If a single color is taller, it’s because that specific mother was selected multiple times. This visualizes how some mothers (colors) contribute multiple times to the overall bootstrap sample, which is the key feature of sampling with replacement.",
    "crumbs": [
      "12. Uncertainty",
      "• 12. Bootstrap"
    ]
  },
  {
    "objectID": "book_sections/uncertainty/bootstrap.html#introducing-the-bootstrap",
    "href": "book_sections/uncertainty/bootstrap.html#introducing-the-bootstrap",
    "title": "• 12. Bootstrap",
    "section": "",
    "text": "Pick an observation from your sample at random.\n\nNote its value.\n\nPut it back in the pool.\n\nGo back to (1) until you have as many resampled observations as initial observations.\nCalculate an estimate from this collection of observations.",
    "crumbs": [
      "12. Uncertainty",
      "• 12. Bootstrap"
    ]
  },
  {
    "objectID": "book_sections/uncertainty/bootstrap.html#building-a-bootstrap-distribution",
    "href": "book_sections/uncertainty/bootstrap.html#building-a-bootstrap-distribution",
    "title": "• 12. Bootstrap",
    "section": "Building a bootstrap distribution",
    "text": "Building a bootstrap distribution\nTo build a bootstrap distribution, we (not so) simply repeat this process many times. Figure 3 provides an animation of how we build up a bootstrap distribution.\n\n\n\n\n\n\n\n\nFigure 3: An animation illustrating the bootstrap procedure for estimating uncertainty. The left panel loops through individual bootstrap samples. The histogram shows the data for the current sample, with colors indicating which ‘mom’ was selected, the height of which shows how many times the mom was sampled. The blue dashed line marks this bootstrap sample’s mean. The right panel shows the mean from the left panel added as a single blue dot to the distribution on the right. This panel is cumulative, showing the distribution of all bootstrap sample means calculated so far. In both panels the red dashed line represents the mean of the original sample, providing a constant point of reference.\n\n\n\n\n\nThe code to do this is a bit tricky – in fact I will show you an R package to make this easier in the next subsection – so there is no need to carefully follow this code. But here’s one way to build up a bootstrap distribution:\n\nboot_dist &lt;- lapply(1:5000,function(i){\n    gc_rils |&gt;\n        select(prop_hybrid)|&gt;\n        slice_sample(replace = TRUE, prop = 1) |&gt;\n        summarize(est_prop_hybrid = mean(prop_hybrid))})|&gt;\n    bind_rows()\n\n\n\nlapply() is one of R’s ways of just doing the same thing a lot of times.\nNow that we have the bootstrap distribution we can visualize it! Figure 4 shows the bootstrap distribution as a histogram. We can see that estimates are centered around our sample mean of 0.15, but that by chance, some bootstrap replicates are as low as 0.08 or as high as 0.24. This spread is the uncertainty we were looking for, made visible. Now we can move from visualizing it to summarizing it with a single number: the standard error.\n\n\nCode for making a histogram of the bootstrap distribution\nboot_dist |&gt;\n  ggplot(aes(x = est_prop_hybrid))+\n  geom_histogram(bins = 50, color= \"white\")+\n  labs(title = \"The bootstrap distribution of proportion hybrids at GC\",\n       x = \"Mean proportion hybrid (sample estimate)\")+\n  theme(axis.title.x = element_text(size = 14),\n        axis.text.x = element_text(size = 14),\n        title =  element_text(size = 14),\n        axis.title.y = element_blank(),\n        axis.text.y = element_blank())+\n  geom_vline(xintercept = 0.151, color = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nFigure 4: The bootstrap distribution for the mean proportion of hybrids. This histogram was generated by repeatedly resampling from the original data, calculating the mean of each new bootstrap sample, and plotting those means. The resulting distribution is approximately normal and is centered on the mean from the original sample (indicated by the red dashed line at 0.151). The spread of this distribution is a measure of the uncertainty in that original estimate.\n\n\n\n\n\n\nQuantifying uncertainty: The bootstrap standard error\nWe can simply calculate the bootstrap standard error as the standard deviation of the bootstrap distribution.\n\nboot_dist |&gt;\n  summarise(se = sd(est_prop_hybrid))\n\n# A tibble: 1 × 1\n      se\n   &lt;dbl&gt;\n1 0.0237\n\n\nThe bootstrap standard error of 0.024 means that the average distance between an estimate from the bootstrap distribution and the actual sample estimate is roughly 0.024.",
    "crumbs": [
      "12. Uncertainty",
      "• 12. Bootstrap"
    ]
  },
  {
    "objectID": "book_sections/uncertainty/ci.html",
    "href": "book_sections/uncertainty/ci.html",
    "title": "• 12. Confidence Intervals",
    "section": "",
    "text": "Understanding Confidence Intervals\nConfidence intervals offer a different way to describe uncertainty than the standard error. While the SE is a single number representing expected error (e.g. a standard error of 2.37% percent hybridization around our estimated 15% hybridization rate for parviflora RILs in GC), a Confidence Interval provides a range of plausible values for the true population parameter.\nConfidence intervals are a bit tricky to think and talk about. I think about a confidence interval as a net that does or does not catch the true population parameter. If we set a 95% confidence level, as is statistical tradition, we expect ninety five of every one hundred 95% confidence intervals to capture the true population parameter.",
    "crumbs": [
      "12. Uncertainty",
      "• 12. Confidence Intervals"
    ]
  },
  {
    "objectID": "book_sections/uncertainty/ci.html#understanding-confidence-intervals",
    "href": "book_sections/uncertainty/ci.html#understanding-confidence-intervals",
    "title": "• 12. Confidence Intervals",
    "section": "",
    "text": "Choosing a confidence level: The choice of a 95% confidence interval is an arbitrary statistical convention. It is usually best to follow conventions so that people don’t look at you funny. But you may want to break with convention at times. When setting a confidence level there is a trade-off between certainty and precision – a 99% confidence interval gives you greater certainty that the true parameter has been captured but yields a wider, less precise range, while a 90% interval is narrower and more precise but carries a greater risk of having missed the true value.",
    "crumbs": [
      "12. Uncertainty",
      "• 12. Confidence Intervals"
    ]
  },
  {
    "objectID": "book_sections/uncertainty/ci.html#this-webapp-from-whitlock2020-helps-make-this-idea-of-a-confidence-interval-more-concrete.-here-they-produce-a-sample-of-size-n-from-a-population-with-a-true-mean-of-mu-and-population-standard-deviation-of-sigma.-play-with-this-app-to-see-how-these-variables-change-our-expected-confidence-intervals.",
    "href": "book_sections/uncertainty/ci.html#this-webapp-from-whitlock2020-helps-make-this-idea-of-a-confidence-interval-more-concrete.-here-they-produce-a-sample-of-size-n-from-a-population-with-a-true-mean-of-mu-and-population-standard-deviation-of-sigma.-play-with-this-app-to-see-how-these-variables-change-our-expected-confidence-intervals.",
    "title": "• 12. Confidence Intervals",
    "section": "This webapp from Whitlock & Schluter (2020) helps make this idea of a confidence interval more concrete. Here they produce a sample of size \\(n\\) from a population with a true mean of \\(\\mu\\), and population standard deviation of \\(\\sigma\\). Play with this app to see how these variables change our expected confidence intervals.",
    "text": "This webapp from Whitlock & Schluter (2020) helps make this idea of a confidence interval more concrete. Here they produce a sample of size \\(n\\) from a population with a true mean of \\(\\mu\\), and population standard deviation of \\(\\sigma\\). Play with this app to see how these variables change our expected confidence intervals.\n\n\n\n\n\nBecause each confidence interval does or does not capture the true population parameter, it is wrong to say that a given confidence interval has a given chance of catching the parameter. The probability is in the sampling process, not in the parameter itself, which is fixed. Before we take a sample, we have a 95% chance of ‘catching’ that parameter in our net. After we’ve taken the sample and made our interval, that interval either contains the parameter or it does not. Think of it like a coin that has already been flipped but is hidden under a cup. The outcome is already set - it’s either heads or tails. You can’t say there is a 50% chance it’s heads; your statement that ‘it is heads’ is simply either right or wrong. The 95% confidence level applies to the process of creating nets, not to any single net after it has been cast.\nThese concepts are tough. This video from Crash course statistics can be very helpful. Note that it introduces some mathematical formulas we will cover later in the book, so feel free to focus on the core concepts for now.",
    "crumbs": [
      "12. Uncertainty",
      "• 12. Confidence Intervals"
    ]
  },
  {
    "objectID": "book_sections/uncertainty/ci.html#calculating-bootstrap-confidence-intervals",
    "href": "book_sections/uncertainty/ci.html#calculating-bootstrap-confidence-intervals",
    "title": "• 12. Confidence Intervals",
    "section": "Calculating Bootstrap Confidence Intervals",
    "text": "Calculating Bootstrap Confidence Intervals\nCalculating a bootstrap confidence interval from the bootstrap distribution is pretty straightforward. We first decide on our desired confidence level (0.95 is standard), and we will call one minus the confidence level \\(\\alpha\\) (so for a .95 confidence level, \\(\\alpha\\) = 0.05$). We then find the values on the border of the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) by combining the summarize() and quantile() functions:\n\nalpha &lt;- .05\nboot_dist |&gt;\n  summarise(lower_cl = quantile(est_prop_hybrid, probs = alpha/2),\n            upper_cl = quantile(est_prop_hybrid, probs = 1-alpha/2))\n\n# A tibble: 1 × 2\n  lower_cl upper_cl\n     &lt;dbl&gt;    &lt;dbl&gt;\n1    0.108    0.199\n\n\nThis result shows that although our estimated hybridization rate is 15%, values between 10% and 20% are plausible. Figure 1 visually highlights the 95% bootstrap confidence interval from the bootstrap distribution.\n\n\nSay no to bright lines! We found a 95% CI between 0.108 and 0.199. Does this mean that hybridization rate of 0.198 is plausible and 0.200 is implausible? Of course not. Confidence intervals help guide our thinking – but they should not be treated as rigid bounds.\n\n\nCode for making a very involved histogram of the bootstrap distribution and the 95% condidence interval.\nboot_dist &lt;- read_csv(\"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/gc_hyb_prop_boot.csv\")\n\nboot_cis &lt;- boot_dist|&gt;\n  reframe(ci = quantile(est_prop_hybrid,probs = c(0.025, 0.975)))\n\nboot_dist &lt;- boot_dist |&gt; \n  mutate(extreme = (est_prop_hybrid &lt; (boot_cis |&gt; unlist())[1]) |\n                   (est_prop_hybrid &gt; (boot_cis |&gt; unlist())[2])) \n\nboot_dist |&gt;\n  ggplot(aes(x = est_prop_hybrid, alpha = extreme, fill = extreme))+\n  geom_histogram(bins = 50, color= \"white\")+\n  labs(title = \"95% bootstrap confidence interval for proportion hybrids at GC\",\n       x = \"Mean proportion hybrid (sample estimate)\",\n       fill = \"2.5% tail\",\n       alpha = \"2.5% tail\")+\n  theme_light()+\n  guides(alpha  = guide_legend(position = \"inside\"),\n         fill  = guide_legend(position = \"inside\"))+\n  theme(axis.title.x = element_text(size = 14),\n        axis.text.x = element_text(size = 14),\n        title =  element_text(size = 14),\n        axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        legend.position = c(.8,.8),\n        legend.box.background = element_rect(colour = \"black\"))+\n  geom_vline(data =  boot_dist|&gt;\n               reframe(ci = quantile(est_prop_hybrid,\n                       probs = c(0.025, 0.975))),\n             aes(xintercept = ci), \n             color = \"purple\", \n             linewidth = 1, \n             lty = 2)+\n  scale_fill_manual(values = c(\"cornflowerblue\",\"firebrick\"))+\n  scale_alpha_manual(values = c(.2,1))\n\n\n\n\n\n\n\n\nFigure 1: A histogram showing the bootstrap distribution of the estimated proportion of hybrid offspring at site GC. The two vertical purple dashed lines show the bounds of the 95% bootstrap confidence interval. Values within these lines are filled in light blue. Estimates in the extreme 2.5% tails outside of these lines are in dark red.\n\n\n\n\n\n\n\nI introduce the idea of a confidence interval from the quantiles of the bootstrap distribution. As noted elsewhere, we can also use math tricks to estimate a confidence interval. For example:\n\nA 95% confidence interval is roughly equal to the estimated mean plus or minus two standard errors. (you may have seen this in the youtube video above).\n\nYou do not need to know this yet, but know that these approaches usually yield very similar confidence intervals.",
    "crumbs": [
      "12. Uncertainty",
      "• 12. Confidence Intervals"
    ]
  },
  {
    "objectID": "book_sections/uncertainty/ci.html#visualizing-confidence-intervals",
    "href": "book_sections/uncertainty/ci.html#visualizing-confidence-intervals",
    "title": "• 12. Confidence Intervals",
    "section": "Visualizing confidence intervals",
    "text": "Visualizing confidence intervals\nWe have previously discussed the importance of presenting honest, clear, and transparent plots. This includes both showing your data and highlighting patterns. It is therefore best practice to include both means and a confidence interval in your plots. You can do this with ggplot’s stat_summary() function.\n\n\nNOTE stat_summary() requires the Hmisc package so you will likely need to install and load it.\nWe don’t even need to bootstrap the data ourselves! Supplying the argument fun.data = \"mean_cl_boot\" tells ggplot to perform the entire bootstrap procedure for us (just as we did manually), calculate the mean and confidence interval, and add them to the plot! Figure 2 shows that the bootstrap CI we calculated roughly matches the one that R calculated. Figure Figure 3 displays the output of the code below which adds confidence intervals to all groups.\n\n\nLoading data\nril_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_rils.csv\"\nril_data &lt;- readr::read_csv(ril_link)|&gt;\n  filter(!is.na(location))\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: A visual comparison of two methods for calculating a 95% bootstrap confidence interval for the GC site. The black point and vertical error bar were generated automatically using ggplot2’s stat_summary() function. The two horizontal red lines represent the bounds of the confidence interval calculated manually from our bootstrap distribution. The close alignment of the red lines with the ends of the black error bar confirms that both methods produce nearly identical results.\n\n\n\n\n\nggplot(ril_data, aes(x = location, y =prop_hybrid, color = location))+\n  geom_jitter(height = 0, width = .2, size = 3, alpha = .6)+\n  # THIS IS WHERE WE ADD BOOTSRAP CONFIDENCE INTERVALS\n  stat_summary(fun.data = \"mean_cl_boot\", colour = \"black\")+\n  # YAY WE JUST ADDED BOOTSRAP CONFIDENCE INTERVALS\n  theme(legend.position = \"none\",\n        axis.text = element_text(size = 14),\n        axis.title = element_text(size = 16))\n\n\n\n\n\n\n\nFigure 3: Raw data with means and bootstrap confidence intervals for the proportion of hybrid offspring at four different locations. For each location, the black point represents the mean, and the vertical black error bar represents the 95% bootstrap confidence interval.",
    "crumbs": [
      "12. Uncertainty",
      "• 12. Confidence Intervals"
    ]
  },
  {
    "objectID": "book_sections/uncertainty/ci.html#what-influences-the-width-of-a-confidence-interval",
    "href": "book_sections/uncertainty/ci.html#what-influences-the-width-of-a-confidence-interval",
    "title": "• 12. Confidence Intervals",
    "section": "What influences the width of a confidence interval?",
    "text": "What influences the width of a confidence interval?\nThree main factors determine the width, or precision, of a confidence interval – the sample size, the variability in the data, and the confidence level:\n\nConfidence intervals get narrower as the sample size gets bigger.\n\nConfidence intervals get wider as the variability in the data gets bigger.\nConfidence intervals get wider as we increase the confidence level (e.g., from 90% to 99%).\n\n\n\n\n\nWhitlock, M. C., & Schluter, D. (2020). The analysis of biological data (Third). Macmillan.",
    "crumbs": [
      "12. Uncertainty",
      "• 12. Confidence Intervals"
    ]
  },
  {
    "objectID": "book_sections/uncertainty/boot_infer.html",
    "href": "book_sections/uncertainty/boot_infer.html",
    "title": "• 12. Bootstrapping w/infer",
    "section": "",
    "text": "Bootstrapping the mean with infer\nWe previously generated a bootstrap distribution with slice_sample(). But this required some work on our part. As with much in R there are specialized packages that bootstrap for us. I like these packages because they make it easier to do, harder to mess up, and are often coded more efficiently than we can code.\nHere, I show you how to bootstrap with the infer package package. While our first example of bootstrapping a single mean will look similar to our manual approach, you will quickly see how infer’s consistent grammar makes bootstrapping more complex statistics (e.g. a difference in means or a regression slope) surprisingly straightforward.\nTo bootstrap with infer, we\nThe code below shows how to conduct the first two steps – it makes a HUGE tibble – with reps (5000 in this example) replicates. I only show a subset of these replicates here, because the table is too large to be hosted by my server.\nlibrary(infer)\nboot_phyb &lt;-  gc_rils %&gt;%\n  specify(response = prop_hybrid)    |&gt;     # Specify variable of interest\n  generate(reps = 5000, type = \"bootstrap\") # Generate 5000 bootstrap resamples\nNext we summarize each bootstrap replicate and calculate summary statistics as before. Reassuringly, the answer is basically the same as we have found previously.\nalpha &lt;- 0.05\n\nboot_phyb |&gt;\n  summarise(est_prop_hybrid = mean(prop_hybrid)) |&gt;\n  summarise(standard_error  = sd(est_prop_hybrid ),\n            lower_cl = quantile(est_prop_hybrid, probs = alpha/2),\n            upper_cl = quantile(est_prop_hybrid, probs = 1-alpha/2))\n\n# A tibble: 1 × 3\n  standard_error lower_cl upper_cl\n           &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1         0.0235    0.108    0.199",
    "crumbs": [
      "12. Uncertainty",
      "• 12. Bootstrapping w/`infer`"
    ]
  },
  {
    "objectID": "book_sections/uncertainty/boot_infer.html#bootstrapping-the-mean-with-infer",
    "href": "book_sections/uncertainty/boot_infer.html#bootstrapping-the-mean-with-infer",
    "title": "• 12. Bootstrapping w/infer",
    "section": "",
    "text": "specify() the response variable.\n\ngenerate() the bootstrap distribution.\n\nsummarise() results for each bootstrap replicate.\n\nCalculate estimates of uncertainty\n\nwith sd() to find the bootstrap standard error.\n\nand quantile() to find the confidence interval.\n\n\n\n\n\n\n\n\nNote: The infer package has handy functions to do these steps too, as I will show you soon. For now I show you how to do it ‘manually’ to remove the mystery of a magic function.\n\n\n\nShow how the entire pipeling with infer tools\n\n\nlibrary(infer)\nboot_dist_phyb &lt;-  gc_rils %&gt;%\n  specify(response = prop_hybrid)           |&gt;  # Specify variable of interest\n  generate(reps = 5000, type = \"bootstrap\") |&gt;  # Generate 5000 bootstrap resamples\n  calculate(stat = \"mean\")\n\nFind standard error\n\nsummarise(boot_dist_phyb , se = sd(stat))\n\n# A tibble: 1 × 1\n      se\n   &lt;dbl&gt;\n1 0.0234\n\n\nFind 95% confidence interval\n\nget_ci(boot_dist_phyb, level = .95, type = \"percentile\")\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1    0.106    0.198\n\n\n\n\nBecause bootstrapping involves chance sampling bootstrap estimates of uncertainty will vary slightly each time. We say a bit more about this in the next subsection.",
    "crumbs": [
      "12. Uncertainty",
      "• 12. Bootstrapping w/`infer`"
    ]
  },
  {
    "objectID": "book_sections/uncertainty/boot_infer.html#bootstrapping-is-not-just-for-means",
    "href": "book_sections/uncertainty/boot_infer.html#bootstrapping-is-not-just-for-means",
    "title": "• 12. Bootstrapping w/infer",
    "section": "Bootstrapping is not just for means!!",
    "text": "Bootstrapping is not just for means!!\nRecall one of our major questions in Clarkia speciation is how floral traits like petal color or petal area impact the propensity to set hybrid seed. We can resample with replacement to estimate uncertainty in all sorts of estimates such as these. Below we’ll see how the infer package makes it easy to do so.\n\nDifference in means\n\n\ngc_rils                   |&gt; \n group_by(petal_color)    |&gt;\n summarise(prop_hybrid =\n           mean(prop_hybrid))\n\n# A tibble: 3 × 2\n  petal_color prop_hybrid\n  &lt;chr&gt;             &lt;dbl&gt;\n1 pink             0.260 \n2 white            0.0322\n3 &lt;NA&gt;             0.112 \n\nNearly 26% of seeds on pink-petaled parviflora RILS at GC are hybrids, while only 3.22% of seeds on white-petaled parviflora RILS at GC are hybrids. So, we estimate a 22.78% difference (or 0.2278) in the proportion of hybrid seeds by petal color morph. However, we know that the estimates going into this are associated with uncertainty. We can even quantify uncertainty in the mean of each group as before (see Figure 1). But how do we estimate our uncertainty in the difference between these means?\nTo do this, we must:\n\n\n\n\n\n\n\n\n\nFigure 1: The proportion of hybrid seed set for pink- and white-petaled flowers at GC. Each point represents an individual plant. Overlaid in bright green is the mean and the 95% bootstrap confidence interval for each petal color morph.\n\n\n\n\n\nResample white and pink-petaled flower morphs with replacement.\nCalculate the mean hybrid seed set for each morph in this bootstrap replicate.\nFind the difference in hybrid seed set by petal-color morph in this bootstrap replicate.\nRepeat this a bunch of times to find the bootstrap distribution.\nQuantify uncertainty, for example, as the bootstrap standard error and the bootstrap 95% confidence interval.\n\nI could code this myself, but it’s tedious. infer makes it straightforward. The code below uses the formula syntax prop_hybrid ~ petal_color (as we did in our linear model setion) inside specify() to indicate our interest in this relationship. We then tell R to [generate()]((https://infer.netlify.app/reference/generate) the bootstrap distribution and calculate() to find the \"diff in means\" for each bootstrap replicate:\n\nlibrary(infer)\nboot_diffs &lt;- gc_rils                   |&gt;\n  filter(!is.na(petal_color))           |&gt;\n  specify(prop_hybrid ~ petal_color)    |&gt;                      # The linear model we're looking into\n  generate(reps = 5000, type = \"bootstrap\") |&gt;                   # Bootstrapping\n  calculate(stat = \"diff in means\", order = c(\"pink\", \"white\"))  # Summarizing the bootstrap replicate\n\nWe can now visualize this bootstrap distribution of differences as a histogram to see the range of plausible values. Figure Figure 2 shows this distribution and its 95% confidence interval.\n\n\nCode for plotting the bootstrap distribution of hybrid seed set by petal color with 95% CI\nboot_diffs_cis &lt;- boot_diffs |&gt;\n  reframe(ci = quantile(stat,probs = c(0.025, 0.975)))\n\nboot_diffs_2 &lt;- boot_diffs |&gt; \n  mutate(extreme = (stat &lt; (boot_diffs_cis |&gt; unlist())[1]) |\n                   (stat &gt; (boot_diffs_cis |&gt; unlist())[2])) \n\nboot_diffs_2  |&gt;\n  ggplot(aes(x = stat, alpha = extreme, fill = extreme))+\n  geom_histogram(bins = 50, color= \"white\")+\n  labs(title = \"95% bootstrap confidence interval for difference in\\nproportion hybrids of pink and white petal-color morphs at GC\",\n       x = \"Mean difference in hybrid proportion (pink - white)\",\n       fill = \"2.5% tail\",\n       alpha = \"2.5% tail\")+\n  theme_light()+\n  guides(alpha  = guide_legend(position = \"inside\"),\n         fill  = guide_legend(position = \"inside\"))+\n  theme(axis.title.x = element_text(size = 14),\n        axis.text.x = element_text(size = 14),\n        title =  element_text(size = 14),\n        axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        legend.position = c(.2,.8),\n        legend.box.background = element_rect(colour = \"black\"))+\n  geom_vline(data =  boot_diffs_2 |&gt;\n               reframe(ci = quantile(stat,\n                       probs = c(0.025, 0.975))),\n             aes(xintercept = ci), \n             color = \"purple\", \n             linewidth = 1, \n             lty = 2)+\n  scale_fill_manual(values = c(\"cornflowerblue\",\"firebrick\"))+\n  scale_alpha_manual(values = c(.2,1))+\n  geom_vline(xintercept = 0)\n\n\n\n\n\n\n\n\nFigure 2: The bootstrap distribution of the difference in mean proportion of hybrid seeds between pink and white morphs. The distribution is centered far from zero, and the 95% confidence interval (bounded by the purple dashed lines) does not overlap with the solid black line at zero.\n\n\n\n\n\nWith our bootstrap distribution of differences in hand, we can now precisely quantify our uncertainty. We can find the bootstrap standard error with the sd() function and use infer’s convenient get_ci() function to find the 95% confidence interval.\n\n\nboot_diffs |&gt;\n  summarise(se = sd(stat))\n\n# A tibble: 1 × 1\n      se\n   &lt;dbl&gt;\n1 0.0426\n\n\nboot_diffs |&gt;\n  get_ci(level = 0.95, type = \"percentile\")\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1    0.147    0.314\n\n\nThus, we conclude that a reasonable estimate of the difference in the proportion of hybrid seed lies between 0.147 and 0.314. Importantly, this confidence interval excludes zero so we are pretty sure that pink RILs do set more hybrid seed than white ones.\n\n\nSlope\nWe can similarly use infer to find the bootstrap distribution of the slope. For example, we can quantify the uncertainty in our estimate of a 0.00566 increase in proportion hybrid seed with every additional squared mm in petal area (Figure 3) as shown in the code below.\n\n\n# Estimate the slope\n# As covariance / variance\ngc_rils |&gt;\n  filter(!is.na(petal_area_mm)) |&gt;\n  summarise(slope = cov(petal_area_mm, prop_hybrid) / var(petal_area_mm))\n\n# A tibble: 1 × 1\n    slope\n    &lt;dbl&gt;\n1 0.00566\n\n\nboot_slopes &lt;- gc_rils                      |&gt;\n  filter(!is.na(petal_area_mm))             |&gt;\n  specify(prop_hybrid ~ petal_area_mm)      |&gt;        # The linear model we're looking into\n  generate(reps = 5000, type = \"bootstrap\") |&gt;        # Bootstrapping\n  calculate(stat = \"slope\")                           # Summarizing the bootstrap replicate\n\nsummarise(boot_slopes , se = sd(stat))                # find SE\n\n# A tibble: 1 × 1\n       se\n    &lt;dbl&gt;\n1 0.00178\n\nget_ci(boot_slopes,level = 0.95, type = \"percentile\") # 95% CI\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1  0.00198  0.00890\n\n\n\n\nCode for plotting the relationship between petal area and proportion hybrid seed.\nggplot( gc_rils, aes(x = petal_area_mm, y = prop_hybrid))+\n  geom_point(size = 6,alpha = .7)+\n  geom_smooth(method = \"lm\",linewidth = 1.6)+\n  labs( x= \"Petal area (in square mm)\", y = \"proportion hybrid\")+\n  theme(legend.position = \"none\",\n        axis.text = element_text(size  = 34),\n        axis.title = element_text(size = 30))\n\n\n\n\n\n\n\n\n\nFigure 3: The relationship between petal area (in mm²) and the proportion of hybrid seed. The blue line is the line of best fit from a linear model, and the gray shaded region represents the 95% confidence interval for this line.\n\n\n\n\nHere we conclude that a reasonable estimate of the increase in proportion hybrid seed for each unit increase in square mm of petal area lies between 0.002 and 0.009. Importantly, this confidence interval excludes zero, so we are pretty sure that the proportion of hybrid see increase with petal area as seen in Figure 3. For me this is super helpful, because without reflection, we could wrongly conclude that a slope of 0.00566 is unimpressive.\n\n\n… and more!\nThe infer package can calculate() the many stats including: “mean”, “median”, “sum”, “sd”, “prop”, “count”, “diff in means”, “diff in medians”, “diff in props”, “ratio of props”, “slope”, “odds ratio”, “ratio of means”, and “correlation”! This means we can quantify uncertainty in a bunch of the estimates we have introduced previously.",
    "crumbs": [
      "12. Uncertainty",
      "• 12. Bootstrapping w/`infer`"
    ]
  },
  {
    "objectID": "book_sections/uncertainty/boot_gotchas.html",
    "href": "book_sections/uncertainty/boot_gotchas.html",
    "title": "• 12. Gotchas",
    "section": "",
    "text": "Bootstrap gone wrong\nBootstrapping is great! It allows us to estimate uncertainty without relying on formulas or too many assumptions. But as with all stats tools, there are limits to the use of bootstrapping. So before moving onto the next section it is worth going over these limits and how to deal with them.",
    "crumbs": [
      "12. Uncertainty",
      "• 12. Gotchas"
    ]
  },
  {
    "objectID": "book_sections/uncertainty/boot_gotchas.html#bootstrap-gone-wrong",
    "href": "book_sections/uncertainty/boot_gotchas.html#bootstrap-gone-wrong",
    "title": "• 12. Gotchas",
    "section": "",
    "text": "Bootstrapping itself is a sample\n\n\n\n\n\n\n\n\n\nFigure 1: An xkcd comic about uncertainty in our uncertainty. Rollover text: “…an effect size of 1.68 (95% CI: 1.56 (95% CI: 1.52 (95% CI: 1.504 (95% CI: 1.494 (95% CI: 1.488 (95% CI: 1.485 (95% CI: 1.482 (95% CI: 1.481 (95% CI: 1.4799 (95% CI: 1.4791 (95% CI: 1.4784…”. See explainxkcd for more discussion.\n\n\n\n\nBootstrapping uses sampling to approximate the sampling distribution. This means that every time we bootstrap we will get a different estimate of uncertainty - even if we have the same data going into the bootstrap. As with other forms of sampling, the more bootstrap replicates you make, the more stable your estimate of uncertainty is. However there is a trade-off between computational effort and the precision of bootstrap estimates of uncertainty.\nHow many bootstrap replicates are enough? I don’t know, and it depends on your data. I usually start with 1000 bootstrap replicates and calculate the bootstrap standard error and confidence intervals. I repeat this a few times and see if my estimates are stable.\n\nIf my estimated uncertainty bounces around a lot I increase the number of replicates to five thousand and see if that helps. I keep increasing the number of replicates until my estimates of uncertainty are stable.\nIf my estimate is stable I’m happy.\nIf the bootstrapping is taking forever, I decrease the number of reps.\n\nWhy not make millions of bootstrap replicates? It will take a lot of time, and overtax your computer with very little reward.\n\n\n100 Bootstrap replicates: For me in this example, 100 bootstrap replicates leads to confidence intervals which are too variable to feel comfortable reporting.\n\n\n\n\n\nlower_ci\nupper_ci\n\n\n\n\n0.1070\n0.1946\n\n\n0.1069\n0.1880\n\n\n0.1091\n0.1910\n\n\n0.1133\n0.1887\n\n\n0.1083\n0.1896\n\n\n\n\n\n\n\n5000 Bootstrap replicates By contrast estimates from 5000 bootstraps seem stable enough for me (e.g. I would declare that the lower CI is 0.107 and all that close enough).\n\n\n\n\n\nlower_ci\nupper_ci\n\n\n\n\n0.1075\n0.1968\n\n\n0.1064\n0.1986\n\n\n0.1062\n0.1998\n\n\n0.1063\n0.1997\n\n\n0.1060\n0.2004\n\n\n\n\n\n\n\n\n\nBootstrapping is unreliable when n is small\nBootstraping works by approximating the sampling distribution by resampling your data with replacement. Thus it inherently assumes your sample is a good representation of the population. A very small sample is unlikely to capture the true shape of the population’s distribution.\nTo think about this, consider an extremely small sample of two. Here bootstrapping will provide three estimates:\n\nA quarter of the time we will sample individual one twice and their value will be the estimate of the mean.\nHalf the time we will sample both individuals so the mean of these two will be our estimate of the mean.\nA quarter of the time we will sample individual two twice and their value will be the estimate of the mean. None of these are good estimates.\n\nThe bootstrap distribution is similarly limited for somewhat larger sample sizes. For example, Figure 2 shows the data and associated bootstrap distributions for three different samples of size five. The first consists of moms making zero or few hybrid seeds, the second has a mix of moms with few and many hybrid seeds, and the third consists of moms with many hybrid seeds. For the first and third samples, the 95% CI (purple dashed line in Figure 2) fails to capture the actual estimated mean from our original sample (red line in Figure 2).\nAs a rule of thumb, a sample of twenty or more is required for a reliable bootstrap estimate of uncertainty. But there is wiggle room and nuance - when data are less variable and more bell shaped we can get away with a smaller sample, while more variable data with strange distributions require a larger sample size for reliable bootstrapping.\n\n\n\n\n\n\n\n\nFigure 2: Three different random samples of size five (top row) and their associated bootstrap distributions of the mean (bottom row). The bootstrap distributions are sparse and irregular due to the small sample size. Purple dashed lines show the 95% confidence interval for each bootstrap, and the red line shows the actual estimated sample mean of 0.151 from a sample of size 101.\n\n\n\n\n\n\n\nBootstrapping cannot fix bad sampling\nMore generally, resampling a poor sample will only give you a precise but wrong answer.\n\nIf data are non-independent, a simple bootstrap will not properly estimate uncertainty. Say we happened to sample intensely in a few sections of GC rather than sampling randomly across space. Within each section plants are likely to be more closely related genetically or to experience similar environmental influences etc which will make them more similar to one another. As such, our sample will not represent the variability of this population appropriately and we will be overconfident in our estimates.\nIf a sample is biased, a simple bootstrap will not properly estimate uncertainty. For instance, if you tended to sample more pink than white flowers because they were easier to see, the bootstrap would give you a precise estimate of a sample tilted toward more pink flowers than we see in the population. This will inflate our estimated probability of hybridization.",
    "crumbs": [
      "12. Uncertainty",
      "• 12. Gotchas"
    ]
  },
  {
    "objectID": "book_sections/uncertainty/boot_gotchas.html#there-are-other-ways-to-estimate-uncertainty",
    "href": "book_sections/uncertainty/boot_gotchas.html#there-are-other-ways-to-estimate-uncertainty",
    "title": "• 12. Gotchas",
    "section": "There are other ways to estimate uncertainty",
    "text": "There are other ways to estimate uncertainty\nPerhaps it’s my fault. I just love bootstrapping so much. But students sometimes finish this section thinking bootstrapping is the only way to estimate uncertainty. This is not true. While bootstrapping is very handy there are other – and more common approaches – to estimate uncertainty. These alternative approaches use common mathematical distributions to generate a sampling distribution and are therefore less computationally intense. We will encounter these later in the book.",
    "crumbs": [
      "12. Uncertainty",
      "• 12. Gotchas"
    ]
  },
  {
    "objectID": "book_sections/uncertainty/uncertainty_summary.html",
    "href": "book_sections/uncertainty/uncertainty_summary.html",
    "title": "• 12. Uncertainty summary",
    "section": "",
    "text": "Links to: Summary. Chatbot tutor. Questions. Glossary. R packages. R functions. More resources.",
    "crumbs": [
      "12. Uncertainty",
      "• 12. Uncertainty summary"
    ]
  },
  {
    "objectID": "book_sections/uncertainty/uncertainty_summary.html#uncertainty_summary_chapter-summary",
    "href": "book_sections/uncertainty/uncertainty_summary.html#uncertainty_summary_chapter-summary",
    "title": "• 12. Uncertainty summary",
    "section": "Chapter summary",
    "text": "Chapter summary\nIn the previous chapter, we saw that knowing the sampling distribution is the key to quantifying sampling error. However, we never have access to the true sampling distribution because that would require knowing the entire population—at which point we wouldn’t need to estimate uncertainty at all. Bootstrapping is a solves this problem by allowing us to approximate the sampling distribution using only our own sample. Bootstapping by repeatedly resampling our data with replacement to create thousands of new samples. The resulting bootstrap distribution of estimates can then be used to quantify uncertainty, for example by calculating the bootstrap standard error (the standard deviation of the bootstrap distribution) or by finding the X% confidence interval to provide a range of plausible values for the true population parameter.\n\n\n\n\n\n\n\n\nFigure 1: Understanding confidence intervals is tricky. This cartoon by Ellie Murray is the best explanation I have found to date. I copy and paste directly from her medium post “I’ve often heard students mis-use the target analogy to describe their confidence interval as if it were the fixed target, and the ”true” value was the arrow that may or may not land on the target 95% of the time…. But, a confidence interval isn’t fixed — the ”true” value is the part they should view as fixed. To avoid this misunderstanding, I like to explain confidence intervals with ring toss. The ”truth” is in a fixed place, and it’s the confidence interval ring that may or may not land where you want it to… A side benefit: this analogy can help students see why a 99% confidence interval would be wider than a 95% interval — the bigger the ring, the easier the game of ring toss!”. While we’re at it, Ishould mention that Ellie Murray co-host’s a fantastic podcast called “Casual inference”.\n\n\n\n\n\n\nChatbot tutor\n\nPlease interact with this custom chatbot (link here). I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you got what you needed from it.",
    "crumbs": [
      "12. Uncertainty",
      "• 12. Uncertainty summary"
    ]
  },
  {
    "objectID": "book_sections/uncertainty/uncertainty_summary.html#uncertainty_summary_practice-questions",
    "href": "book_sections/uncertainty/uncertainty_summary.html#uncertainty_summary_practice-questions",
    "title": "• 12. Uncertainty summary",
    "section": "Practice Questions",
    "text": "Practice Questions\nTry these questions! By using the R environment you can work without leaving this “book”. I even pre-loaded all the packages you need!\n\nQ1) The ___ is the key idea we use to think about uncertainty due to sampling error. Standard errorStandard deviationSampling distributionError functionBootstrap distribution.\n\n\nExplanation of Q1\n\nThe Sampling distribution is the foundation idea for thinking about uncertainty. It’s a the distribution sample estimates, and it’s what allows us to think about the nature of sampling error.\n\n\nWhy standard error is not quite right.\n\nIf you chose “Standard error”: You’re on the right track! The standard error is how we quantify or measure the uncertainty. However, the sampling distribution is the larger idea or conceptual framework that allows us to understand where that standard error comes from.\n\n\n\nWhy bootstrap distribution is not quite right.\n\n\nIf you chose “Bootstrap distribution”: This was a tricky one! The bootstrap distribution is a fantastic tool we use to approximate the sampling distribution when we only have one sample. The sampling distribution is the true, underlying theoretical idea that we are trying to estimate with the bootstrap.\n\n\n\n\nWhy bootstrap distribution is wrong.\n\nIf you chose “Standard deviation”: This is a very common point of confusion, as the two terms are closely related! A standard deviation measures the spread or variability of individual data points within your single sample. The standard error measures the variability of a sample estimate (like the sample mean) across many different potential samples (which is what the sampling distribution shows). So, standard deviation describes your data’s variability, while the standard error describes the uncertainty of your estimate due to sampling.\n\n\n\nError function is wrong.\n\nCome on. What are you even thinking.\n\n\n\nQ2) For real data, we can use the _, which we make by sampling  replacement to estimate with uncertainty. Bootstrap distribution, withBootstrap distribution, withoutSampling distribution, withoutSampling distribution, with\n\nQ3) Which of these estimates tend to get bigger as sample sizes got smaller? (there is more than one right answer. find them all). HINT: playing with this webapp can help. Standard errorStandard deviationMeanConfidence interval\n\n\nExplanation of Q3\n\nOur uncertainty decreases with increasing sample size.\nIf you chose “Standard deviation” or “Mean”: That’s a great thought, because estimates of the mean and standard deviation are definitely less precise with smaller samples! However, their values don’t systematically get bigger or smaller. A sample mean from a small sample is just as likely to be above the true population mean as it is below. Likewise, the standard deviation of your sample is your best guess for the spread of the whole population, and that guess doesn’t have a tendency to get bigger, just less reliable.\n\n\nQ4) You calculated a 95% confidence interval from a random sample. What is the probability that the confidence interval captured the true population parameter? (check all that apply). There is no probability about this, it either did or it did not95%It depends on the sample sizeIt depends on the sample standard deviation\n\n\nExplanation of Q4\n\nThe true population parameter is also a single, fixed number. Once you have taken your sample and calculated the confidence interval, that interval is a fixed range of numbers (e.g., 0.108 to 0.199). At this point, your fixed interval either contains the fixed parameter or it does not. The “chance” part of the process is over.\n\n\nWhy 95% is not quite right\n\nThe 95% refers to the long-run success rate of the method you used. It means that if you were to repeat your entire sampling process 100 times, you would expect about 95 of the resulting confidence intervals to capture the true parameter. It does not apply to any single, already-calculated interval.\n\n\n\nThe influence of sample size and variability\n\nThe sample size and variability influence the width of the confidence interval, not its interpretation!\n\n\n\nQ5) You actually know the population parameter. A bunch of friends sample randomly from this population, and calcualte 95% confidence intervals. What proportion of these confidence intervals will catch the true parameter? (check all that apply). There is no probability about this, it either did or it did not95%It depends on the sample sizeIt depends on the sample standard deviation\n\n\nExplanation for Q5\n\nIn this case we are sampling from a population with a known actual parameter!\n\n\n\nThe faithful data: Old faithful is meant to erupt pretty regularly, how regularly is this? Lets look into it by estimating our uncertainty in the mean waiting time in the faithful data set in R.\nFirst, let’s take a glimpse of the faithful data.\n\nglimpse(faithful)\n\nRows: 272\nColumns: 2\n$ eruptions &lt;dbl&gt; 3.600, 1.800, 3.333, 2.283, 4.533, 2.883, 4.700, 3.600, 1.95…\n$ waiting   &lt;dbl&gt; 79, 54, 74, 62, 85, 55, 88, 85, 51, 85, 54, 84, 78, 47, 83, …\n\n\nWe see two columns:\n- eruptions is how long an eruption lasts.\n- waiting is the time until the next eruption.\nMake a histogram of the waiting time between eruptions in the webr console below!\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ6) The distribution of waiting times at Old Faithful’s is ___ UnimodalBimodalSymmetric\n\nQ7) Return to the code space above and find the mean waiting time at Old Faithful. \n\nQ8) Return to the code space above – generate 5000 bootstrap replicates of mean waiting time. The bootstrap distribution is roughly ___ (select all that apply). UnimodalBimodalSymmetric\n\n\nExplanation for Q8\n\n\n\nR code\n\n\nwait_dist &lt;-  faithful %&gt;%\n    specify(response = waiting)           |&gt;\n    generate(reps = 5000, type = \"bootstrap\") |&gt; \n    calculate(stat = \"mean\")\n\nggplot(wait_dist, aes(x = stat))+\n    geom_histogram(color = \"white\")\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\nExplanation of answer\n\nAlthough the original data of waiting times was bimodal (from Q6), the bootstrap distribution of the mean becomes unimodal and roughly symmetric. (Although there’s room to quibble - the data aren’t fully symmetric, so if you only picked unimodal, you’re ok!).\nThis happens because the process of averaging smooths out extremes. Each bootstrap sample contains a mix of high and low waiting times, so the means calculated from them tend to cluster together in the middle. This creates the single, symmetric peak you see here. We will rely on this idea when we explore the normal distribution and the central limit theorem later in the book.\n\n\n\nQ9) Following up on the previous question, the lower bound of the 99% confidence interval of mean waiting time is `r \n\n\nExaplanation for Q9\n\nNotice this question asks for a 99% confidence interval. We choose a higher confidence level like this when the consequences of being wrong (i.e., failing to capture the true population mean) are more severe.\nThis greater confidence comes at a cost, however. A 99% confidence interval is always wider and less precise than a 95% interval calculated from the same data.\n\n\nQ10) If you were to find the bootstrap standard error you would calculate the ___ of the bootstrap distribution Standard deviationMeanIQR",
    "crumbs": [
      "12. Uncertainty",
      "• 12. Uncertainty summary"
    ]
  },
  {
    "objectID": "book_sections/uncertainty/uncertainty_summary.html#uncertainty_summary_glossary-of-terms",
    "href": "book_sections/uncertainty/uncertainty_summary.html#uncertainty_summary_glossary-of-terms",
    "title": "• 12. Uncertainty summary",
    "section": "📊 Glossary of Terms",
    "text": "📊 Glossary of Terms\n\n\nBootstrap Distribution: The distribution of a statistic (e.g., the mean) calculated from a large number of bootstrap samples. It’s used to approximate the sampling distribution.\nBootstrap Replicate (or Bootstrap Sample): A new sample of the same size as the original, created by randomly drawing observations from the original sample with replacement.\nBootstrap Standard Error: The standard deviation of the bootstrap distribution, which serves as an estimate of the standard error of an estimate.\nBootstrapping: A computational resampling method that approximates the sampling distribution by repeatedly taking samples with replacement from the original data.\nConfidence Interval (CI): A range of plausible values for an unknown population parameter, calculated from sample data. For example, a 95% confidence interval is generated by a process that is expected to capture the true parameter 95% of the time.\nConfidence Level: Reasonable bounds we put around our estimate to acknowledge sampling error’s impact on it.\nSampling with Replacement: A sampling process where each selected item is returned to the pool before the next item is drawn, meaning an individual can be selected more than once. This is the core mechanism of bootstrapping.\nSampling without Replacement: A sampling process where a selected item is not returned to the pool, ensuring that all items in the final sample are distinct. This is how traditional statistical samples are taken.",
    "crumbs": [
      "12. Uncertainty",
      "• 12. Uncertainty summary"
    ]
  },
  {
    "objectID": "book_sections/uncertainty/uncertainty_summary.html#uncertainty_summaryR_packages_introduced",
    "href": "book_sections/uncertainty/uncertainty_summary.html#uncertainty_summaryR_packages_introduced",
    "title": "• 12. Uncertainty summary",
    "section": "R Packages Introduced",
    "text": "R Packages Introduced\n\n\ninfer: provides a standardized, intuitive framework for performing statistical inference, including bootstrapping.\nHmisc: ggplot2 requires this package to use the mean_cl_boot option within stat_summary(), which automatically calculates and plots bootstrap confidence intervals.",
    "crumbs": [
      "12. Uncertainty",
      "• 12. Uncertainty summary"
    ]
  },
  {
    "objectID": "book_sections/uncertainty/uncertainty_summary.html#uncertainty_summary_key-r-functions",
    "href": "book_sections/uncertainty/uncertainty_summary.html#uncertainty_summary_key-r-functions",
    "title": "• 12. Uncertainty summary",
    "section": "🛠️ Key R Functions",
    "text": "🛠️ Key R Functions\n\n\nThe infer pipeline\n\nspecify(): The first step in an infer pipeline, where you declare the variable(s) of interest, using formula syntax like response ~ explanatory.\ngenerate(): The second step, used to create resamples from the data. You specify type = \"bootstrap\" to generate bootstrap samples.\ncalculate(): The third step, which computes a summary statistic (e.g., stat = \"mean\", stat = \"diff in means\", stat = \"slope\") for each of the bootstrap replicates.\nget_ci(): A convenient final step that calculates a confidence interval from the bootstrap distribution generated by the pipeline.\n\n\n\n\nOther usefull functions\n\nstat_summary(): A powerful function that calculates and plots a summary of your data. In this chapter, it’s used with fun.data = \"mean_cl_boot\" to automatically perform bootstrapping and plot the mean and confidence interval.\nquantile() (stats): Calculates quantiles from a distribution, used to find the lower and upper bounds of a confidence interval.\nsd() (stats): Calculates the standard deviation, used to find the standard error from the bootstrap distribution.",
    "crumbs": [
      "12. Uncertainty",
      "• 12. Uncertainty summary"
    ]
  },
  {
    "objectID": "book_sections/uncertainty/uncertainty_summary.html#uncertainty_summary_additional-resources",
    "href": "book_sections/uncertainty/uncertainty_summary.html#uncertainty_summary_additional-resources",
    "title": "• 12. Uncertainty summary",
    "section": "Additional resources",
    "text": "Additional resources\n\nReadings:\n\nResampling-based methods for biologists - J. R. Fieberg et al. (2020).\nChapter 8: Estimation, Confidence Intervals, and Bootstrapping from (Ismay & Kim, 2019).\nChapter 2: Bootstrapping from J. Fieberg (2024).\nChapter 16: Visualizing uncertainty from (Wilke, 2019)\n\nWebapps:\n\nThis webapp from Whitlock & Schluter (2020) is great for thinking about confidence intervals. It’s math-based, not bootstrapped-based, but hte concepts and ideas are the same.\n\n\n\n\n\n\nFieberg, J. (2024). Statistics for ecologists: A frequentist and bayesian treatment of modern regression models. University of Minnesota Libraries Publishing. Retrieved from the University of Minnesota Digital Conservancy. https://doi.org/10.24926/9781959870029\n\n\nFieberg, J. R., Vitense, K., & Johnson, D. H. (2020). Resampling-based methods for biologists. PeerJ, 8, e9089. https://doi.org/10.7717/peerj.9089\n\n\nIsmay, C., & Kim, A. Y. (2019). Statistical inference via data science: A ModernDive into r and the tidyverse. CRC Press.\n\n\nWhitlock, M. C., & Schluter, D. (2020). The analysis of biological data (Third). Macmillan.\n\n\nWilke, C. O. (2019). Fundamentals of data visualization: A primer on making informative and compelling figures. O’Reilly Media.",
    "crumbs": [
      "12. Uncertainty",
      "• 12. Uncertainty summary"
    ]
  },
  {
    "objectID": "book_sections/nhst.html",
    "href": "book_sections/nhst.html",
    "title": "13. Null Hypothesis Significance Testing",
    "section": "",
    "text": "Review and Motivation for NHST\nThe major goals of statistics are:\nWe have spent some effort in estimation, and now know (one way) to include uncertainty. We will put off inferring cause for later in the book. So, here we will introduce the ideas behind hypothesis testing, why we do it, and what it does and does not mean.\nTo get started, let’s return to the foundational challenge in estimation. That is, we want to know. about parameters from populations, but can only access estimates from samples. We know that even in the best-designed scientific studies, sampling error will pull sample estimates away from population parameters.\nThe goal of null hypothesis significance testing (NHST) is to determine whether our observations can be reasonably explained by sampling error.",
    "crumbs": [
      "13. Null Hypothesis Significance Testing"
    ]
  },
  {
    "objectID": "book_sections/nhst.html#review-and-motivation-for-nhst",
    "href": "book_sections/nhst.html#review-and-motivation-for-nhst",
    "title": "13. Null Hypothesis Significance Testing",
    "section": "",
    "text": "Estimation (with uncertainty).\n\nHypothesis testing, and\n\nInferring causation.",
    "crumbs": [
      "13. Null Hypothesis Significance Testing"
    ]
  },
  {
    "objectID": "book_sections/nhst.html#nhst-example",
    "href": "book_sections/nhst.html#nhst-example",
    "title": "13. Null Hypothesis Significance Testing",
    "section": "NHST Example",
    "text": "NHST Example\nLet’s work through an example: We planted pink and white-petaled parviflora RILs at the Upper Sawmill (US) location. Of the 114 assayed plants with known petal color:\n\n9 of the 56 pink-petaled RILs (aka 16.1%) set at least one hybrid seed.\n\n4 of the 58 white-petaled RILs (aka 6.9%) set at least one hybrid seed.\n\nWe may naturally want to compare these values. So, are white-flowered parviflora RILs at Upper Sawmill less likely to have at least one hybrid seed than pink-petaled plants? Let’s consider some ways we could address this question with our tools so far:\n\n\nLet’s get real. I hand-picked this somewhat strange case to get us thinking about the problem. A direct way to address the plausibility that these two samples came from the same statistical population is to evaluate if the 95% confidence interval of their difference overlaps zero. This is one valid way to perform a hypothesis test. The rest of this chapter will focus on the traditional NHST framework, which uses p-values to answer the same question.\n\nBy simple estimation – 6.9% is way less than 16.1%. But we know by now that must incorporate uncertainty.\n\nComparing Figure 1 A to Figure 1 B we see that the 95% confidence intervals of these estimates broadly overlap one another.\nBut the more appropriate comparison of the 95% confidence interval of the difference in proportions (Figure 1 C) just barely overlaps zero .\ntl/dr it is unclear.\n\n\n\n\n\n\n\n\n\n\nFigure 1: A) The bootstrap distribution for the proportion of pink-flowered plants setting at least one hybrid seed. B) The bootstrap for white-flowered plants. C) The distribution of the difference in proportions (pink minus white). The 95% confidence interval, shown with dashed purple lines, slightly overlaps with zero. This sets us up for the chapter’s challenge: could the proportion of white and pink moms with one or more hybrid seeds at Upper Sawmill Road plausibly represent samples from the same statistical population?",
    "crumbs": [
      "13. Null Hypothesis Significance Testing"
    ]
  },
  {
    "objectID": "book_sections/nhst.html#null-hypothesis-significance-testing",
    "href": "book_sections/nhst.html#null-hypothesis-significance-testing",
    "title": "13. Null Hypothesis Significance Testing",
    "section": "Null Hypothesis Significance Testing",
    "text": "Null Hypothesis Significance Testing\nIn principle, in addition to a real effect, sampling bias, non-independent sampling, and sampling error could all lead to a deviation between estimates and true population parameters. Our goal in null hypotheses significance testing is to see if results are easily explained by sampling error. That is, NHST helps us assess whether the differences between our observations and expectations can be attributed to sampling error.\n\n\n\nIn null hypothesis significance testing, we aim to determine how easily our results can be explained by a “null model.” To do this, we follow three key steps:\n\nState the null hypothesis and its alternative (subsection Statistical Hypotheses).\n\nCalculate a test statistic to summarize the data, and compare it to its sampling distribution under the null model (subsection P Values).\n\nInterpret the results. If the test statistic falls in an extreme tail of the sampling distribution, we reject the null hypothesis; otherwise, we do not. (subsection Statistical Significance.\n\nThis last step is relatively easy to do. But explaining and understanding this step represents one of the more challenging concepts in statistics. Part of the difficulty lies in the fact that what we traditionally do in the field doesn’t entirely make sense. We will therefore go over Considerations for NHST before moving on to our chapter summary.",
    "crumbs": [
      "13. Null Hypothesis Significance Testing"
    ]
  },
  {
    "objectID": "book_sections/nhst/statistical_hypotheses.html",
    "href": "book_sections/nhst/statistical_hypotheses.html",
    "title": "• 13. Statistical Hypotheses",
    "section": "",
    "text": "The null hypothesis\nScientific hypotheses are exciting. As scientists, we ask interesting questions. For example, throughout this book, we are asking if parviflora flowers have evolved in ways to make them less likely to make hybrids with their close relative, xantiana. Other scientific questions include: do vaccines cause autism, does a novel drug have its claimed effect, etc etc… . These are our scientific hypotheses. They are meaningful, and grounded in our understanding of the biological world. They are the reason we do science.\nAs scientists, we’re usually trying to evaluate support for a scientific hypothesis. But in the null hypothesis significance testing framework of frequentist statistics (which we follow for most of this book), we do this in a somewhat backwards way. We evaluate the plausibility of a boring statistical hypothesis, known as the null hypothesis. If our observations are inconsistent with the null, we conclude that there is likely something else going on.\nThe Null Hypothesis (\\(H_0\\)) is the ultimate skeptic. It argues that any pattern you see in your data is just an illusion created by random chance or sampling error. It’s the voice that says, “nothing interesting is happening here” (Figure 2). This is a very specific claim so the null model is very specific.\nFigure 2: The null hypothesis is unimpressed by your sampling error.\nThe Alternative Hypothesis (\\(H_A\\) or \\(H_1\\)) is the simple opposite. It just claims that the null hypothesis is wrong… i.e. that something other than chance sampling error is likely responsible for the pattern in the data. This is a vague claim so the alternative hypothesis is not specific.\nFor our flower example, the null and alternative hypotheses are:",
    "crumbs": [
      "13. Null Hypothesis Significance Testing",
      "• 13. Statistical Hypotheses"
    ]
  },
  {
    "objectID": "book_sections/nhst/statistical_hypotheses.html#the-null-hypothesis",
    "href": "book_sections/nhst/statistical_hypotheses.html#the-null-hypothesis",
    "title": "• 13. Statistical Hypotheses",
    "section": "",
    "text": "\\(H_0\\): The proportion of moms with at least one hybrid seed does not differ between white and pink flowered plants.\n\\(H_A\\): The proportion of moms with at least one hybrid seed does differ between white and pink flowered plants.\n\n\nThe null hypothesis doesn’t care about your theories, it does not evaluate effect size, and has no sense of biological relevance.",
    "crumbs": [
      "13. Null Hypothesis Significance Testing",
      "• 13. Statistical Hypotheses"
    ]
  },
  {
    "objectID": "book_sections/nhst/statistical_hypotheses.html#properties-of-good-null-hypotheses",
    "href": "book_sections/nhst/statistical_hypotheses.html#properties-of-good-null-hypotheses",
    "title": "• 13. Statistical Hypotheses",
    "section": "Properties of good null hypotheses",
    "text": "Properties of good null hypotheses\nNotice that we chose to compare proportions, not the raw counts of plants with hybrids. This is a crucial feature of a good hypothesis test: it must make a fair comparison. Because our sample sizes for pink (56) and white (58) flowers were unequal, comparing raw counts would be misleading and biologically uninteresting. More generally, because the null hypothesis is a skeptic that doesn’t understand biology, it’s our job to design studies where its rejection is both interesting and informative.\n\nGood nulls are non-trivial: Testing the null that white flowers have zero hybrids is lame. If we see at least one hybrid then we couldn’t have gotten such a result by sampling error from a population with no hybrids. Similarly, the null hypothesis that mean petal length is zero mm squared should not be tested!\nGood nulls represent a fair comparison: As stated above, we compared the proportion white and pink flowered plants with at least one hybrid seed, not the raw counts to avoid bias.When you design your studies make sure the comparison is fair!.\nGreat nulls isolate the effect of interest: A great null model creates a world where “all else is equal” (ceteris paribus). For example, the best test would ensure that other covariates, that differ (e.g. differences in petal length) between our explanatory variable (e.g. flower color morph), aren’t the real cause of a difference in our response variable.\n\ntl/dr: It is our responsible to design studies with a clean link between our exciting scientific question and the rigid world of statistics.\n\n\n\n\n\n\n\n\nFigure 3: A PhD Comic on the “Analysis of Value”. If your null is a trivial model that is uninteresting to reject, the NHST can feel more like a painful analysis of your work’s value than a meaningful scientific inquiry.\n\n\n\n\n\n\nA Technical Note: One-Tailed vs. Two-Tailed Tests\nNotice that our \\(H_A\\) above says the proportions “differ” between white and pink morphs. It did not specify a direction. This is a two-tailed test, and it’s standard practice. We are open to the effect going in either direction. A one-tailed test is when we only care about a specific direction (e.g., \\(H_A\\): pink flowers have a higher proportion of hybrids). In practice, one-tailed tests are rare and often inappropriate because we’d almost always want to know about a strong effect in the unexpected direction. Additionally, one-tailed tests often breed distrust in your audience – they signal that you are trying to pull a fast one.\n\nRare cases when a one-tailed test is appropriate occur when both extremes of the outcome are on the same side of the null distribution. For instance, if I were studying the absolute value of something, the null hypothesis would be that it’s zero, and the alternative would be that it’s greater than zero. We’ll see that some test statistics, like the \\(F\\) statistic and (often) the \\(\\chi^2\\) statistic, only have one relevant tail.",
    "crumbs": [
      "13. Null Hypothesis Significance Testing",
      "• 13. Statistical Hypotheses"
    ]
  },
  {
    "objectID": "book_sections/nhst/p_values.html",
    "href": "book_sections/nhst/p_values.html",
    "title": "• 13. P Values",
    "section": "",
    "text": "The Test Statistic\nIn null hypothesis significance testing we ask how unusual our observation would be if it came from the null model. To do this we must\nIn this subsection we work through this!\nWhen we get into doing stats with math tricks, we will run into set of test statistics that you may have heard of already (e.g. t, F, \\(\\chi^2\\), Z etc.) But, using computational tools allows us to come up with whatever test statistic we deem most appropriate. For example, in our motivating example about setting at least one hybrid seed, we are comparing two proportions (the proportion of pink-flowered plants with at least one hybrid and the proportion of white-flowered plants with at least one hybrid). Let’s summarize that as the difference in proportions, and have this be our test statistic.",
    "crumbs": [
      "13. Null Hypothesis Significance Testing",
      "• 13. P Values"
    ]
  },
  {
    "objectID": "book_sections/nhst/p_values.html#the-sampling-distribution-under-the-null",
    "href": "book_sections/nhst/p_values.html#the-sampling-distribution-under-the-null",
    "title": "• 13. P Values",
    "section": "The Sampling Distribution Under the Null",
    "text": "The Sampling Distribution Under the Null\n\n\n\n\n\n\n\n\n\nFigure 1: A null sampling distribution of the difference in proportion of pink and white flowered parviflora RILs with at least one hybrid. Data are from upper sawmill.\n\n\n\n\nBecause the null model is specific, we can generate the expected distribution of the test statistic by creating a sampling distribution from the null model. For now, I will provide you with sampling distributions of test statistics under the null. Later, we’ll learn more about how to generate these distributions ourselves.\nWe can visualize the sampling distribution under the null as a histogram, just like any other sampling distribution (Figure 1).",
    "crumbs": [
      "13. Null Hypothesis Significance Testing",
      "• 13. P Values"
    ]
  },
  {
    "objectID": "book_sections/nhst/p_values.html#comparing-observation-to-the-null",
    "href": "book_sections/nhst/p_values.html#comparing-observation-to-the-null",
    "title": "• 13. P Values",
    "section": "Comparing Observation to the Null",
    "text": "Comparing Observation to the Null\n\n\n\n\n\n\n\n\n\nFigure 2: Placing our observed test statistic of 0.0917 (red dashed line) on the null sampling distribution. Data are from upper sawmill.\n\n\n\n\nNext, we compare our actual test statistic to its sampling distribution under the null hypothesis (Figure 2). Recall that we found that 9 of the 56 pink-petaled RIL and 4 of the 58 white-petaled RILs set at least one hybrid seed. SO our test statistic is \\(\\frac{9}{56}- \\frac{4}{58}=  0.0917\\)",
    "crumbs": [
      "13. Null Hypothesis Significance Testing",
      "• 13. P Values"
    ]
  },
  {
    "objectID": "book_sections/nhst/p_values.html#summarizing-surprise-as-a-p-value",
    "href": "book_sections/nhst/p_values.html#summarizing-surprise-as-a-p-value",
    "title": "• 13. P Values",
    "section": "Summarizing surprise as a P-value",
    "text": "Summarizing surprise as a P-value\nThe observed test statistic shown in figure @fig-null2 is a bit right of the mode of the null sampling distribution – it’s not the most common value the null would generate, but it (to my eye at least) not shocking.\n\n\n\n\n\n\n\n\n\nFigure 3: Placing our observed test statistic of 0.0917 (red dashed line) on the null sampling distribution. The red fill highlights samples from the null distribution as or more extreme than the observed test statistic. Data are from upper sawmill.\n\n\n\n\nWe use the P-value to quantify how surprising it would be to observe a test statistic as extreme (or more extreme) under the null model. To calculate the P-value, we sum (or integrate) the area under the curve from our observation outward to the tails of the distribution. Since we are equally surprised by extreme values on both the lower (left) and upper (right) tails, we typically sum the extremes on both sides.\nIn Figure 3, we sum the areas as or more extreme than our observation on both the lower and upper tails. Because (roughly) 0.1 of the distribution larger than 0.0917 and roughly 0.043 of the distribution is less than -0.0917, our P-value is 0.1 + 0.043 = 0.143. This means that if there were truly no difference between the groups (the null hypothesis was true), we’d get a test statistic this extreme or more extreme in about 14.3% of experiments in which pink and white did not differ due to sampling alone.\n\nIf you have read carefully you may have noticed that the area to be more extreme on the left is not the same as the area to be more extreme on the right. You might expect these two tail areas to be identical (and they usually are), but in this example it is not. Here, the unevenness of sample sizes of pink and white flowered parviflora RILs leads to an asymmetric null sampling distribution.\n\n\n\n\n\n\n\n\n\n\nFigure 4: A null sampling distribution of the difference in proportion of pink and white flowered parviflora RILs with at least one hybrid at site GC.\n\n\n\n\nLet’s contrast this observation with what we see at site GC where 35 of 50 pink flowered plants and 6 of 45 white flowered plants set at least one hybrid seed. Our test statistic equals \\(\\frac{35}{50} - \\frac{6}{45} = 0.567\\). Figure 4 shows that the null model almost never generates a test statistic as extreme as we see in our data. This p-value would therefore be pretty close to zero (p &lt; 0.001). This means we would be quite shocked to see such a result come from the null model of no associateion between petal color and setting hybrid seed.",
    "crumbs": [
      "13. Null Hypothesis Significance Testing",
      "• 13. P Values"
    ]
  },
  {
    "objectID": "book_sections/nhst/statistical_significance.html",
    "href": "book_sections/nhst/statistical_significance.html",
    "title": "• 13. Statistical Significance",
    "section": "",
    "text": "Drawing a conclusion from a p-value.\nSo what do scientists do with a p-value? A p-value itself is an informative summary – it tells us the probability that a random draw from the null distribution would be as or more extreme than what we observed.\nBut here’s where things get weird. We use this probability as a measure of our data and the process that generated it.\nWhat is “sufficiently small?” A glib answer is the greek letter, \\(\\alpha\\). What should \\(\\alpha\\) be? There is no real answer, and the value of \\(\\alpha\\) is up to us as a scientific community. \\(\\alpha\\) reflects a trade-off between the power to reject a false null and the fear of rejecting a true null. Despite this nuance, \\(\\alpha\\) is traditionally set to 0.05, meaning that we have a 5% chance of rejecting a true null. This convention comes from a few words from RA Fisher such as this quote below from Fisher (1926) link here:\nFigure 1: xkcd poking fun at null hypothesis significance testing. Rollover text: If all else fails use significance at the \\(\\alpha\\) = 0.5 level and hope no one notices.\nThese customary rituals are taken quite seriously by some scientists. For certain audiences, the difference between a p-value of 0.051 and 0.049 is the difference between a non-significant and significant result—and potentially between publication or rejection. I, and many others (e.g., this article by Amrhein et al. (2019)), think this is a bad custom, and not all scientists adhere to it. Even Fisher himself seemed to waffle on this (link). Nonetheless, this is the world you will navigate, so you should be aware of these customs.",
    "crumbs": [
      "13. Null Hypothesis Significance Testing",
      "• 13. Statistical Significance"
    ]
  },
  {
    "objectID": "book_sections/nhst/statistical_significance.html#drawing-a-conclusion-from-a-p-value.",
    "href": "book_sections/nhst/statistical_significance.html#drawing-a-conclusion-from-a-p-value.",
    "title": "• 13. Statistical Significance",
    "section": "",
    "text": "If our p-value is sufficiently small, we “reject” the null hypothesis and tentatively conclude that it did not generate our data.\n\n\n\nThis is a sneaky bit of logic that is not fully justified but seems to work anyways (see the next section).\n\nIf our p-value is not sufficiently small, we “fail to reject” the null hypothesis, and tentatively conclude that there is not enough evidence to reject it.\n\n\n\n“. . . it is convenient to draw the line at about the level at which we can say: Either there is something in the treatment, or a coincidence has occurred such as does not occur more than once in twenty trials. . .”",
    "crumbs": [
      "13. Null Hypothesis Significance Testing",
      "• 13. Statistical Significance"
    ]
  },
  {
    "objectID": "book_sections/nhst/statistical_significance.html#we-never-accept-the-null",
    "href": "book_sections/nhst/statistical_significance.html#we-never-accept-the-null",
    "title": "• 13. Statistical Significance",
    "section": "We never “accept” the null",
    "text": "We never “accept” the null\nBy convention we never accept the null hypothesis – we simply “fail to reject” it. The reason for this is that we know that the null still may well be incorrect. In fact, the same effect size in a larger study would have more power to reject the null.",
    "crumbs": [
      "13. Null Hypothesis Significance Testing",
      "• 13. Statistical Significance"
    ]
  },
  {
    "objectID": "book_sections/nhst/statistical_significance.html#false-positives-and-false-negatives",
    "href": "book_sections/nhst/statistical_significance.html#false-positives-and-false-negatives",
    "title": "• 13. Statistical Significance",
    "section": "False positives and false negatives",
    "text": "False positives and false negatives\n\n\n\n\n\n\np-value &gt; α  Fail to Reject H₀\n\n\np-value ≤ α  Reject H₀\n\n\n\n\n\n\nTrue null hypothesis (H₀ is TRUE)\n\n\nFail to reject true null hypothesis  Occurs with probability 1 - α This is the correct decision\n\n\nReject true null hypothesis  Occurs with probability α (Type I Error)\n\n\n\n\nFalse null hypothesis (H₀ is FALSE)\n\n\nFail to reject false null hypothesis  Occurs with probability β (Type II Error)\n\n\nReject false null hypothesis  Occurs with probability 1 - β  (akaPower) This is the correct decision\n\n\n\n\n\nThe table above should serve as a reminder that the result of the binary decision of a null hypothesis significance test does not cleanly map onto the truth of the null hypothesis.\n\nRejecting the null does not mean that the null is untrue. The basic logic of null hypothesis significance testing means that we will reject a true null hypothesis 5% of the time (if \\(\\alpha\\) is set to the conventional 0.05). This outcome is called a “false positive result”. When the null hypothesis is true, p-values will be uniformly distributed, and the false positive rate will be \\(\\alpha\\) regardless of sample size (Figure 2).\nFailure to reject the null does not mean that the null is true. We will occasionally fail to reject some false nulls. This occurs with probability \\(\\beta\\). So, \\(1-\\beta\\) is our so called “power” to reject a false null. Unlike our false positive rate, which always equals \\(\\alpha\\), our false negative rate depends on both the sample size and the effect size. When the null hypothesis is false, we will observe more smaller p-values as the sample size increases, and the true positive rate will increase with the sample size (Figure 2).\n\nWe address this in some detail in the next subsection!\n\n\n\n\n\n\n\n\nFigure 2: a) The cumulative distribution of p-values when the null is true (left facet) and false (right facet), shown for different sample sizes. b) The probability of rejecting the null hypothesis as sample size increases. When the null is true (left), this probability remains constant at α=0.05. When the null is false (right), the probability (power) increases with sample size. In this example, the null hypothesis assumes a value of 0, while the true parameter is 0.3 standard deviations away from zero in the panel on the right. Code is available here\n\n\n\n\n\n\n\n\n\nAmrhein, V., Greenland, S., & McShane, B. (2019). Scientists rise up against statistical significance. Nature, 567(7748), 305.\n\n\nFisher, R. A. (1926). The arrangement of field experiments. Journal of the Ministry of Agriculture, 33, 503–515.",
    "crumbs": [
      "13. Null Hypothesis Significance Testing",
      "• 13. Statistical Significance"
    ]
  },
  {
    "objectID": "book_sections/nhst/nhst_gotchas.html",
    "href": "book_sections/nhst/nhst_gotchas.html",
    "title": "• 13. Considerations for NHST",
    "section": "",
    "text": "Why the Interpretation of P-values is Hard\nFigure 1: Why are p-values so confusing?\nStudents often struggle to understand p-values, and for good reason. The NHST framework is built with backwards logic on a shaky foundation. We want to know the probability that our biological hypothesis is correct, but the p-value tells us nothing about it. Instead, a p-value tells us the probability of observing our data (or something more extreme) assuming the null hypothesis is true. We then make an indirect inference: if our data are unusual under the null, we tentatively conclude the alternative is correct, even though we never actually tested the alternative model. This flawed premise, combined with scientific customs that can feel arbitrary, is the source of much confusion.\nThe common misinterpretations of a p-value I believe, reflect people wishing that a p-value reported something useful or logical. But it is not. A p-value is none of the following things:\nWhat does this mean for us as scientists? It means that we have two challenging responsibilities. First, we must understand the process of null hypothesis testing well enough to participate in its customs and rituals. Second, we must simultaneously interpret these statistics with caution and responsibility. A responsible interpretation means remembering that rejecting \\(H_0\\) does not prove it’s false, and failing to reject \\(H_0\\) does not prove it’s true or mean there is no effect. A good practice is to always look beyond the p-value to the effect size and consider if it’s consistent with a plausible biological model.",
    "crumbs": [
      "13. Null Hypothesis Significance Testing",
      "• 13. Considerations for NHST"
    ]
  },
  {
    "objectID": "book_sections/nhst/nhst_gotchas.html#why-the-interpretation-of-p-values-is-hard",
    "href": "book_sections/nhst/nhst_gotchas.html#why-the-interpretation-of-p-values-is-hard",
    "title": "• 13. Considerations for NHST",
    "section": "",
    "text": "A P-VALUE IS NOT “the probability that the null hypothesis is true” NOR IS IT “the probability that what we observed is due to chance.” These are both incorrect because the p-value is simply the probability that we would observe our data, or something more extreme, assuming the null hypothesis is true.\nA P-VALUE DOES NOT say anything about the alternative hypothesis. A p-value simply describes how unusual it would be for the null model to generate such an extreme result. Again, I understand the desire to have the p-value tell us about the alternative hypothesis, because this is usually more interesting. Sadly, p-values can’t do that.\nA P-VALUE DOES NOT measure the importance of a result. Again, such a measure would be great to have, but we don’t have that. The importance of a result depends on its effect size and its role in the biological problem we’re investigating.",
    "crumbs": [
      "13. Null Hypothesis Significance Testing",
      "• 13. Considerations for NHST"
    ]
  },
  {
    "objectID": "book_sections/nhst/nhst_gotchas.html#the-prosecutors-fallacy",
    "href": "book_sections/nhst/nhst_gotchas.html#the-prosecutors-fallacy",
    "title": "• 13. Considerations for NHST",
    "section": "The Prosecutor’s Fallacy",
    "text": "The Prosecutor’s Fallacy\n\n\n\n\n\nThe video above makes a clear point: We calculate the probability that a random sample from the null distribution would produce our test statistic or something more extreme (i.e. \\(\\text{p-value} = P(\\text{Data or more extreme}|\\text{H}_0\\)). Unfortunately, what we really want to know – probability of our null model given the data (\\(P(\\text{H}_0|\\text{Data or more extreme})\\) – is not provided by NHST.\nLater in the term, we’ll see that Bayes’ theorem sets up a different way to do stats, one that answers questions like “what’s the probability of the null hypothesis given my data?” by flipping these conditional probabilities. However, for most of this class, we cover classic frequentist statistics, so we have to remember that we are not answering that question.",
    "crumbs": [
      "13. Null Hypothesis Significance Testing",
      "• 13. Considerations for NHST"
    ]
  },
  {
    "objectID": "book_sections/nhst/nhst_gotchas.html#why-is-nhst-so-prevalent",
    "href": "book_sections/nhst/nhst_gotchas.html#why-is-nhst-so-prevalent",
    "title": "• 13. Considerations for NHST",
    "section": "Why is NHST so prevalent?",
    "text": "Why is NHST so prevalent?\n\nQ: Why do so many colleges and grad schools teach p = 0.05?\nA: Because that’s still what the scientific community and journal editors use.\nQ: Why do so many people still use p = 0.05?\nA: Because that’s what they were taught in college or grad school.\n\n— George Cobb, on the American Statistics Association forum\n\n\nSo, with all the issues with p-values and null hypothesis testing, why am I teaching it, and why do we still use it as a field? I think there are two compelling reasons:\n\nTradition! I teach this because this is how science is often done, and you should understand the culture of science and its rituals. When you read studies, you will see p-values, and when you write results, people will expect p-values. While the quote above makes light of this tradition, society function because of a set of agreed upon conventions – e.g. “why do we drive on the right side of the road?” At the same time, you should recognize that this isn’t the only way to do statistics – For example, Bayesian stats is quite mainstream. We will return to Bayesian stats at the end of the term.\nIt works! Scientists have used this approach for decades and have made continual progress. So, although the theoretical underpinnings of null hypothesis significance testing are shaky, it’s practically quite useful. Unlike George Cobb, I believe we keep using p-values and p = 0.05 because it seems to work well enough. That said, I believe that the nuanced understanding I’ve tried to equip you with in this chapter helps us make even better use of p-values.",
    "crumbs": [
      "13. Null Hypothesis Significance Testing",
      "• 13. Considerations for NHST"
    ]
  },
  {
    "objectID": "book_sections/nhst/nhst_gotchas.html#alternatives-to-null-hypothesis-significance-testing",
    "href": "book_sections/nhst/nhst_gotchas.html#alternatives-to-null-hypothesis-significance-testing",
    "title": "• 13. Considerations for NHST",
    "section": "Alternatives to Null Hypothesis Significance Testing",
    "text": "Alternatives to Null Hypothesis Significance Testing\nDue to the issues surrounding p-values, such as the arbitrary distinction between “significant” and “non-significant” results, some have proposed alternative approaches to statistics.\nThese alternatives include banning p-values, replacing them with confidence intervals, and conducting Bayesian analyses, among others. I highly recommend the paper, Some Natural Solutions to the p-Value Communication Problem—and Why They Won’t Work (Gelman & Carlin, 2017), for a fun take on these proposals.\nNotably, Gelman rejects my solution of teaching better. He says teachers are the problem.",
    "crumbs": [
      "13. Null Hypothesis Significance Testing",
      "• 13. Considerations for NHST"
    ]
  },
  {
    "objectID": "book_sections/nhst/nhst_gotchas.html#optional-the-bayesian-approach",
    "href": "book_sections/nhst/nhst_gotchas.html#optional-the-bayesian-approach",
    "title": "• 13. Considerations for NHST",
    "section": "OPTIONAL: The Bayesian Approach",
    "text": "OPTIONAL: The Bayesian Approach\nHere, I briefly introduce Bayesian statistics. Bayesian statistics aims to find the probability of a model given the data, using Bayes’ theorem. This is often the type of question we want to answer. However, a word of caution—frequentists believe there is no probability associated with the true parameter, as populations have fixed parameters. In contrast, Bayesians believe that parameters have probability distributions that reflect uncertainty or prior knowledge about them. This represents a fundamentally different way of thinking about the world.\n\\[P(\\text{Model}|\\text{Data}) = \\frac{P(\\text{Data}|\\text{Model}) \\times P(\\text{Model})}{P(\\text{Data})}\\]\nWe can break this down with new terminology:\n\n\\(P(\\text{Model}|\\text{Data})\\): the “posterior probability” — the probability of the model given the observed data.\n\n\\(P(\\text{Data}|\\text{Model})\\): the “likelihood” — the probability of observing the data under the given model.\n\n\\(P(\\text{Model})\\): the “prior” — our prior belief about the probability of the model or parameter values before seeing the data. This can come from previous studies, expert knowledge, or assumptions.\n\n\\(P(\\text{Data})\\): the “evidence” — the overall probability of the data, which serves to normalize the result. It can be computationally intensive to calculate, but there are methods like Markov Chain Monte Carlo (MCMC) to approximate this value.\n\n\\[\\text{Posterior Probability} = \\frac{\\text{Likelihood}(\\text{Model}|\\text{Data}) \\times \\text{Prior}}{\\text{Evidence}}\\]\nNotably, Bayesian methods allow us to study “credible intervals” — regions with a 95% probability of containing the true population parameter, as opposed to “confidence intervals,” which in frequentist statistics only describe the frequency with which the interval will contain the true parameter in repeated samples.",
    "crumbs": [
      "13. Null Hypothesis Significance Testing",
      "• 13. Considerations for NHST"
    ]
  },
  {
    "objectID": "book_sections/nhst/nhst_summary.html",
    "href": "book_sections/nhst/nhst_summary.html",
    "title": "• 13. NHST summary",
    "section": "",
    "text": "Links to: Summary. Chatbot tutor. Questions. Glossary. More resources.",
    "crumbs": [
      "13. Null Hypothesis Significance Testing",
      "• 13. NHST summary"
    ]
  },
  {
    "objectID": "book_sections/nhst/nhst_summary.html#nhst_summary_chapter-summary",
    "href": "book_sections/nhst/nhst_summary.html#nhst_summary_chapter-summary",
    "title": "• 13. NHST summary",
    "section": "Chapter summary",
    "text": "Chapter summary\nNull Hypothesis Significance Testing is a tool to judge if we should interpret a scientific result as sampling error or as a genuine biological process. The first step is coming up with the null hypothesis and finding a “test statistic” to boil our observations down to a single number. We then find the null distribution of the test statistic under the null model and place our actual observation on this null sampling distribution. We calculate a p-value as the proportion of the null distribution that is “as or more extreme” as our observation (remember to look at both tails). We “reject the null” when our p-value is less than \\(\\alpha\\) (our “false positive rate”), which is set to 0.05 by tradition. We fail to reject the null otherwise. Unfortunately, failing to reject the null does not mean the null is true. This whole business is a bit tricky, so make sure you don’t misinterpret your p-value.\n\n\n\n\n\n🌿 🌿 Enjoy this humorous, frustrating, and correct presentation on the meaning and interpretation of p-values.\n\n\n\nChatbot tutor\n\nPlease interact with this custom chatbot (link here). I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you got what you needed from it.",
    "crumbs": [
      "13. Null Hypothesis Significance Testing",
      "• 13. NHST summary"
    ]
  },
  {
    "objectID": "book_sections/nhst/nhst_summary.html#nhst_summary_practice-questions",
    "href": "book_sections/nhst/nhst_summary.html#nhst_summary_practice-questions",
    "title": "• 13. NHST summary",
    "section": "Practice Questions",
    "text": "Practice Questions\nTry these questions!\n\nQ1) Null or alternative? Identify whether each of the following statements is more appropriate as a null hypothesis:\n\n The number of hours that grade school children spend doing homework predicts their future success on standardized tests. King cheetahs on average run the same speed as standard spotted cheetahs. The mean length of African elephant tusks has changed over the last 100 years.\n\n.\n\nQ2) Assume the null hypothesis is TRUE. Which one of the following statements is true?\n\n A study with a larger sample is more likely than a smaller study to get the result that P &lt; 0.05. A study with a large sample is less likely than a smaller study to get the result that P &lt; 0.05 A study with a large sample is equally likely compared to a smaller study to get the result that P &lt; 0.05.\n\n\nQ3) Assume the null hypothesis is FALSE. Which one of the following statements is true?\n\n A study with a larger sample is more likely than a smaller study to get the result that P &lt; 0.05. A study with a large sample is less likely than a smaller study to get the result that P &lt; 0.05 A study with a large sample is equally likely compared to a smaller study to get the result that P &lt; 0.05.\n\n\n\nCan parents distinguish their own children by smell alone? To investigate, Porter and Moore (1981) gave new T-shirts to children of nine mothers. Each child wore his or shirt to bed for three consecutive nights. During the day, from waking until bedtime, the shirts were kept in individually sealed plastic bags. No scented soaps or perfumes were used during the study. Each mother was given the shirt of her child and that of another, randomly chosen child and asked to identify her own by smell. Eight of nine mothers identified their children correctly. Use this study to answer the following questions, using a two-sided significance level 𝛼 = 0.05.\n\n\nQ4) \\(H_0\\): What is the appropriate NULL hypothesis?\n\n Mothers correctly guess their children's shirts half of the time Mothers correctly guess their children's shirts half of the time or less Mothers correctly guess their children's shirts more than half of the time Mothers do not correctly guess their children's shirts half of the time\n\n\nQ5) \\(H_A\\): What is the appropriate ALTERNATIVE hypothesis?\n\n Mothers correctly guess their children's shirts half of the time Mothers correctly guess their children's shirts half of the time or less Mothers correctly guess their children's shirts more than half of the time Mothers do not correctly guess their children's shirts half of the time\n\n\n\nExplanation of Q5\n\nRemember we should do two-tailed tests!\n\n\nFor the questions below refer to the following figure. Numbers over bars show the probability of x moms guessing right assuming the null hypothesis is true.\n\n\n\n\n\n\n\n\n\n\nQ6) Recall that eight of nine mothers identified their children correctly. Use the numbers on the figure above to estimate a P-value. \n\n#Q7) What do we do to the null hypothesis in this case? Reject itFail to Reject itAccept itFail to accept itNot enough information\n\nQ8) In this case, the null hypothesis Is TrueIs Falsehas a 3.6% chance of being truehas a 4% chance of being trueneed more info\n\nQ9) The prosecutor’s fallacy is the mistaken thought that arises from assuming that\n\n people are innocent until proven guilty people are guilty until proven innocent data are collected without sampling error the probability of the model given the data is the probability of the data given the model assuming all evidence in a criminal case are independent and collected without bias",
    "crumbs": [
      "13. Null Hypothesis Significance Testing",
      "• 13. NHST summary"
    ]
  },
  {
    "objectID": "book_sections/nhst/nhst_summary.html#nhst_summary_glossary-of-terms",
    "href": "book_sections/nhst/nhst_summary.html#nhst_summary_glossary-of-terms",
    "title": "• 13. NHST summary",
    "section": "📊 Glossary of Terms",
    "text": "📊 Glossary of Terms\n\n\nThe Null Hypothesis: A skeptical explanation, made for the sake of argument, which suggests that data come from an uninteresting or “boring” population.\nThe Alternative Hypothesis: A claim that the data do not come from the “boring” population.\nThe Test Statistic: A single number that summarizes the data. We compare the observed test statistic to its sampling distribution under the null model.\nP-value: The probability that a random sample drawn from the null model would be as extreme or more extreme than what is observed.\n\\(\\alpha\\): The probability that we reject a true null hypothesis. We can decide what this is, but by convention, \\(\\alpha\\) is usually set at 0.05.\nFalse Positive: Rejecting the null when the null is true.\nFalse Negative: Failing to reject the null when it is false.\nPower: The probability of rejecting a false null hypothesis. We cannot directly set this — it depends on sample size and the size of the effect, but we can design experiments aiming for a certain level of power.\nTest Statistic: A single number calculated from sample data that summarizes the difference between observations and what is expected under the null hypothesis.",
    "crumbs": [
      "13. Null Hypothesis Significance Testing",
      "• 13. NHST summary"
    ]
  },
  {
    "objectID": "book_sections/nhst/nhst_summary.html#nhst_summary_additional-resources",
    "href": "book_sections/nhst/nhst_summary.html#nhst_summary_additional-resources",
    "title": "• 13. NHST summary",
    "section": "Additional resources",
    "text": "Additional resources\n\nReadings:\n\nNumerous readings from the Scientist Sees Squirrel blog by Stephen B. Heard including: In defence of the P-value, Is “nearly significant” ridiculous?, and Two tired misconceptions about null hypotheses.\nSome Natural Solutions to the p-Value Communication Problem—and Why They Won’t Work (Gelman & Carlin, 2017).\nStatement on P-values from the American Statistical Association (Wasserstein & Lazar, 2016).\nThe p-value statement, five years on(Matthews, 2021)\nScientists rise up against statistical significance by Amrhein et al. (2019).\n\nWebapps:\n\nErrors and Power in Significance Testing webapp.\n\nVideos:\n\nP-values and The prosecutor’s falacy from calling bullshit.\nProbability, Part 7: What is Null Hypothesis Significance Testing? from Simplistics (QuantPsych). An opiniated but entertaining presentation of p-values & NHST.\n\n\n\n\n\n\nAmrhein, V., Greenland, S., & McShane, B. (2019). Scientists rise up against statistical significance. Nature, 567(7748), 305.\n\n\nGelman, A., & Carlin, J. (2017). Some natural solutions to the p-value communication problem—and why they won’t work. Journal of the American Statistical Association, 112(519), 899–901. https://doi.org/10.1080/01621459.2017.1311263\n\n\nMatthews, R. (2021). The p-value statement, five years on. Significance, 18(2), 16–19. https://doi.org/https://doi.org/10.1111/1740-9713.01505\n\n\nWasserstein, R. L., & Lazar, N. A. (2016). The ASA statement on p-values: Context, process, and purpose. The American Statistician, 70(2), 129–133. https://doi.org/10.1080/00031305.2016.1154108",
    "crumbs": [
      "13. Null Hypothesis Significance Testing",
      "• 13. NHST summary"
    ]
  },
  {
    "objectID": "book_sections/shuffling.html",
    "href": "book_sections/shuffling.html",
    "title": "14. Shuffling",
    "section": "",
    "text": "Generating a Null Distribution with Permutation\nOne of the most common statistical questions we ask is, “Do these two samples differ?” For example:\nAnswering questions like these is fundamental to scientific research, and Null Hypothesis Significance Testing (NHST) provides a framework for addressing them. Recall from the previous chapter that NHST works by comparing the observed test statistic to its sampling distribution under the assumption that the null hypothesis is true. We then reject the null hypothesis if our data (or something more extreme) is a rare outcome of the null model and fail to reject the null hypothesis if our data (or something more extreme) is a common outcome of the null model.\nFigure 1: Listen to the permutation anthem here.\nBut how do we find the null sampling distribution? One way, known as permutation, generates a null distribution directly from the data by randomly shuffling the explanatory variable. By breaking the link between variables and reshuffling many times, permutation simulates the null distribution because any observed differences or associations in the permuted data must be due to random chance. We can then generate a p-value by comparing our test statistic to this empirically generated null distribution (that is permuted).\nUnlike mathematical tricks we will cover later, permutation makes very few assumptions. The most critical assumptions are that samples are random, independent, collected without bias, and that the null hypothesis assumes no association. What’s more, fancier permutation methods can accommodate some assumption violations (as we will see in the Structured Permutation section). Because permutation tests are highly versatile, they can be applied to a wide range of scenarios. This makes them a flexible tool for testing hypotheses, even in more complex designs.",
    "crumbs": [
      "14. Shuffling"
    ]
  },
  {
    "objectID": "book_sections/shuffling.html#steps-in-a-permutation-test",
    "href": "book_sections/shuffling.html#steps-in-a-permutation-test",
    "title": "14. Shuffling",
    "section": "Steps in a permutation test",
    "text": "Steps in a permutation test\nThere are a few steps to conducting a permutation test, and to conduct NHST, more generally. I work through them here so that we are prepared!\n\nState the null (\\(H_0\\)), and alternative (\\(H_A\\)) hypotheses.\nDecide on a test statistic.\nCalculate the test statistic for the actual data.\nPermute the data by shuffling values of the explanatory variables across observations.\nCalculate the test statistic on this permuted data set.\nRepeat steps 4 and 5 many times to generate the sampling distribution under the null.\nCalculate a p-value as the proportion of permuted values that are as or more extreme than what was observed in the actual data.\nInterpret the p-value.\nWrite up the results.",
    "crumbs": [
      "14. Shuffling"
    ]
  },
  {
    "objectID": "book_sections/shuffling.html#whats-ahead",
    "href": "book_sections/shuffling.html#whats-ahead",
    "title": "14. Shuffling",
    "section": "What’s ahead",
    "text": "What’s ahead\nTo give you a quick break from Clarkia, and perhaps more importantly, to set up a groan-inducing pun we will move from plants to amphibians! But we are still concerned with mating and its implications. We will use this system to go through the steps above (and more) as follows:\n\nThe next section introduces the biological system, scientific hypotheses and the data set. In doing so, we also go over steps 1-3 above. This section will also review key concepts of bootstrapping and uncertainty!\nIn the section after that we generate the null sampling distribution and calculate a p-value with the infer package thereby addressing steps 4-7 and interpret and write up the results (steps 8-9)!\nI even show you how to permute in a way that mirrors the structure of our data to generate a null distribution with non-independent data.\nFinally, we address limitations of a standard permutation. We also introduce how we can accommodate non-independence in our statistical test by permuting in a way that matches the structure of our data!\n\nAs always, we conclude with a chapter summary containing all the goodies!\n\nAn exact permutation tests all possible shuffle of the data to get an exact p-value. However this is computationally not feasible for all but the smallest data sets.\nInstead, we take a “Monte Carlo” (French for random stuff… jk) approach. We just take a large random sample of possible shuffles (say, 5,000) to build our null distribution. The p-value is then an approximation. But it’s good enough for NHST and is the standard way these tests are done.",
    "crumbs": [
      "14. Shuffling"
    ]
  },
  {
    "objectID": "book_sections/shuffling/the_frogs.html",
    "href": "book_sections/shuffling/the_frogs.html",
    "title": "• 14. The frogs",
    "section": "",
    "text": "Case study: Mate choice & fitness in frogs\nThere are plenty of reasons to choose your partner carefully. In much of the biological world a key reason is “evolutionary fitness” – presumably organisms evolve to choose mates that will help them make more (or healthier) children. This could, for example explain Kermit’s resistance in one of the more complex love stories of our time, as frogs and pigs are unlikely to make healthy children.\nTo evaluate this this idea Swierk & Langkilde (2019), identified a males top choice out of two female wood frogs and then had them mate with the preferred or nonpreferred female and counted the number of hatched eggs. The raw data are available here\nFigure 1: A picture of the woodfrog. Image contributed to wikipedia under a CC BY 2.0 license by Brian Gratwicke.",
    "crumbs": [
      "14. Shuffling",
      "• 14. The frogs"
    ]
  },
  {
    "objectID": "book_sections/shuffling/the_frogs.html#case-study-mate-choice-fitness-in-frogs",
    "href": "book_sections/shuffling/the_frogs.html#case-study-mate-choice-fitness-in-frogs",
    "title": "• 14. The frogs",
    "section": "",
    "text": "In fact they will form fewer hybrids by mating than will parviflora and xantiana.\n\n\n\nConcept check\n\nIs this an experimental or observational study?\n\n Experimental Observational\n\n\n\nExplanation\n\nIt’s an experimental study because the researchers actively intervened and manipulated a variable. They assigned males to mate with either a preferred or a non-preferred female, rather than just observing which mates they chose on their own.\n\n\nWhich of the following describes the biological hypothesis?\n\n That males who mate with their preferred female will have higher fitness (more hatched eggs). That frogs and pigs cannot produce healthy offspring. That mate preference does not affect the number of hatched eggs. That there will be a difference in the number of hatched eggs between the two groups.\n\n\n\nExplanation\n\nThe core biological idea is that mate choice should be adaptive. Therefore, the hypothesis is that a male’s choice should lead to a better evolutionary outcome, in this case, higher reproductive success measured by the number of hatched eggs.\n\nWhy not “That there will be a difference in the number of hatched eggs between the two groups”? A biological hypothesis is based on a theory, and therefore predicts a specific direction, such as higher fitness from mate choice. The statement “that there will be a difference” is a non-directional statistical hypothesis; it lacks the underlying biological reasoning and direction.\n\n\n\nWhat is the statistical null hypothesis (\\(H_0\\))?\n\n The observed difference in mean hatched eggs in our sample is zero. Males who mate with preferred females will have more hatched eggs. The average number of hatched eggs between the two treatment groups does not differ. The study design is biased.\n\n\n\nExplanation\n\nThe null hypothesis (\\(H_0\\)) is the hypothesis of ‘no effect.’ It posits that there is no true difference in the average number of hatched eggs between the preferred and non-preferred groups in the population. Any difference we see in our sample is assumed to be due to random sampling variation.\n\nWhy not “The observed difference in mean hatched eggs in our sample is zero.”? Because null hypotheses are about population parameters not observed sample estimates.\n\n\n\nWhich of the following is a potential source of bias in this experiment?\n\n If the 'non-preferred' females were systematically smaller or less healthy than the 'preferred' females. The study was only conducted on wood frogs. The frogs were collected from different ponds. Some matings might not produce any eggs.\n\n\n\nExplanation\n\nBias occurs when there is a systematic difference between your treatment groups that is not the treatment itself. If the non-preferred females were also different in another key aspect (like health or size), we wouldn’t be able to tell if the results were due to ‘preference’ or that other factor. Random assignment helps to prevent this.",
    "crumbs": [
      "14. Shuffling",
      "• 14. The frogs"
    ]
  },
  {
    "objectID": "book_sections/shuffling/the_frogs.html#visualize-patterns",
    "href": "book_sections/shuffling/the_frogs.html#visualize-patterns",
    "title": "• 14. The frogs",
    "section": "Visualize Patterns",
    "text": "Visualize Patterns\n\n\nLoading, formating, and plotting the data\nfrogs &lt;- read_csv(\"https://raw.githubusercontent.com/ybrandvain/biostat/master/data/Swierk_Langkilde_BEHECO_1.csv\")|&gt;\n  dplyr::select(year, pond, treatment,hatched.eggs, total.eggs, num.metamorphosed,field.survival.num)%&gt;%\n  mutate_at(.vars = c(\"hatched.eggs\", \"total.eggs\", \"num.metamorphosed\" , \"field.survival.num\"), as.numeric)\nlibrary(ggforce)\nggplot(frogs, aes(x = treatment, y = hatched.eggs, color = treatment, shape = treatment)) +\n  geom_violin(show.legend = FALSE)+\n  geom_sina(show.legend = FALSE, size = 6, alpha = .6) +  # if you don't have ggforce, use geom_jitter\n  stat_summary(fun.data = mean_cl_boot, geom = \"pointrange\", color = \"black\", show.legend = FALSE,linewidth = 1.2)+\n  theme(axis.title = element_text(size = 30),\n        axis.text = element_text(size = 30))\n\n\n\n\n\n\n\n\n\nFigure 2: Visualizing the frog mating data.\n\n\n\n\nOne of the first things we do with a new dataset is visualize it! A quick exploratory plot (refer back to our Intro to ggplot) helps us understand the distribution and shape of our data, and informs us how to develop an appropriate analysis plan.",
    "crumbs": [
      "14. Shuffling",
      "• 14. The frogs"
    ]
  },
  {
    "objectID": "book_sections/shuffling/the_frogs.html#estimate-parameters",
    "href": "book_sections/shuffling/the_frogs.html#estimate-parameters",
    "title": "• 14. The frogs",
    "section": "Estimate Parameters",
    "text": "Estimate Parameters\nNow that we have a better understanding of the data’s shape, we can create meaningful summaries by estimating appropriate parameters (see the chapters on data summaries, associations, and linear models).\nFor the frog mating data, let’s first calculate the mean and standard deviation of the number of hatched eggs for both the preferred and non-preferred mating groups.\n\nfrog_summary &lt;- frogs       |&gt;\n    group_by(treatment)     |&gt;\n    summarise( mean_hatched = mean(hatched.eggs),\n        sd_hatched = sd(hatched.eggs))\n\n\n\n\n\n\ntreatment\nmean_hatched\nsd_hatched\n\n\n\n\nnonpreferred\n413.9310\n236.8918\n\n\npreferred\n345.1111\n259.6380",
    "crumbs": [
      "14. Shuffling",
      "• 14. The frogs"
    ]
  },
  {
    "objectID": "book_sections/shuffling/the_frogs.html#calculating-a-test-statistic",
    "href": "book_sections/shuffling/the_frogs.html#calculating-a-test-statistic",
    "title": "• 14. The frogs",
    "section": "Calculating a test statistic",
    "text": "Calculating a test statistic\nHow do we summarize this variation down to a single test statistic? A high-level summary could be the difference in mean hatched eggs between the preferred and non-preferred mating groups. We could find this as 345.11 - 413.93 = -68.82. But I want to remind you how to do this with the infer package as we will be using it throughout this section:\n\nlibrary(infer)\nfrogs |&gt;\n    specify(hatched.eggs ~ treatment) |&gt;\n    calculate(stat = \"diff in means\", order = c(\"preferred\",\"nonpreferred\"))\n\nResponse: hatched.eggs (numeric)\nExplanatory: treatment (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1 -68.8\n\n\n… So both visual and numerical summaries suggest that, contrary to our expectations, more eggs hatch when males mate with non-preferred females. However, we know that sample estimates will differ from population parameters by chance (i.e., sampling error). So we …",
    "crumbs": [
      "14. Shuffling",
      "• 14. The frogs"
    ]
  },
  {
    "objectID": "book_sections/shuffling/the_frogs.html#bootstrap-to-estimate-the-difference-in-means",
    "href": "book_sections/shuffling/the_frogs.html#bootstrap-to-estimate-the-difference-in-means",
    "title": "• 14. The frogs",
    "section": "Bootstrap to estimate the difference in means",
    "text": "Bootstrap to estimate the difference in means\nWe can separately bootstrap to estimate the uncertainty in each mean, and then bootstrap the data together to estimate the uncertainty in the difference in means:\n\n\nBootstrapping the data\nlibrary(infer)\nci_preffered &lt;- frogs |&gt;\n    filter(treatment == \"preferred\")|&gt;\n    specify(response = hatched.eggs) |&gt;\n    generate(reps = 5000, type = \"bootstrap\") |&gt;\n    calculate(stat = \"mean\")|&gt;\n    get_ci()\n\nci_nonpreffered &lt;- frogs |&gt;\n    filter(treatment == \"nonpreferred\")|&gt;\n    specify(response = hatched.eggs) |&gt;\n    generate(reps = 5000, type = \"bootstrap\") |&gt;\n    calculate(stat = \"mean\")|&gt;\n    get_ci()\n\nci_diff &lt;- frogs |&gt;\n    specify(hatched.eggs ~ treatment) |&gt;\n    generate(reps = 5000, type = \"bootstrap\") |&gt;\n    calculate(stat = \"diff in means\", order = c(\"preferred\",\"nonpreferred\"))|&gt;\n    get_ci()\n\nkable(bind_rows(ci_preffered,ci_nonpreffered,ci_diff)|&gt;\n        mutate_all(round)|&gt;\n        mutate(type = c(\"preferred\",\"nonpreferred\",\"difference in means\")))\n\n\n\n\n\nlower_ci\nupper_ci\ntype\n\n\n\n\n252\n445\npreferred\n\n\n332\n498\nnonpreferred\n\n\n-198\n61\ndifference in means\n\n\n\n\n\nThere is considerable uncertainty in our estimates. Our bootstrapped 95% confidence interval includes cases where preferred matings yield more hatched eggs than nonpreferred matings, as well as cases where nonpreferred matings result in more hatched eggs.\nHow do we test the null hypothesis that mate preference does not affect egg hatching? Let’s generate a sampling distribution under the null!\n\n\n\n\nSwierk, L., & Langkilde, T. (2019). Fitness costs of mating with preferred females in a scramble mating system. Behavioral Ecology, 30(3), 658–665. https://doi.org/10.1093/beheco/arz001",
    "crumbs": [
      "14. Shuffling",
      "• 14. The frogs"
    ]
  },
  {
    "objectID": "book_sections/shuffling/permute.html",
    "href": "book_sections/shuffling/permute.html",
    "title": "• 14. Permute",
    "section": "",
    "text": "Permuting once\nRecall the steps in a permutation test:\nWe are at step four. This means\nIT’S TIME TO…\nPERMUTE THE FROG\nStep 4: Permute the data by shuffling values of the explanatory variables across observations.\nNow we permute the data by randomly shuffling the treatment onto our observed response variable, hatched.eggs. We can again use the infer package to do this! But now when we generate() we set type = \"permute\" instead of type = \"bootstrap\" We also specify() the model!\none_perm &lt;- frogs |&gt;\n  specify(hatched.eggs ~ treatment)   |&gt;\n  hypothesize(null = \"independence\")  |&gt;\n  generate(reps = 1, type = \"permute\")\nFigure 2 illustrates a single permutation. The colors show the frogs’ actual treatment group, while the x-axis shows their new, randomly permuted “treatment” group. As a result of the shuffle, each permuted group is now a mix of frogs from both original treatments.\nFigure 2: A single permutation of the frog mating data. The colors indicate the actual experimental treatment each frog received (‘preferred’ or ‘nonpreferred’). The x-axis shows the new, randomly assigned labels after one shuffle. This process of breaking the link between the treatment and the outcome (hatched eggs) is repeated many times to create a null distribution.\nStep 5: Calculate the test statistic on this permuted data set.\nIn this permuted dataset we see an observed test statistic of\none_perm |&gt; \n  calculate(stat = \"diff in means\", order = c(\"preferred\",\"nonpreferred\"))\n\nResponse: hatched.eggs (numeric)\nExplanatory: treatment (factor)\nNull Hypothe...\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  52.5\nThis represents one draw from the permuted (null) distribution. Now we do this a bunch to find the sampling distribution and calculate a p-value!",
    "crumbs": [
      "14. Shuffling",
      "• 14. Permute"
    ]
  },
  {
    "objectID": "book_sections/shuffling/permute.html#permuting-once",
    "href": "book_sections/shuffling/permute.html#permuting-once",
    "title": "• 14. Permute",
    "section": "",
    "text": "I permuted once (showing steps 4 and 5) for illustrative purposes. When we actually permute we just do them all at once. I only say this because I sometimes see students repeat this step in their analyses, and that is fully unnecessary!",
    "crumbs": [
      "14. Shuffling",
      "• 14. Permute"
    ]
  },
  {
    "objectID": "book_sections/shuffling/permute.html#the-permuted-null-distribution",
    "href": "book_sections/shuffling/permute.html#the-permuted-null-distribution",
    "title": "• 14. Permute",
    "section": "The permuted (null) distribution",
    "text": "The permuted (null) distribution\nStep 6: Repeat steps 4 and 5 many times to generate the sampling distribution under the null.\nWe generate the permuted distribution (to approximate the null sampling distribution) by increasing the number of permuted reps from 1 to a large number. In the example below, we generate 5000 permuted data sets.\n\n\nHow many permutations? There’s no strict rule for the number of permutations. It’s a trade-off between computational time and the precision of your p-value. As with bootstrapping, this is a trade-off between computational time and the precision of your p-value: more replicates are more precise but take longer to run. A good starting point is 5,000 permutations, but 1,000 is generally sufficient if the process is slow.\n\nmany_perms &lt;- frogs |&gt;\n  specify(hatched.eggs ~ treatment)   |&gt;\n  hypothesize(null = \"independence\")  |&gt;\n  generate(reps = 5000, type = \"permute\")|&gt;\n  calculate(stat = \"diff in means\", order = c(\"preferred\",\"nonpreferred\"))",
    "crumbs": [
      "14. Shuffling",
      "• 14. Permute"
    ]
  },
  {
    "objectID": "book_sections/shuffling/permute.html#the-p-value",
    "href": "book_sections/shuffling/permute.html#the-p-value",
    "title": "• 14. Permute",
    "section": "The p-value",
    "text": "The p-value\nStep 7: Calculate a p-value as the proportion of permuted values that are as or more extreme than what was observed in the actual data.\nNow we can find the p-value with infer’s get_p_value() function. This function requires three arguments:\n\nx: The permuted distribution.\n\nobs_stat: The observed test statistic. Note that I just pasted in our value of -68.82. This isn’t the best practice. I usually assign the observed value directly to a variable in R (to avoid rounding issues induced by copy and pasting).\n\ndirection: Use \"two-sided\" for a two tailed test.\n\n\n\nCode to visualize the permuted distribution and our test stat.\nmany_perms |&gt;\n  mutate(as_or_more_extreme = abs(stat) &gt;= 68.8)|&gt;\n  ggplot(aes(x = stat, fill = as_or_more_extreme))+\n  geom_histogram()+\n  scale_fill_manual(values = c(\"darkgrey\",\"firebrick\"))+\n  geom_vline(xintercept = c(68.8,-68.8), color = \"red\",lty = 2, linewidth = 1)+\n  labs(x = \"Difference in means\", fill = \"As or more extreme\", title = \"Permuted distribution\")+\n  theme(axis.text.x = element_text(size = 30),\n        axis.title.x = element_text(size = 30),\n        axis.text.y = element_blank(),\n        axis.title.y = element_blank(), \n        legend.title =  element_text(size = 30),\n        legend.text =  element_text(size = 30), \n        title = element_text(size = 30),\n        legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nFigure 3: The null distribution generated from 5,000 permutations of the frog data. This histogram shows the difference in means we would expect to see if there were no true effect of mate preference. Our observed difference of -68.8 (marked by the dashed line) falls well within the typical range of results expected under the null hypothesis. The p-value is the proportion of permuted results (the red tails) that are as or more extreme than our observed statistic.\n\n\n\n\n\nget_p_value(x = many_perms, obs_stat = -68.82, direction = \"two-sided\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.318\n\n\n\nThe get_p_value() gets the two-tailed p-value by multiplying the tail we’re looking at by two, rather than summing each side on its own. For symmetric distributions this is fine. For asymmetric sampling distributions, this requires more thought, and its probably righter to add up both sides. But this is probably close enough… we’re not trying to land a rocket ship here.",
    "crumbs": [
      "14. Shuffling",
      "• 14. Permute"
    ]
  },
  {
    "objectID": "book_sections/shuffling/permute.html#interpret-the-p-value",
    "href": "book_sections/shuffling/permute.html#interpret-the-p-value",
    "title": "• 14. Permute",
    "section": "Interpret the p-value",
    "text": "Interpret the p-value\nStep 8: Interpret the p-value.\nOur p-value is roughly 0.33 (Figure 3). This means that roughly one out of every three permuted data sets in which the association between mating treatment and egg number was randomly associated, generate a difference in means as or more extreme than the one we observed.\nTherefore, we fail to reject the null hypothesis, as we do not have enough evidence to support the alternative hypothesis that the number of eggs hatched differs between preferred and non-preferred matings.",
    "crumbs": [
      "14. Shuffling",
      "• 14. Permute"
    ]
  },
  {
    "objectID": "book_sections/shuffling/permute.html#a-write-up",
    "href": "book_sections/shuffling/permute.html#a-write-up",
    "title": "• 14. Permute",
    "section": "A write up",
    "text": "A write up\nStep 9: Write up the results. The final step is to write-up our results:\nIn this study, we tested whether the number of hatched eggs differed between wood frogs that mated with preferred versus non-preferred females. Using a permutation test, we evaluated the null hypothesis that the number of eggs hatched does not differ between these two groups.\nOn average, frogs that mated with their preferred partner hatched 345.11 eggs, while those that mated with their non-preferred partner hatched 413.93 eggs. The observed difference in the number of eggs hatched between the two groups was -68.82 eggs.\nTo assess the significance of this observed difference, we generated a null distribution of the differences in egg counts by permuting the treatment labels (preferred vs. non-preferred). After running 5,000 permutations, we calculated a two-tailed p-value of 0.3176. This indicates that in a world where there is no true association between mating treatment and the number of eggs hatched, we would expect to see a difference as extreme as the observed one in about one in three cases due to random chance alone. We therefore fail to reject the null hypothesis. This suggests that there is insufficient evidence to support the claim that the number of hatched eggs is influenced by whether the male mated with a preferred or non-preferred female.",
    "crumbs": [
      "14. Shuffling",
      "• 14. Permute"
    ]
  },
  {
    "objectID": "book_sections/shuffling/permute_nonindependent.html",
    "href": "book_sections/shuffling/permute_nonindependent.html",
    "title": "• 14. Structured Permutation",
    "section": "",
    "text": "The problem of nonindependence\nPermutation can solve many statistical problems. But when our data is complex (e.g., involving more than one explanatory variable), we need to carefully consider how to shuffle the data to ensure that we’re testing the hypotheses we’re interested in, and not inadvertently introducing confounding factors.\nThis, of course, is a standard challenge for most statistical tests. When we test a null hypothesis of ‘no association,’ we want to ensure that any effect we find is due to our main explanatory variable, not a confounding factor. Unfortunately statistical tests won’t do this for us by magic, we need to build a null sampling distribution that reflects the structure of our data.\nyear\nnonpreferred\npreferred\n\n\n\n\n2010\n15\n15\n\n\n2013\n14\n12\nFor example, in our frog experiment, the data were collected over two years, and the design was slightly unbalanced. That is, there were more non-preferred couples than preferred ones in 2013. This imbalance makes year a confounding factor, meaning our previous p-value might not be accurate, as the difference between years could influence our results.\nIndeed, the mean number of hatched eggs differs substantially across the two years, which could confound our analysis.\nCode\nfrogs|&gt;\n    ggplot(aes(x = treatment, \n               y = hatched.eggs, \n               color = treatment,\n               shape = treatment)) +\n    geom_jitter( size = 3, alpha = .5) +\n    stat_summary(color = \"black\", show.legend = FALSE)+\n    labs(x = \"treatment\", y = \"hatched eggs\")+\n    facet_wrap(~year)\n\n\n\n\n\nThe number of hatched eggs by treatment, faceted by year. This visualization clearly reveals the confounding effect of year: egg counts are much higher overall in 2013 than in 2010. A standard permutation test would mix this strong year effect with the treatment effect, leading to an incorrect null distribution.\nThis plot reveals a classic statistical problem. It’s possible that one treatment was better within each year, but this pattern is hidden in the overall data because the treatments were not balanced across the ‘good’ and ‘bad’ years. If we permute treatment labels without considering the effect of year, our null distribution will be wrong because it won’t reflect these large differences between years.",
    "crumbs": [
      "14. Shuffling",
      "• 14. Structured Permutation"
    ]
  },
  {
    "objectID": "book_sections/shuffling/permute_nonindependent.html#the-problem-of-nonindependence",
    "href": "book_sections/shuffling/permute_nonindependent.html#the-problem-of-nonindependence",
    "title": "• 14. Structured Permutation",
    "section": "",
    "text": "library(janitor)\nfrogs %&gt;%\n  tabyl(year, treatment)",
    "crumbs": [
      "14. Shuffling",
      "• 14. Structured Permutation"
    ]
  },
  {
    "objectID": "book_sections/shuffling/permute_nonindependent.html#permuting-with-structure",
    "href": "book_sections/shuffling/permute_nonindependent.html#permuting-with-structure",
    "title": "• 14. Structured Permutation",
    "section": "Permuting with structure",
    "text": "Permuting with structure\nTo address this non-independence, we need to shuffle treatments within each year rather than across the entire dataset. This ensures that any differences we detect are not confounded by the year of data collection. With more complex datasets, this issue can become even trickier, but the same principle applies: you need to preserve the structure of the data by permuting within appropriate groups.\nTo do this, we will split our data into different years, permute within each year, and then combine the permuted data to build a null sampling distribution reflecting the structure in the data. Unfortunately infer can only get us so far. So instead I’ll take matters into our own hands!\n\nlibrary(dplyr)\nlibrary(tidyr)\n# This code creates the structured null distribution.\n# It's an advanced example; the goal is to understand the logical steps,\n# not to memorize the specific syntax.\n\n# `replicate()` runs the code inside it many times (here, 1000 times).\n# We use `simplify = FALSE` so that each result is stored in a list.\nstructured_perm &lt;- replicate(1000, simplify = FALSE, {\n  # The code inside the curly braces is what gets repeated.\n  # For each repetition, it starts with the original 'frogs' dataset.\n  frogs |&gt;\n    # First, group the data by the blocking variable, 'year'.\n    group_by(year) |&gt;\n    # This is the key permutation step.\n    # `sample(treatment)` shuffles the treatment labels *within each year*.\n    # We store these newly shuffled labels in a new column.\n    mutate(permuted_treatment = sample(treatment)) |&gt;\n    # Now that the labels are shuffled correctly, we can forget the  \n    # 'year' grouping and re-group the whole dataset by our new\n    # shuffled treatment labels to calculate the overall mean difference.\n    group_by(permuted_treatment) |&gt;\n    summarise(mean_hatched = mean(hatched.eggs, na.rm = TRUE)) |&gt;    # Calculate the mean number of eggs for each shuffled group.\n    # Change data from long to wide format to a wide format, with separate columns for the 'preferred' and 'nonpreferred' means.\n    pivot_wider(names_from = permuted_treatment,values_from = mean_hatched) |&gt;\n    mutate(diff_in_means = preferred - nonpreferred)}) |&gt; # Calculate the difference in means for this single replicate.\n  bind_rows() # stacks all replicates into a single, clean data frame.\n\n\n\nCode to make the histogram below.\npermuted_mean      &lt;- summarise(structured_perm, mean(diff_in_means))|&gt;pull()\nobserved_mean      &lt;- -68.8\ndistance_from_null &lt;- abs(observed_mean  - permuted_mean)\nright_bound        &lt;- permuted_mean + distance_from_null\n\nstructured_perm |&gt;\n  mutate(as_or_more_extreme = (diff_in_means &lt; observed_mean) |(diff_in_means &gt; right_bound))|&gt;\n  ggplot(aes(x = diff_in_means, fill = as_or_more_extreme))+\n  geom_histogram()+\n  scale_fill_manual(values = c(\"darkgrey\",\"firebrick\"))+\n  geom_vline(xintercept = c(observed_mean, right_bound), color = \"red\",lty = 2, linewidth = 1)+\n  geom_vline(xintercept = summarise(structured_perm, mean(diff_in_means))|&gt;pull())+\n  labs(x = \"Difference in means\", fill = \"As or more extreme\", title = \"Structured Permuted distribution\")+\n  theme(axis.text.x = element_text(size = 16),\n        axis.title.x = element_text(size = 16),\n        axis.text.y = element_blank(),\n        axis.title.y = element_blank(), \n        legend.title =  element_text(size = 16),\n        legend.text =  element_text(size = 16), \n        title = element_text(size = 16),\n        legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nFigure 1: The null distribution from a structured permutation, where treatment labels were shuffled only within each year. Notice that the distribution is not centered at zero. This shift occurs because we have preserved the unbalanced structure of the data (the year effect), which is now correctly reflected in our null distribution. The p-value is the proportion of results in the red tail that are as or more extreme than our observed statistic.",
    "crumbs": [
      "14. Shuffling",
      "• 14. Structured Permutation"
    ]
  },
  {
    "objectID": "book_sections/shuffling/permute_nonindependent.html#finding-the-p-value",
    "href": "book_sections/shuffling/permute_nonindependent.html#finding-the-p-value",
    "title": "• 14. Structured Permutation",
    "section": "Finding the p-value",
    "text": "Finding the p-value\nBy permuting within years, we’ve controlled for the potential confounding effect of year in our analysis. Now, let’s calculate summary statistics and find the p-value. Since we permuted within year, we no longer have to worry about the confounding effect of year!\nBut because our null distribution is no longer centered at zero (see Figure 1), we redefine” extreme” as the distance away from the mean of the null sampling distribution (rather than the distance away from zero). We then add up the area on the right and left tail to find our p-value.\n\npermuted_mean      &lt;- summarise(structured_perm, mean(diff_in_means))|&gt;pull()\nobserved_mean      &lt;- -68.8\ndistance_from_null &lt;- abs(observed_mean  - permuted_mean)\nright_bound        &lt;- permuted_mean + distance_from_null\n\nstructured_perm |&gt;\n  mutate(left_tail  = diff_in_means &lt; observed_mean,\n         right_tail = diff_in_means &gt; right_bound )|&gt;\n  summarise(prop_left_tail = mean(as.numeric(left_tail)),\n            prop_right_tail = mean(as.numeric(right_tail)),\n            p_value = prop_left_tail + prop_right_tail ) |&gt;\n  select(p_value)\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.214\n\n\nThis more appropriate p-value is still greater than the conventional \\(\\alpha\\) threshold of 0.05. This means that even after accounting for the confounding effect of year, we still do not have enough evidence to reject the null hypothesis.",
    "crumbs": [
      "14. Shuffling",
      "• 14. Structured Permutation"
    ]
  },
  {
    "objectID": "book_sections/shuffling/shuffling_summary.html",
    "href": "book_sections/shuffling/shuffling_summary.html",
    "title": "• 14. Shuffling summary",
    "section": "",
    "text": "Links to: Summary. Chatbot tutor. Questions. Glossary. R packages. R functions. More resources.",
    "crumbs": [
      "14. Shuffling",
      "• 14. Shuffling summary"
    ]
  },
  {
    "objectID": "book_sections/shuffling/shuffling_summary.html#shuffling_summary_chapter-summary",
    "href": "book_sections/shuffling/shuffling_summary.html#shuffling_summary_chapter-summary",
    "title": "• 14. Shuffling summary",
    "section": "Chapter summary",
    "text": "Chapter summary\nThe null hypothesis is usually something like “the true population parameter of my response variable is the same for all values of my explanatory variable” (i.e. independence). Permutation approximates the null sampling distribution by (1) randomly swapping values of the explanatory variable in our data, and (2) calculating a summary of the associations in this randomly permuted data set (3) repeating this many times. We then find a p-value by finding the proportion of this null distribution that is as or more extreme than our observation. R’s infer package has a bunch of handy tools to make permutation easier!\n\n\n\nWe Swapped Group Labels 10,000 Times…You Won’t Believe What Happened Next!\n\n– Permutation clickbait.\n\nChatbot tutor\n\nPlease interact with this custom chatbot (link here). I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you got what you needed from it.",
    "crumbs": [
      "14. Shuffling",
      "• 14. Shuffling summary"
    ]
  },
  {
    "objectID": "book_sections/shuffling/shuffling_summary.html#shuffling_summary_practice-questions",
    "href": "book_sections/shuffling/shuffling_summary.html#shuffling_summary_practice-questions",
    "title": "• 14. Shuffling summary",
    "section": "Practice Questions",
    "text": "Practice Questions\nTry these questions! By using the R environment you can work without leaving this “book”. I even pre-loaded all the packages you need!\n\nQ1) In which method is the overall mean of the entire dataset identical in every single replicate? BootstrapPermutationBothNeither\n\n\nExplanation\n\nPermutation. Because permutation only shuffles labels without changing the underlying numbers, the sum (and therefore the mean) of the entire dataset is constant across all replicates. Bootstrapping changes the data values in each replicate, so the overall mean will vary.\n\n\nQ2) Which method generates a confidence interval by simulating sampling error around an observed statistic? BootstrapPermutationBothNeither\n\n\nExplanation\n\nBootstrap. Bootstrapping is used to estimate the uncertainty of a sample statistic, which is what a confidence interval represents. It answers the question, “How precise is my estimate?”\n\n\nQ3) Which method is primarily used to generate a p-value by simulating a null hypothesis? BootstrapPermutationBothNeither\n\n\nExplanation\n\nPermutation. Permutation testing directly simulates the null hypothesis by breaking the association between variables. This process is used to calculate a p-value to answer the question, “How likely is my result if the null hypothesis is true?”\n\n\nQ4) For a bootstrap focussing on differences in mean body mass between male and female penguins, are the following statements True or False?\n\n4a: The male mean will be identical in every replicate. TRUEFALSE.\n\n4b: The overall mean will be identical in every replicate. TRUEFALSE.\n\n4c: The bootstrap distribution of the sex difference in body mass will be centered around the original estimate. TRUEFALSE.\n\n4d: The bootstrap distribution of the sex difference in body mass will be centered around the value proposed by the null model (e.g. 0). TRUEFALSE.\n\n\n\nOverall Explanation\n\nExplanation: Bootstrapping resamples the original data with replacement. This means the data values in each replicate are different, so the means for subgroups and the overall sample will vary. The entire purpose of bootstrapping is to simulate sampling error around the observed sample statistic to see how precise it is.\n\n\nQ5) For a permutation focusing on differences in mean body mass between male and female penguins, are the following statements True or False?\n\n5a: The male mean will be identical in every replicate. TRUEFALSE.\n\n5b: The overall mean will be identical in every replicate. TRUEFALSE.\n\n5c: The permuted distribution of the sex difference in body mass will be centered around the original estimate. TRUEFALSE.\n\n5d: The permuted distribution of the sex difference in body mass will be centered around the value proposed by the null model (e.g. 0). TRUEFALSE.\n\n\n\nOverall Explanation\n\nExplanation: A permutation test only shuffles the labels (e.g., “male” or “female”) without changing the underlying data values. Because the set of numbers is always the same, the overall mean is constant in every replicate. This shuffling process breaks the link between the labels and the data, creating a null distribution that is centered on the value expected under the null hypothesis.\n\n\n\nThe penguin data:\nSexual dimorphism - that is, differences in size, shape, behavior etc. between sexes is common in animals and can reveal a great deal about a species. The Adelie penguin has a whole bunch of “interesting” sexual behavior (e.g. this note), and it is worth exploring sex difference in their phenotypes.\nThe penguins data set (loaded) below – a fun data set provided by the palmerpenguins package contains some basic data on Adelie penguins and other penguin species. The code below loads in the packages and data we need to explore these guys!\n\n# Loading libraries\nlibrary(palmerpenguins)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(infer)\n\nThe code below builds on the quick glimpse() of the data (above) by demonstrating how to filter() the data and get numeric summaries by group. This refresher should help prepare you for the the next tasks\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nModify the code above to find the mean body_mass_g of Adelie penguins from Torgersen island for each sex.\nQ6) The mean body mass of Adelie penguins from Torgersen island is  g more larger than females.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nFiltering data or Q7 - Q9:\nadelie_torgersen &lt;- penguins             |&gt;\n  filter(species==\"Adelie\", \n         island==\"Torgersen\", \n         !is.na(body_mass_g), \n         !is.na(sex))\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ7) Once you fill in the blanks, the code below finds the same answer for the sex difference in body mass using infer’s syntax. The blanks should be:\n\nA: .\n\nB: .\n\nC: .\n\nD: .\n\nE: .\n\n\n\nQ7 Hint\n\n\nHint A: Should be your response variable.\n\n\nAnswer\n\nIn this case “body_mass_g”\n\nHint B: Should be your explanatory variable.\n\n\nAnswer\n\nIn this case “sex”\n\nHint C: Should be your stat.\n\n\nAnswer\n\nIn this case “diff in means”\n\nHint D: The first group in the subtraction order (group1 - group2)\n\n\nAnswer\n\nIn this case “male”\n\nHint E: The second group in the subtraction order (group1 - group2).\n\n\nAnswer\n\nIn this case “female”\n\n\n\n\nQ8) Modify the code above to find the lower bound of the bootstrap 95% CI of the sex difference in body mass in grams from 5000 bootstrap replicates .\n\n\nQ8 Hint\n\nStart with your pipeline from Q7, then:\n\nUse infer’s generate() function to generate reps bootstrap replicates.\n\nRemember to say that type = \"bootstrap\".\n\nRemember this goes before calculate().\n\nUse infer’s get_ci() function to get the confidence interval.\n\nRemember this goes after calculate().\n\n\n\n\nJust show the code already\n\n\nadelie_torgersen                            |&gt;     \n  specify( A ~ B)                           |&gt; # Fill in with answers from above\n  generate(reps = 5000, type = \"bootstrap\") |&gt; # Makes the bootstrap\n  calculate(stat = \"C\", order = c(D,E))     |&gt; # Fill in with answers from above \n  get_ci(level = 0.95, type = \"percentile\")    # Get CI (remember the question asks about the lower bound)\n\n\n\n\nQ9) What is the null hypothesis for this example?\n\n Male Adelie penguins are heavier, on average, than female Adelie penguins. There is no difference in the mean body mass between male and female Adelie penguins. There is a difference in the mean body mass between male and female Adelie penguins. The observed difference in our sample is zero.\n\n\nQ10) What is the alternative hypothesis for this example?\n\n Male Adelie penguins are heavier, on average, than female Adelie penguins. There is no difference in the mean body mass between male and female Adelie penguins. There is a difference in the mean body mass between male and female Adelie penguins. The observed difference in our sample is zero.\n\n\nQ11) Modify the code above to find the permutation-base p-value (from 5000 permutations).How would you report it? 0&lt; 3/5000NA0.02Between 0.02 and 0.05&gt;0.051/Inf\n\n\nQ11 Hint\n\nStart with your pipeline from Q8, then:\n\nUse infer’s hypothesize() to state the null.\n\nRemember this goes after specify().\n\nIn this case null = \"independence\".\n\nModify the line with generate(), so that now type = \"permute\".\n\nRemember this goes after hypothesize().\n\nReplace get_ci() with get_p_value().\n\nRemember \"obs_stat = 639\". Actually this is lazy. the better thing to do is calculate this, pull() it, and assign it to variable in R. This will make your answer more exact because the permutation will understand ties correctly.\ndirection = \"two-sided\"\n\n\n\n\nJust show the code already\n\n\nadelie_torgersen                            |&gt;     \n  specify( A ~ B)                           |&gt; \n  generate(reps = 5000, type = \"permute\")   |&gt; \n  calculate(stat = \"C\", order = c(D,E))     |&gt; \n  get_p_value(obs_stat = 639, direction = \"two-sided\")   # Get CI (remember the question asks about the lower bound)\n\n\n\n\nWhat the he🏒🏒? Why is the answer from R wrong? A-hole\n\nIf you did this right, you got a p-value of 0 with the warning:\n\nWarning: Please be cautious in reporting a p-value of 0. This result is an approximation based on the number of reps chosen in the generate() step. ℹ See get_p_value() (?infer::get_p_value()) for more information.\n\nIf you follow the link to get_p_value() it has a section, Zero p-value, which says:\nThough a true p-value of 0 is impossible, get_p_value() may return 0 in some cases. This is due to the simulation-based nature of the {infer} package; the output of this function is an approximation based on the number of reps chosen in the generate() step. When the observed statistic is very unlikely given the null hypothesis, and only a small number of reps have been generated to form a null distribution, it is possible that the observed statistic will be more extreme than every test statistic generated to form the null distribution, resulting in an approximate p-value of 0. In this case, the true p-value is a small value likely less than 3/reps (based on a poisson approximation).\n\n\n\nQ12) What do we do to the null hypothesis? Reject itFail to reject itAccept itFail to accept it\n\nQ13) We can’t KNOW if the null hypothesis is true or false, but which do you think is far more likely?)\n\n Null hypothesis is true Null hypothesis is false he null hypothesis is equally likely to be TRUE or FALSE\n\n\nQ14) An ecologist studies the effect of sunlight on fish growth. They measure the body length of fish from two habitat types (‘sunny’ and ‘shady’) in five different ponds. A standard permutation test would be invalid because fish from the same pond are not independent.\nHow should they correctly shuffle the data to test for a difference between habitats?\n\n Shuffle all habitat labels ('sunny', 'shady') across all fish from all ponds. Shuffle the habitat labels only among the fish within each individual pond. Shuffle the pond labels ('Pond 1', 'Pond 2', etc.) across all the fish. The data cannot be permuted because it violates the assumption of independence.\n\n\n\nExplanation\n\nExplanation: This is a classic example of structured or “blocked” data. Fish within the same pond are more similar to each other than to fish in other ponds (a “pond effect”). To test the habitat effect without it being confounded by the pond effect, you must shuffle the habitat labels within each pond. This breaks the link between habitat and fish size while correctly preserving the underlying structure of the data.\n\n\nQ15) The logic of permutation testing is very flexible. For which of the following null hypotheses could you generate a null distribution by shuffling one variable relative to another?\n\n (A) The slope of a regression line between penguin bill depth and bill length is zero. (B) There is no association between penguin species and their preferred island. (C) The mean body mass of a single sample of penguins is 3500g. Both A and B. Both A and C. Both B and C. All of the above. None of the above.",
    "crumbs": [
      "14. Shuffling",
      "• 14. Shuffling summary"
    ]
  },
  {
    "objectID": "book_sections/shuffling/shuffling_summary.html#shuffling_summary_glossary-of-terms",
    "href": "book_sections/shuffling/shuffling_summary.html#shuffling_summary_glossary-of-terms",
    "title": "• 14. Shuffling summary",
    "section": "📊 Glossary of Terms",
    "text": "📊 Glossary of Terms\n\n\nBootstrap Distribution: The distribution of a statistic (e.g., the mean) calculated from a large number of bootstrap samples. It’s used to approximate the sampling distribution.\nBootstrap Replicate (or Bootstrap Sample): A new sample of the same size as the original, created by randomly drawing observations from the original sample with replacement.\nBootstrap Standard Error: The standard deviation of the bootstrap distribution, which serves as an estimate of the standard error of an estimate.\nBootstrapping: A computational resampling method that approximates the sampling distribution by repeatedly taking samples with replacement from the original data.\nConfidence Interval (CI): A range of plausible values for an unknown population parameter, calculated from sample data. For example, a 95% confidence interval is generated by a process that is expected to capture the true parameter 95% of the time.\nConfidence Level: Reasonable bounds we put around our estimate to acknowledge sampling error’s impact on it.\nSampling with Replacement: A sampling process where each selected item is returned to the pool before the next item is drawn, meaning an individual can be selected more than once. This is the core mechanism of bootstrapping.\nSampling without Replacement: A sampling process where a selected item is not returned to the pool, ensuring that all items in the final sample are distinct. This is how traditional statistical samples are taken.",
    "crumbs": [
      "14. Shuffling",
      "• 14. Shuffling summary"
    ]
  },
  {
    "objectID": "book_sections/shuffling/shuffling_summary.html#shuffling_summaryR_packages_introduced",
    "href": "book_sections/shuffling/shuffling_summary.html#shuffling_summaryR_packages_introduced",
    "title": "• 14. Shuffling summary",
    "section": "R Packages Introduced",
    "text": "R Packages Introduced\n\nThere a re no new packages, but we continue to use infer: this time for permutation.",
    "crumbs": [
      "14. Shuffling",
      "• 14. Shuffling summary"
    ]
  },
  {
    "objectID": "book_sections/shuffling/shuffling_summary.html#shuffling_summary_key-r-functions",
    "href": "book_sections/shuffling/shuffling_summary.html#shuffling_summary_key-r-functions",
    "title": "• 14. Shuffling summary",
    "section": "🛠️ Key R Functions",
    "text": "🛠️ Key R Functions\n\nThe infer pipeline\n\nspecify(): Used to declare the response and explanatory variables in an analysis (e.g., specify(body_mass_g ~ sex)).\nhypothesize(): Used to state the null hypothesis for a permutation test (e.g., hypothesize(null = \"independence\")).\ngenerate(): The core resampling function. Used to create bootstrap replicates (type = \"bootstrap\") or permuted replicates (type = \"permute\").\nget_p_value(): Calculates a p-value by comparing an observed statistic to a null distribution generated by permutation.",
    "crumbs": [
      "14. Shuffling",
      "• 14. Shuffling summary"
    ]
  },
  {
    "objectID": "book_sections/shuffling/shuffling_summary.html#shuffling_summary_additional-resources",
    "href": "book_sections/shuffling/shuffling_summary.html#shuffling_summary_additional-resources",
    "title": "• 14. Shuffling summary",
    "section": "Additional resources",
    "text": "Additional resources\n\nReadings:\n\nResampling-based methods for biologists - Fieberg et al. (2020).\nChapter 9: Hypothesis Testing from (Ismay & Kim, 2019).\nThe Permutation Test: A Visual Explanation of Statistical Testing.\nPermutation tests for hypothesis testing with animal social network data: Problems and potential solutions Farine & Carter (2022).\nCommon permutation methods in animal social network analysis do not control for non-independence - Hart et al. (2022).\nThe benefits of permutation-based genome-wide association studies John et al. (2024).\n\n\n\n\n\n\nFarine, D. R., & Carter, G. G. (2022). Permutation tests for hypothesis testing with animal social network data: Problems and potential solutions. Methods in Ecology and Evolution, 13(1), 144–156. https://doi.org/https://doi.org/10.1111/2041-210X.13741\n\n\nFieberg, J. R., Vitense, K., & Johnson, D. H. (2020). Resampling-based methods for biologists. PeerJ, 8, e9089. https://doi.org/10.7717/peerj.9089\n\n\nHart, J. D. A., Weiss, M. N., Brent, L. J. N., & Franks, D. W. (2022). Common permutation methods in animal social network analysis do not control for non-independence. Behavioral Ecology and Sociobiology, 76(11), 151.\n\n\nIsmay, C., & Kim, A. Y. (2019). Statistical inference via data science: A ModernDive into r and the tidyverse. CRC Press.\n\n\nJohn, M., Korte, A., & Grimm, D. G. (2024). The benefits of permutation-based genome-wide association studies. Journal of Experimental Botany, 75(17), 5377–5389. https://doi.org/10.1093/jxb/erae280",
    "crumbs": [
      "14. Shuffling",
      "• 14. Shuffling summary"
    ]
  },
  {
    "objectID": "book_sections/chi2.html",
    "href": "book_sections/chi2.html",
    "title": "14-B. χ2",
    "section": "",
    "text": "Motivating Scenario:\nWe want more practice with the idea of permuting and hope to connect null distributions generated by simulation (in general), and (specifically) permutation to those using mathematical formulae.\nLearning Goals: By the end of this chapter, you should be able to:\n\nKnow that computational tools like can be used to generate null distributions. Specifically, we can\n\nSimulate (with sample() to generate the null distribution of expected counts to evaluate the “goodness of fit”.\n\nPermute, (with infer) to generate the null distribution for associations between categorical variables.\n\n\nDescribe how these computationally generated nulls match the theoretical \\(\\chi^2\\) distribution, and why that’s useful.\n\nUse R to perform and interpret \\(\\chi^2\\) tests.\n\n\n\nFundamentally, a p-value quantifies the idea of some outcome being “unexpected.” P-values work by comparing our observed value of some test statistic to its expected distribution under the null hypothesis.\nIn the previous chapter, we generated a null sampling distribution by shuffling (or “permuting”) the relationship between the explanatory and response variables. Here, we will both introduce how to simulate to generate a null sampling distribution or “expected counts”, and how we can permute to generate a null for associations between categorical variables.\nWe begin by introducing the \\(\\chi^2\\) statistic itself — a test statistic that quantifies the difference between expected and observed counts. We will work through this concept with a “goodness of fit” example, in which we see if data in categories are consistent with their null distribution. We then use the familiar permutation approach from the previous chapter to explore the distribution of this test statistic under the null.\nWe then show that these computationally-derived distribution match the mathematical \\(\\chi^2\\) distribution. We can therefore use this distribution to test whether count data are truly “unexpected” under the null hypothesis without simulating or permuting the data! This section bridges our permutation-based intuition for a null distribution to analytical shortcuts used to generate null distributions with less computational effort.\nWe’ll use this test to see if we can reject the idea that students in class picked numbers at random, and to evaluate whether pink and white flowers are equally likely to receive zero pollinator visits. This statistical model gets at the motivating biological question: does petal color influence pollinator visitation (and perhaps hybridization rates)?",
    "crumbs": [
      "14-B. χ2"
    ]
  },
  {
    "objectID": "book_sections/chi2/expectations_and_differences.html",
    "href": "book_sections/chi2/expectations_and_differences.html",
    "title": "• 14B. What to expect",
    "section": "",
    "text": "\\(\\chi^2\\) quantifies the deviation from expectation.\nIf I told a class of forty-one students to pick an integer between one and ten at random, we would expect the following:\nBut, of course, we never get exactly what we expect. This difference between expectation and observation can be due to both sampling error (because our sample was finite) and sampling bias (because people don’t really pick numbers at random).\nFigure 1 below shows the number of times each number (one through ten) was chosen by a student in my course. Because these results come from students, the deviation can be attributed to some combination of sampling error and sampling bias.\nHow can we summarize the multidimensional view in Figure 1 into a single summary statistic? While there are many potential options (e.g., the number of times the most common number appears, the difference in counts between odds and evens, etc.). \\(\\chi^2\\) – the sum of squared differences between observed and expected counts in each category divided by the expected counts in each category – is the most common summary.\n\\[\\chi^2 = \\sum{\\frac{(\\text{Observed}_i - \\text{Expected}_i)^2}{\\text{Expected}_i}}\\]\nquantity\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nexpected\n4.100\n4.100\n4.10\n4.100\n4.100\n4.10\n4.100\n4.100\n4.100\n4.100\n\n\nobserved\n1.000\n4.000\n6.00\n3.000\n3.000\n8.00\n7.000\n3.000\n3.000\n3.000\n\n\n\\(\\text{expected} - \\text{observed}\\)\n3.100\n0.100\n-1.90\n1.100\n1.100\n-3.90\n-2.900\n1.100\n1.100\n1.100\n\n\n\\((\\text{expected} - \\text{observed})^2\\)\n9.610\n0.010\n3.61\n1.210\n1.210\n15.21\n8.410\n1.210\n1.210\n1.210\n\n\n\\(\\frac{(\\text{expected} - \\text{observed})^2}{\\text{expected}}\\)\n2.344\n0.002\n0.88\n0.295\n0.295\n3.71\n2.051\n0.295\n0.295\n0.295\nSo in our case, \\(\\chi^2 = 2.34 + 0.00244 + 0.880+ 0.295 + 0.295 + 3.71 + 2.05 +0.295+ 0.295 +0.295\\) \\(\\chi^2 = 10.462\\).",
    "crumbs": [
      "14-B. χ2",
      "• 14B. What to expect"
    ]
  },
  {
    "objectID": "book_sections/chi2/expectations_and_differences.html#quantifying-exceptionality-of-deviations",
    "href": "book_sections/chi2/expectations_and_differences.html#quantifying-exceptionality-of-deviations",
    "title": "• 14B. What to expect",
    "section": "Quantifying exceptionality of deviations",
    "text": "Quantifying exceptionality of deviations\nI literally have no idea what a \\(\\chi^2\\) of ten means. Ten sounds like a big number, but what do I know???\nOne issue is that χ² depends on sample size: with more data, even tiny deviations from expectation can give you a large χ² value. So we need to quantify how large the deviation itself is, independent of how many observations we have.\nFortunately, our friend Cohen came up with a measure of the “effect size” for this case. Cohen’s w measures the overall departure from the expected pattern in a standardized way:\n\\(\\text{Cohen’s }w = \\sqrt{\\frac{\\chi^2}{n}}\\)\nSo for our case, \\(\\text{Cohen's w} = \\frac{10.462}{41} = \\sqrt{\\frac{1}{4}}=\\frac{1}{2}\\). This is a borderline large effect!\n\n\nInterpreting the “effect size” of Cohen’s w.\n\n\n\nEffect size\nw\n\n\n\n\nSmall\n0.10\n\n\nMedium\n0.30\n\n\nLarge\n0.50\n\n\n\nSo in this case, even though our sample isn’t huge, the pattern of student “random” number choices differs substantially from the uniform expectation. Cohen’s w helps us separate how interesting the pattern is from how certain we are about it.",
    "crumbs": [
      "14-B. χ2",
      "• 14B. What to expect"
    ]
  },
  {
    "objectID": "book_sections/chi2/expectations_and_differences.html#quantifying-surprisingness-of-deviations",
    "href": "book_sections/chi2/expectations_and_differences.html#quantifying-surprisingness-of-deviations",
    "title": "• 14B. What to expect",
    "section": "Quantifying surprisingness of deviations",
    "text": "Quantifying surprisingness of deviations\nEffect size is important, but we also want to know if our observations can be easily attributable to sampling error. This is where we turn to NHST.\nTo conduct this (or any) NHST, we compare our observed test statistic to its expected distribution under the null hypothesis. Because we do not have paired observations to permute, I introduce simulation as a different computational approach to generate a null sampling distribution.\n\n\nLater in this chapter we will see that the math for this sampling distribution is also worked out , so we don’t need to simulate, but I hope this helps us understand.\nTo start with, we can randomly select an integer from one to ten forty-one times to generate a single sample:\n\nsome_numbers &lt;- sample(1:10,size = 41, replace = TRUE) # generate a random sample\n\n\nFigure 2 shows that our sample differs from one random sample from the sampling distribution.\n\n\nCode\n# to find chi2\nobs &lt;- table(some_numbers)\nexpected &lt;- rep(41/10, 10)  # expected counts under uniform distribution 4.1 for each bin\nchi2 &lt;- sum((obs - expected)^2 / expected) # calculate chi-squared statistic\n\n\n\nrand_num &lt;- tibble(rand_num = some_numbers) # put these in a tibble for plotting\n\nggplot(rand_num, aes(x=rand_num))+\n    geom_bar()+\n    scale_x_continuous(breaks = 1:10)+\n    geom_hline(yintercept = 0:max(table(pull(rand_num))), color = \"white\")+\n    geom_hline(yintercept = 4.1, color = \"blue\")+\ngeom_point(data = peoples_numbers|&gt;group_by(numbers)|&gt;tally(),\naes(x = numbers, y = n), color = \"firebrick\", size = 4)+\nlabs(title=\"Comparing random answers, to students \\'randomly\\' choosing numbers\")+\nannotate(x = 3, y = 9.25,label = paste(\"χ² random data = \" , round(chi2, 2)), geom = \"text\",size= 6)+\nannotate(x = 3, y = 7.5 ,label = \"χ² student data = 10.46\" , geom = \"text\",size= 6, color = \"firebrick\")+\ncoord_cartesian(ylim = c(0,10))\n\n\n\n\n\n\n\n\nFigure 2: Observed counts of numbers 1–10 as provided by 41 random answers from R. Each number has an expected count of 4.1 (blue horizontal line) under a uniform distribution, but sampling alone causes observed frequencies to deviate from this expectation. The red points display student responses.\n\n\n\n\n\n\nBut, of course, that is just comparing one sample to another. To get a p-value, we find the proportion of the sampling distribution as or more extreme than what we have seen. Figure 3 shows that about one quarter of the 50 random samples have a \\(\\chi^2\\) value more extreme than what we observed in our data. Thus, we will fail to reject the null hypothesis.\n\n\nThis is a one-tailed test because the \\(\\chi^2\\) statistic incorporates all ways to be weird.\n\n\n\n\n\n\n\n\nFigure 3: Animation comparing a computer’s random number choices (bars) to students’ “random” choices (red dots) over repeated trials. The blue horizontal line shows the expected count for each number if choices were perfectly uniform. Each frame represents a new simulated dataset for the computer.\n\n\n\n\n\n\nTo find a more precise p-value, we will conduct one thousand such simulations. Figure 4 reveals that 29% of the null sampling distribution has \\(\\chi^2\\) values greater than what we saw in class, so our p-value is 0.29, and we fail to reject the null hypothesis that people chose numbers at random.\n\n\n\n\n\n\n\n\nFigure 4: Sampling distribution of χ² statistics under the null model, generated by repeated random simulations. Each bar shows how often a particular χ² value arose when the data truly were generated under the null. The red vertical line marks the χ² value observed in our actual data.",
    "crumbs": [
      "14-B. χ2",
      "• 14B. What to expect"
    ]
  },
  {
    "objectID": "book_sections/chi2/expectations_and_differences.html#interpretation",
    "href": "book_sections/chi2/expectations_and_differences.html#interpretation",
    "title": "• 14B. What to expect",
    "section": "Interpretation",
    "text": "Interpretation\nWe fail to reject the null. This means that sampling error can easily explain the difference between our observations and expectations. But this does not mean that people chose numbers at random it simply means that we don’t have enough evidence to argue against this skeptical “null hypothesis.”\nhttps://www.youtube.com/watch?v=7_cs1YlZoug",
    "crumbs": [
      "14-B. χ2",
      "• 14B. What to expect"
    ]
  },
  {
    "objectID": "book_sections/chi2/associations_between_catvars.html",
    "href": "book_sections/chi2/associations_between_catvars.html",
    "title": "• 14B. \\(\\chi^2\\) Two Cats",
    "section": "",
    "text": "Quantifying associations\nWe are often interested to know if two variables are associated. The \\(\\chi^2\\) statistic is useful when examining associations between two categorical variables. For example, we can calculate a deviation from expected counts to see if pink-flowered parviflora RILs are more likely to receive a pollinator than are white-flowered parviflora RILs.\nAs you may recall, if two categorical outcomes are independent, the probability of both occurring together equals the product of their individual probabilities. For example, if these two variables are independent, we would expect the probability that a plant is both pink-flowered and visited by a pollinator to equal the proportion of pink flowers multiplied by the proportion of flowers receiving a pollinator visit. We can quantify the association between two binary variables as:\nSo, for our example, the covariance between pink flowers and receiving a pollinator visit among parviflora RILs at site Sawmill Road is 0.0502, and the correlation is 0.310.\nsr_rils|&gt;\n    filter(!is.na(petal_color), !is.na(visited))|&gt;\n    summarise(n            = n(), \n        p_visited          = mean(visited),\n        p_pink             = mean(petal_color== \"pink\"),\n        p_pink_and_visited = mean(visited &petal_color== \"pink\" ),\n        cov_pink_visited   = p_pink_and_visited - p_pink * p_visited,\n        cor_pink_visited   = cov_pink_visited / (sd(visited)* sd(petal_color==\"pink\")))\n\n# A tibble: 1 × 6\n      n p_visited p_pink p_pink_and_visited cov_pink_visited cor_pink_visited\n  &lt;int&gt;     &lt;dbl&gt;  &lt;dbl&gt;              &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1   103     0.883  0.515              0.505           0.0502            0.310\nWe can estimate the uncertainty in this value using bootstrapping. To do so, I will convert these binary variables to 0/1 so R treats them as numeric.\nlibrary(infer)\nsr_rils &lt;- sr_rils|&gt;\n  filter(!is.na(petal_color), !is.na(visited))|&gt;\n  mutate(pink_01 = as.numeric(petal_color == \"pink\"),\n         visited_01 = as.numeric(visited))\n\n# find the bootstrap confidence interval\nsr_rils |&gt;\n    specify(visited_01 ~ pink_01) |&gt;\n    generate(reps = 50000, type = \"bootstrap\") |&gt;\n    calculate(stat = \"correlation\")  |&gt;\n    get_ci(level = 0.95)\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1    0.157    0.442\nThis bootstrap-based 95% confidence interval does not cross zero, suggesting that we should reject the null hypothesis of no association.\nTo formally evaluate this idea using null hypothesis significance testing (NHST), we need:\nAs you can see by permutation, using our observed correlation as the test statistic, this suggestion is correct. More precisely, we find a p-value of 0.0027. We therefore reject the null hypothesis.\nsr_rils |&gt;\n    specify(visited_01 ~ pink_01) |&gt;\n    hypothesize(null = \"independence\") |&gt; \n    generate(reps = 10000, type = \"permute\") |&gt;\n    calculate(stat = \"correlation\")  |&gt;\n    get_p_value(0.310,direction = \"both\")\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1 0.00272",
    "crumbs": [
      "14-B. χ2",
      "• 14B. $\\chi^2$ Two Cats"
    ]
  },
  {
    "objectID": "book_sections/chi2/associations_between_catvars.html#quantifying-associations",
    "href": "book_sections/chi2/associations_between_catvars.html#quantifying-associations",
    "title": "• 14B. \\(\\chi^2\\) Two Cats",
    "section": "",
    "text": "The covariance: \\(Cov_{A,B} = P_{AB} -P_A \\times P_B\\).\n\nThe correlation: \\(r_{A,B}=\\frac{Cov_{A,B}}{s_A,s_B}\\).\n\n\n\nThis formula is approximately right, but to get the answer exactly we would need to introduce Bessel’s correction (i.e. the denominator should be \\(n-1\\), not \\(n\\)). For now, this is good enough.\nThis formula is approximately correct, but to get the exact value we’d need to include Bessel’s correction—that is, divide by n – 1 instead of n (we could achieve this by multiplying by \\(n /(n-1)\\). For our purposes, this simpler version is good enough to capture the idea.\n\n\n\n\n\n\n\nA test statistic to measure,\n\nA way to generate the sampling distribution of this test statistic under the null hypothesis of no association.",
    "crumbs": [
      "14-B. χ2",
      "• 14B. $\\chi^2$ Two Cats"
    ]
  },
  {
    "objectID": "book_sections/chi2/associations_between_catvars.html#contingent-expectations",
    "href": "book_sections/chi2/associations_between_catvars.html#contingent-expectations",
    "title": "• 14B. \\(\\chi^2\\) Two Cats",
    "section": "Contingent expectations",
    "text": "Contingent expectations\nEverything above is 100% legit. We can conduct a permutation test on any test statistic. However, as a bridge to the idea of an analytically derived null distribution, let’s calculate a \\(\\chi^2\\) value as our test statistic:\n\\[\\chi^2 = \\Sigma{\\frac{(\\text{observed}_i - \\text{expected}_i)^2}{\\text{expected}_i}}\\]\n\n\nNote that while \\(\\chi^2\\) is a useful test statistic, it does not convey effect size, so it should be used for hypothesis testing rather than as a summary measure.\nOBSERVATIONS\nTo start, we need the observed counts in each category:\n\nsr_rils|&gt;\n    filter(!is.na(petal_color), !is.na(visited))|&gt;\n    summarise(n_pink_visit     = sum(petal_color== \"pink\"  & visited),\n              n_white_visit    = sum(petal_color== \"white\" & visited),\n              n_pink_novisit   = sum(petal_color== \"pink\"  & !visited),\n              n_white_novisit  = sum(petal_color== \"white\" & !visited))                                           |&gt;kable()\n\n\n\n\nn_pink_visit\nn_white_visit\nn_pink_novisit\nn_white_novisit\n\n\n\n\n52\n39\n1\n11\n\n\n\n\n\n\nEXPECTATIONS\nWe then need the expected counts in each category under the null of no expected association. To do so we multiply probabilities of each outcome to find the expected proportion in each category (same intuition above), and multiply by n to turn these expected proportions into expected counts:\n\nsr_rils|&gt;\n    filter(!is.na(petal_color), !is.na(visited))|&gt;\n    summarise(n            = n(), \n        p_visited          = mean(visited),\n        p_pink             = mean(petal_color== \"pink\"),\n        pink_visit_expect  =  n * p_visited * p_pink,\n        white_visit_expect =  n * p_visited * (1-p_pink),\n        pink_novisit_expect  =  n * (1-p_visited) * p_pink,\n        white_novisit_expect  =  n * (1-p_visited) * (1-p_pink))                                           |&gt;kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\nn\np_visited\np_pink\npink_visit_expect\nwhite_visit_expect\npink_novisit_expect\nwhite_novisit_expect\n\n\n\n\n103\n0.8834951\n0.5145631\n46.82524\n44.17476\n6.174757\n5.825243\n\n\n\n\n\n\n\\(\\chi^2\\) We can then calculate \\(\\chi^2\\) as\n\\[\\frac{(52-46.8)^2}{46.8}+\\frac{(39-44.2)^2}{44.2}+\\frac{(1-6.17)^2}{6.17}+\\frac{(11-5.83)^2}{5.83} =  10.10633\\]\n\nNHST:\nWe can permute with the infer() pipeline. Note that we specify stat = \"Chisq\", and correct = FALSE. This later argument calcualtes the actual \\(\\chi^2\\) value.\n\n### permute the data 10k times\nall_perms &lt;- sr_rils|&gt;\n    specify(visited ~ petal_color,success = \"TRUE\") |&gt;  \n    hypothesize(null = \"independence\") |&gt;\n    generate(reps = 10000, type = \"permute\") |&gt;\n    calculate(stat = \"Chisq\", correct = FALSE)\n\n\n\nPlot the permutation results\nall_perms |&gt;\n  ggplot(aes(x = stat))+\n  geom_histogram(bins = 9, color = \"white\")+\n  geom_vline(xintercept = 10.1, color = \"firebrick\")+\n  annotate(x = 8,y =6000,color = \"firebrick\", geom = \"label\",   label = \"observed~chi^2\",  parse = TRUE, size = 6)+\nlabs(x = expression(chi^2), title = \"Permutation based null distribution\")\n\n\n\n\n\n\n\n\nFigure 1: Permutation-based null distribution of χ² values under the hypothesis of independence. The red vertical line marks the observed χ² statistic from the real petal color and pollinator visitation data at the Sawmill Road site.\n\n\n\n\n\nWe can use this to find the p-value. Note that we only examine the right tail of the \\(\\chi^2\\) distribution because it encompasses all possible ways to deviate from expectations. This analysis confirms our rejection of the null. It appears that pink flowers are more likely to receive pollinator visits than white flowers.\n\n# calculate the actual chi2\nobserved_chi2 &lt;- sr_rils|&gt;\n    specify(visited ~ petal_color,success = \"TRUE\") |&gt;  \n    hypothesize(null = \"independence\") |&gt;\n    calculate(stat = \"Chisq\", correct = FALSE)\n\nget_p_value(obs_stat = observed_chi2, all_perms, direction = \"right\" )\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1  0.0009",
    "crumbs": [
      "14-B. χ2",
      "• 14B. $\\chi^2$ Two Cats"
    ]
  },
  {
    "objectID": "book_sections/chi2/chi2_dist.html",
    "href": "book_sections/chi2/chi2_dist.html",
    "title": "• 14B. \\(\\chi^2\\) distribution",
    "section": "",
    "text": "The \\(\\chi^2\\) “Goodness of fit” test\nThere are two reasons why I chose to add this chapter on the \\(\\chi^2\\) tests:\nHere, I (reassuringly) show that our permutation-based and analytical approaches to NHST for the \\(\\chi^2\\) lead to the same conclusions.\nchosen\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nexpected\n4.1\n4.1\n4.1\n4.1\n4.1\n4.1\n4.1\n4.1\n4.1\n4.1\n\n\nobserved\n1.0\n4.0\n6.0\n3.0\n3.0\n8.0\n7.0\n3.0\n3.0\n3.0\nWe previously evaluated the null hypothesis that students choose an integer between one and ten at random. To do so, we:\nFigure 1: Simulation-derived null distribution of χ² distribution for our example. The red vertical line shows the observed χ² = 10.46, which falls well within the main body of the simulated distribution. Roughly 29% of simulated χ² values were as large or larger than this observed value, yielding a one-tailed p-value ≈ 0.29.\nlibrary(dplyr)\nchi2_per_sample |&gt;\n  mutate(as_or_more_extreme = chi2 &gt;= 10.462)|&gt;\n  summarise(p.val = mean(as_or_more_extreme))\n\n# A tibble: 1 × 1\n  p.val\n  &lt;dbl&gt;\n1 0.329",
    "crumbs": [
      "14-B. χ2",
      "• 14B. $\\chi^2$ distribution"
    ]
  },
  {
    "objectID": "book_sections/chi2/chi2_dist.html#the-chi2-goodness-of-fit-test",
    "href": "book_sections/chi2/chi2_dist.html#the-chi2-goodness-of-fit-test",
    "title": "• 14B. \\(\\chi^2\\) distribution",
    "section": "",
    "text": "Calculated the expected counts: the number of students we would expect to choose each number if choices were truly random. Since we had 41 observations for 10 categories with a null of “each is equally likely”, our expectation was 4.1 for each category (above).\nComputed the \\(\\chi^2\\) statistic as the sum of squared differences between observed and expected counts, divided by the expected count. In this case this equalled 10.46.\nSimulated the null distribution by repeatedly generating random choices and calculating the resulting \\(\\chi^2\\) values (Figure 1)\nCompared our observed \\(\\chi^2\\) to this simulated null distribution to obtain a p-value and conduct a null hypothesis significance test (NHST). Approximately 29.4% of our null simulation-based \\(\\chi^2\\) values were greater than or equal to our observed value of 10.46. Because the \\(\\chi^2\\) encompasses all types of deviations from null expectations, this is one-tailed test. Our p-value is 0.294, and we fail to reject the null hypothesis.\n\n\n\nNo need to simulate!\nThe null distribution of the \\(\\chi^2\\) test-statistic is known! This means we already have a null sampling distribution, so we could have skipped simulation. To use this null you simply need to know the “degrees of freedom” – the number of values you must know before you can guess the rest. The degrees of freedom for a goodness of fit test is the number of categories minus one. Once we have that number (in this case, 9) we can use the pchisq() in R to find the p value (Make sure to only consider the upper tail of the \\(\\chi^2\\) distribution).\n\n\nDegrees of freedom for a \\(\\chi^2\\) “goodness of fit” test equals the number of categories minus one.\n\npchisq(q = 10.462, lower.tail = FALSE, df = 9)\n\n[1] 0.3143933\n\n\nNot only are these p-values basically identical, but the sampling distributions are (more or less) the same (Figure 2).\n\n\nCode\nlibrary(patchwork)\n\n\nchi2_per_sample&lt;- read_csv( \"../../data/chi2_for_sim.csv\")\n\nplot_a &lt;- ggplot(chi2_per_sample, aes(x = chi2))+\n  geom_histogram(color = \"black\", bins = 25, aes(y = ..density..), fill = \"azure1\")+\n  theme(axis.text = element_text(size = 16), axis.title = element_text(size = 18),\n      title = element_text(size =16))+\n  labs(x = expression(chi^2), y =\"probability\", title = expression(chi^2~~~df==9) )+\n stat_function(fun = dchisq, args = list(df = 9), color = \"red\", linewidth =2,alpha = .4) \n\n\n\n\nplot_b &lt;- chi2_per_sample|&gt;arrange(chi2)|&gt;\n    mutate(pval = rank(-chi2) / max(rank(-chi2)))|&gt;\n    ggplot(aes(x = chi2, y= pval))+ \n    labs(x = expression(chi^2), y =\"p-value\", title = expression(chi^2~~~df==9) )+\n    geom_line(size = 1.5)+\ngeom_line(data = tibble(chi2 = seq(0,46,.01), pval = pchisq(chi2,9,lower.tail = FALSE)), color = \"red\", alpha = .3,linewidth = 3.5)+\n  theme(axis.text = element_text(size = 16), axis.title = element_text(size = 18),\n      title = element_text(size =16))\n\nplot_a +plot_b+plot_annotation(tag_levels = \"A\")\n\n\n\n\n\n\n\n\nFigure 2: Simulated χ² values match the χ² distribution. (A) Histogram of χ² values from 100,000 simulated random samples (41 draws from 10 equally likely categories), overlaid with the theoretical χ² probability density curve for 9 degrees of freedom (red line). (B) The relationship between χ² values and their corresponding p-values. Each point represents a simulated χ² statistic (black), and the red line shows the theoretical p-value function for 9 degrees of freedom.\n\n\n\n\n\n\n\nNo need to calculate!\nBut wait, it gets even easier. We don’t even need to calculate \\(\\chi^2\\), just give it our observations and the expected proportions and R will find our \\(\\chi^2\\) value, the degrees of freedom, and the associated p-value!\n\n\nIn our case, the expected proportions were all the same, so we actually didn’t even need to type them. But some cases (e.g. if you’re testing to see if data meet Hardy Weinberg expectations) the proportion of expected observations differs for each category.\n\nnumber_data&lt;- tibble(number = 1:10, \n                    times_picked= c(1,4,6,3,3,8,7,3,3,3),\n                    expected_proportion = c(0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1))\n\nchisq.test(x = pull(number_data, times_picked),\n           p=  pull(number_data, expected_proportion))\n\n\n    Chi-squared test for given probabilities\n\ndata:  pull(number_data, times_picked)\nX-squared = 10.463, df = 9, p-value = 0.3143",
    "crumbs": [
      "14-B. χ2",
      "• 14B. $\\chi^2$ distribution"
    ]
  },
  {
    "objectID": "book_sections/chi2/chi2_dist.html#the-chi2-contingency-test",
    "href": "book_sections/chi2/chi2_dist.html#the-chi2-contingency-test",
    "title": "• 14B. \\(\\chi^2\\) distribution",
    "section": "The \\(\\chi^2\\) “contingency” test",
    "text": "The \\(\\chi^2\\) “contingency” test\n\n\n\n\n\nvisited\npink\nwhite\n\n\n\n\nFALSE\n1\n11\n\n\nTRUE\n52\n39\n\n\n\n\n\n\nWe also used a simulation approach (in this case a permutation) to test the null hypothesis that two categorical variables are independent. We used the relationship between petal color and if a flower was visited from our RIL data from Sawmill Road for that example. As above, we calculated the \\(\\chi^2\\) values from expectations and simulated to generate the null distribution, which in this case, equaled 10.11. We then compared this to the null distribution (obtained by permutation) and rejected the null (\\(p \\approx 0.0017\\)).\nAs above, we skip permutation and just compare our observed \\(\\chi^2\\) value directly to the theoretical \\(\\chi^2\\) distribution (we just need to know the degrees of freedom). A 2×2 contingency table has just one degree of freedom because, once we know the overall proportions of pink flowers and of those receiving visits (needed to calculate expectations), we just need to know the value in one cell to correctly guess the rest.\n\n\nWhile the \\(\\chi^2\\) example provided here considered binary outcomes, the \\(\\chi^2\\) contingency tests can be applied to categories with any number of levels. More broadly, the degrees of freedom for a \\(chi^2\\) contingency test equals (the number of rows - 1) \\(\\times\\) (the number of columns - 1).\nSo we can again use the pchisq() function to find the p value, being sure to set lower.tail = FALSE. Reassuringly, the permutation-based and analytically derived p-values align closely.\n\npchisq(10.11, df = 1, lower.tail = FALSE)\n\n[1] 0.00147467\n\n\nComparing the p-values for specific (attainable) \\(\\chi^2\\) values in this study (Figure 3), we see that the null sampling distributions for the permutation and mathematical approaches are nearly identical.\n\n\nCode\nall_perms_table_chi2 &lt;- read_csv( \"../../data/chi2_for_perm.csv\")\n\n\nall_perms_table_chi2|&gt;arrange(chi2)|&gt;\n    mutate(pval = rank(-chi2) / max(rank(-chi2)))|&gt;\n    ggplot(aes(x = chi2, y= pval))+ \n    labs(x = expression(chi^2), y =\"p-value\", title = expression(chi^2~~~df==1) )+\n  geom_step(size = 1, lty =2, alpha = .5)+\n    geom_point(size =4)+\ngeom_line(data = tibble(chi2 = seq(0,11,.01), pval = pchisq(chi2,1,lower.tail = FALSE)), color = \"red\", alpha = .3,linewidth = 3.5)+\n  theme(axis.text = element_text(size = 16), axis.title = element_text(size = 18),\n      title = element_text(size =16))\n\n\n\n\n\n\n\n\nFigure 3: Comparing permutation-based p-values (black points, dashed line) to the theoretical χ² distribution with one degree of freedom (smooth red curve). The distributions closely follow one another, but the permutation p-values decrease stepwise because there are a finite number of possible permutations, while the analytical χ² distribution provides a smooth approximation.\n\n\n\n\n\n\nTHINK”: Both our \\(\\chi2\\) values were approximately 10. Why did we comfortably reject the null in this case and fail to reject it in the other?\n\n\nExplanation\n\nThe answer is because they have different degrees of freedom!",
    "crumbs": [
      "14-B. χ2",
      "• 14B. $\\chi^2$ distribution"
    ]
  },
  {
    "objectID": "book_sections/chi2/chi2_dist.html#assumptions-of-the-chi2-test",
    "href": "book_sections/chi2/chi2_dist.html#assumptions-of-the-chi2-test",
    "title": "• 14B. \\(\\chi^2\\) distribution",
    "section": "Assumptions of the \\(\\chi^2\\) test",
    "text": "Assumptions of the \\(\\chi^2\\) test\nThe \\(\\chi^2\\) test (based on its mathematical distribution) make a set of standard assumptions (i.e. random and independent sampling in each group) shared with permutation-based tests. Additionally, the \\(\\chi^2\\) test assumes\n\nAll categories have an expected count greater than one, and\n\nNo more than 20% of categories have an expected count of less than five.\n\nThe “sawtoothed” nature of Figure 3 hints at the basis for these assumptions. Once the expected counts become small, there are only a limited number of distinct ways to rearrange observations among categories. This discreteness leads to a step-like sampling distribution, rather than the smooth curve predicted by the continuous \\(\\chi^2\\) approximation. When expected counts are too low, the approximation breaks down. Not to worry,… you can always permute 😉.",
    "crumbs": [
      "14-B. χ2",
      "• 14B. $\\chi^2$ distribution"
    ]
  },
  {
    "objectID": "book_sections/chi2/chi2_summary.html",
    "href": "book_sections/chi2/chi2_summary.html",
    "title": "• 14B. \\(\\chi^2\\) summary",
    "section": "",
    "text": "Links to:\nSummary. Chatbot tutor. Questions. Glossary. R packages. R functions. More resources.",
    "crumbs": [
      "14-B. χ2",
      "• 14B. $\\chi^2$ summary"
    ]
  },
  {
    "objectID": "book_sections/chi2/chi2_summary.html#chi2_summary_chapter-summary",
    "href": "book_sections/chi2/chi2_summary.html#chi2_summary_chapter-summary",
    "title": "• 14B. \\(\\chi^2\\) summary",
    "section": "Chapter summary",
    "text": "Chapter summary\nThe χ² test-statistic quantifies the difference between observed and expected counts of categorical variables. By comparing an observed \\(\\chi^2\\) value to its null distribution (generated by simulation, permutation, or math) we can caclulate p-values and conduct NHST.\n\nChatbot tutor\n\nPlease interact with this custom chatbot (link here). I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you got what you needed from it.",
    "crumbs": [
      "14-B. χ2",
      "• 14B. $\\chi^2$ summary"
    ]
  },
  {
    "objectID": "book_sections/chi2/chi2_summary.html#chi2_summary_practice-questions",
    "href": "book_sections/chi2/chi2_summary.html#chi2_summary_practice-questions",
    "title": "• 14B. \\(\\chi^2\\) summary",
    "section": "Practice Questions",
    "text": "Practice Questions\nTry these questions! By using the R environment you can work without leaving this “book”. I even pre-loaded all the packages you need!\n\nSETUP: Sometimes female Latrodectus hasselti (redback spiders) eat their mates. Is there anything in it for the males? Maydianne Andrade tested the idea that eating a male might prevent her from re-mating with a second male – that is if she’s too preoccupied eating/digesting her mate that she’s not looking to mate again. She observed whether a female accepted a second male after the first male either escaped or was eaten. link.\nI have these data:\n\nIn long format with three columns (first_male, second_male, and count), and four columns in a tibble called long_spider.\nIn wide format as a contingency table with three columns (first_male, Accepted, and Rejected (both refer to second male)), with eaten and escaped as count data, in a tibble called widespider. first_male Accepted Rejected\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ1) This study is BootstrapObservationalExpiremental\n\nQ2) Plot the data. Which trend is apparent?\n\n There is no obvious trend Following cannibalistic mating females are more likely to accept a second suitor Following cannibalistic mating females are less likely to accept a second suitor\n\n\nQ3) Assuming independence, how many cases do you expect when the first male is eaten and the second male is accepted? \n\n\nExplanation\n\n\n\\(P_{first male eaten}\\) =9/32.\n\\(P_{second male accpeted}\\) =25/32.\n\n\\(\\frac{9}{32} \\times \\frac{25}{32} \\times 32 = 7.03\\)\n\n\nQ4) What is the (two-tailed) null hypothesis?\n\n There is no association being eaten & your mate rejecting a second male Being eaten is not associated with a lower chance of your mate rejecting a second male There is an association being eaten & your mate rejecting a second male\n\n\nQ5) What is the (two-tailed) alternative hypothesis?\n\n There is no association being eaten & your mate rejecting a second male Being eaten is not associated with a lower chance of your mate rejecting a second male There is an association being eaten & your mate rejecting a second male\n\n\n\nwide_spider |&gt; \n  select(-1)|&gt; # remve the label column that does not have numbers\n  chisq.test()\n\nWarning in stats::chisq.test(x, y, ...): Chi-squared approximation may be\nincorrect\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  select(wide_spider, -1)\nX-squared = 11.28, df = 1, p-value = 0.0007836\n\n\n\nQ6) Given our test,\n\n We reject the null We fail to reject the null We accept the null We fail to accept the null",
    "crumbs": [
      "14-B. χ2",
      "• 14B. $\\chi^2$ summary"
    ]
  },
  {
    "objectID": "book_sections/chi2/chi2_summary.html#chi2_summary_glossary-of-terms",
    "href": "book_sections/chi2/chi2_summary.html#chi2_summary_glossary-of-terms",
    "title": "• 14B. \\(\\chi^2\\) summary",
    "section": "📊 Glossary of Terms",
    "text": "📊 Glossary of Terms\n\nGlossary: χ² (Chi-Squared) Tests\n\nChi-Squared (χ²) Statistic: A measure of how far observed counts differ from expected counts under a null model. Larger χ² values indicate greater deviation from expectation.\nChi-Squared Distribution: A probability distribution that describes the sampling distribution of the χ² statistic when the null hypothesis is true. It depends only on the number of degrees of freedom.\nDegrees of Freedom (df): The number of independent values that can vary in a dataset after certain constraints are applied. For a χ² test, this typically equals the number of categories minus one (for goodness-of-fit) or (rows − 1) × (columns − 1) for contingency tables.\nGoodness-of-Fit Test: A χ² test used to assess whether observed frequencies across categories differ significantly from expected frequencies based on a specific theoretical distribution.\nContingency (or Independence) Test: A χ² test used to evaluate whether two categorical variables are independent. Observed frequencies in a contingency table are compared to the frequencies expected if there were no association.",
    "crumbs": [
      "14-B. χ2",
      "• 14B. $\\chi^2$ summary"
    ]
  },
  {
    "objectID": "book_sections/chi2/chi2_summary.html#chi2_summaryR_packages_introduced",
    "href": "book_sections/chi2/chi2_summary.html#chi2_summaryR_packages_introduced",
    "title": "• 14B. \\(\\chi^2\\) summary",
    "section": "R Packages Introduced",
    "text": "R Packages Introduced\n\nThere a re no new packages, but we continue to use infer: this time for permutation.",
    "crumbs": [
      "14-B. χ2",
      "• 14B. $\\chi^2$ summary"
    ]
  },
  {
    "objectID": "book_sections/chi2/chi2_summary.html#chi2_summary_key-r-functions",
    "href": "book_sections/chi2/chi2_summary.html#chi2_summary_key-r-functions",
    "title": "• 14B. \\(\\chi^2\\) summary",
    "section": "🛠️ Key R Functions",
    "text": "🛠️ Key R Functions\n\nHere’s the matching summary for the chi-square functions:\n\n\nchisq.test(): Performs a chi-square test (ether a goodness-of-fit, or a contingency test). Returns the χ² statistic, degrees of freedom, and p-value.\n\n\nchisq.test(table(species, visited), correct = FALSE)\n\n\n\npchisq(): Looks up p-values or cumulative probabilities from the theoretical chi-square distribution. Used to compute p-values from a \\(\\chi^2\\) calculation/ Example:\n\n\npchisq(chi2_stat, df = 1, lower.tail = FALSE)",
    "crumbs": [
      "14-B. χ2",
      "• 14B. $\\chi^2$ summary"
    ]
  },
  {
    "objectID": "book_sections/chi2/chi2_summary.html#chi2_summary_additional-resources",
    "href": "book_sections/chi2/chi2_summary.html#chi2_summary_additional-resources",
    "title": "• 14B. \\(\\chi^2\\) summary",
    "section": "Additional resources",
    "text": "Additional resources\n\nVideos:\n\nCrash course statistics \\(\\chi^2\\)",
    "crumbs": [
      "14-B. χ2",
      "• 14B. $\\chi^2$ summary"
    ]
  },
  {
    "objectID": "book_sections/stats_4_linear_models_index.html",
    "href": "book_sections/stats_4_linear_models_index.html",
    "title": "Section IV: Stats for Linear Models",
    "section": "",
    "text": "Linear Models Review\nRemember that we previously introduced linear models as a framework to estimate the conditional mean of the \\(i^{th}\\) observation of a continuous response variable, \\(\\hat{Y}_i\\) for a (combination) of value(s) of the explanatory variables (\\(\\text{explanatory variables}_i\\)):\n\\[\\begin{equation}\n\\hat{Y}_i = f(\\text{explanatory variables}_i)\n\\end{equation}\\]",
    "crumbs": [
      "Section IV: Stats for Linear Models"
    ]
  },
  {
    "objectID": "book_sections/stats_4_linear_models_index.html#linear-models-review",
    "href": "book_sections/stats_4_linear_models_index.html#linear-models-review",
    "title": "Section IV: Stats for Linear Models",
    "section": "",
    "text": "Conditional mean: The expected value of a response variable given specific values of the explanatory variables (i.e., the model’s best guess for the response based on the explanatory variables).",
    "crumbs": [
      "Section IV: Stats for Linear Models"
    ]
  },
  {
    "objectID": "book_sections/stats_4_linear_models_index.html#adding-uncetainty-and-nhst",
    "href": "book_sections/stats_4_linear_models_index.html#adding-uncetainty-and-nhst",
    "title": "Section IV: Stats for Linear Models",
    "section": "Adding Uncetainty and NHST",
    "text": "Adding Uncetainty and NHST\nWe have just completed our section on the foundations of statistics. In that section, we introduced the idea that we should make sure to quantify uncertainty when presenting estimates.\nWe also introduced the idea that the “null hypothesis significance testing” (NHST) tradition in statistics works by assuming that data came from the “null model”, and that we “reject” this hypothesis when the null model rarely generates values as extreme as what we see empirically.\nHere, rather than using bootstrapping and permutation to quantify uncertainty and test null hypotheses, we run through the common mathematical tricks available to us in linear modelling. The models are the bread and butter of what we see in most bistats papers.\nHowever, whether we use mathematical or computational approaches to estimate uncertainty and test null hypotheses, the concepts are the same.",
    "crumbs": [
      "Section IV: Stats for Linear Models"
    ]
  },
  {
    "objectID": "book_sections/stats_4_linear_models_index.html#assumptions-of-linear-models",
    "href": "book_sections/stats_4_linear_models_index.html#assumptions-of-linear-models",
    "title": "Section IV: Stats for Linear Models",
    "section": "Assumptions of linear models",
    "text": "Assumptions of linear models\nA major difference between linear models and computational approaches to stats is that while all statistical models make assumptions, linear models make a specific set of assumptions that are needed to make the math work.\nLuckily for us, we will see that:\n\nMany of these assumptions are actually appropriate most of the time.\nLinear models are often robust to modest violations of assumptions.\nWe can build more specific models that better fit our data.\n\nWe will say more about these points as we go on, but now let’s introduce the major assumptions of linear models:\n\nLinear models assume linearity\nRecall that “linear models” are “linear” because we find an individual’s predicted value \\(\\hat{Y_i}\\) by adding up predictions from each component of the model. So, for example, \\(\\hat{Y_i}\\) equals the parameter estimate for the “intercept”, \\(a\\), plus its value for the first explanatory variable, \\(y_{1,i}\\), times the effect of this variable, \\(b_1\\), plus its value for the second explanatory variable, \\(y_{2,i}\\) times its effect, \\(b_2\\), etc.\n\\[\\hat{Y}_i = a + b_1 y_{1,i} + b_2 y_{2,i} + \\dots{}\\]\nThus a fundamental assumption of linear models is that we make predictions by adding up the impact of each variable.\n\n\nThis linearity assumption does not mean that we cannot include squared terms or interactions… In fact, the assumption of linearity sometimes requires that we add non-linear terms.\n\n\nLinear models assume independence\nLinear models assume that observations are independent. Or more precisely, that they are independent conditional on the explanatory variables. A simple way to say this is that we assume the residuals are independent.\nAs a reminder, a residual is the difference between observations and model predictions. So the residual value for individual \\(i\\), \\(e_i\\), is the difference between the value of their response variable, \\(Y_i\\), and the value the model predicts given individual \\(i\\)’s values of explanatory variables, \\(\\hat{Y\\_i}\\).\n\\[e_i = Y_i -\\hat{Y_i}\\]\n\n\nLinear models assume normality\nNot only do linear models assume independence of residuals, but they also assume the residuals are “normally distributed”. A normal distribution is a symmetric, bell-shaped curve that occurs frequently in nature and has many convenient mathematical properties. The next chapter is dedicated to the normal distirbution.\n\n\nLinear models assume constant variance\nLinear models assume that the variance of residuals is independent of the predicted value of the response variable, \\(\\hat{Y\\_i}\\).\n\n\nFancy words for these ideas are:  \nHomoscedasticity: Variance of residuals is constant – i.e. variance in residuals, \\(\\sigma_e\\) does not vary by the predicted value, \\(\\hat{Y}\\).\nHeteroscedasticity: Variance of residuals is not constant; it depends on predictors.\n\n\nLinear models assume independence of explanatory variables\nFor models with multiple explanatory variables, it is assumed that these predictors are not too tightly correlated with one another.\n\n\nMulticollinearity is the fancy word for high correlations between predictor variables.\n\n\nWhat’s ahead\nThis section gets into linear models, the workhorse for data analysis. We have previously covered some of the many types of common linear models but have done so without understanding uncertainty or NHST. Outside of this book, you may have heard of and even done some of these analyses before. My goal is therefore for you to know how they work, how to interpret their output, and when the results can be trusted. To do that we will:\n\nChapter 15: Begins with an introduction to the normal distribution. We will also use that opportunity to refresh our understanding of residuals and brush up a bit on simple probability theory.\nChapter 16: Considers how to include uncertainty and null hypothesis significance testing into our linear model, by considering the t-distribution. To do so we will focus on the simplest linear model – one that simply models the response variable as a function of nothing. We can compare this to what we have previously accomplished by bootstrapping.\nChapter 17: Introduces a slightly more complex model – a two sample t-test which compares the means of two groups. We can compare this to what we have previously accomplished by bootstrapping and permutation.\nChapter 18: Compares the means of more than two groups by an ANOVA. This chapter also introduces the F statistic, which plays a key role in interpreting linear models.\nChapter 19: Introduces the “multiple testing problem” and explains how to conduct “post-hoc” tests to see if means of two groups differ when our overall test looked at many groups.\nChapter 20: Introduces the regression – how we predict one continuous variable from another. We also delve into the idea of a polynomial regression.\nChapter 21: Introduces the ANCOVA in which the response variable is a function of one continuous and one categorical predictor. We also take this opportunity to look into “types of sums of squares”and how to be careful with R’s defaults.\nChapter 22: Introduces models that include interactions between explanatory variable.\nChapter 23: Introduces multiple regression, where we model a response using more than one continuous predictor.\n\n\nAfter completing this section we will be equipped with the standard tools of biostatistics.\n\nFollowing this section,\n\nSection V will consider high-level issues including experimental design, causal inference, and bias.\nFinally, Section VI will introduce common approaches for dealing with data where the standard tools of linear modeling do not apply.",
    "crumbs": [
      "Section IV: Stats for Linear Models"
    ]
  },
  {
    "objectID": "book_sections/normal.html",
    "href": "book_sections/normal.html",
    "title": "15. Normal distribution",
    "section": "",
    "text": "Motivating scenario: You are beginning your journey into linear modeling and have heard about the normal distribution. You know it’s a key assumption of linear models and need a brush up.\nLearning goals: By the end of this chapter you should be able to:\n\nIdentify the key features of a normal distribution.\n\nExplain the roles of the mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)) in defining a normal distribution.\n\nExplain the central limit theorem and why the normal distribution is so common in nature.\n\nExplain the difference between a probability mass function (for discrete data) and a probability density function (for continuous data).\n\nConduct a Z-transformation, understanding what a Z-score represents and why it is useful.\n\nVisually evaluate if a distribution is “normal-ish”.\n\nKnow common transformations to make a distribution more closely resemble the normal distribution.\n\n\n\n\nHistograms\n\n\nCode to make a histogram\nggplot(gc_rils, aes(x = petal_area_mm))+\n  geom_histogram(bins = 12, color = \"white\")+\n  theme(axis.title = element_text(size = 30),\n        axis.text = element_text(size = 30))\n\n\n\n\n\n\n\n\n\nFigure 1: A histogram showing the distribution of petal area of parviflora RILs planted at site GC.\n\n\n\n\nBy now, we have seen a bunch of histograms. Some, like Figure 1, have come from raw data, while others have come from resampling techniques like bootstraps or permutations. As a refresher, histograms display the distribution of a single numeric variable by “binning” the data, placing these bins on the x axis, and displaying the number observations in a given x-bin on the y axis.\n\n\nMathematical Distributions\nHistograms are remarkably useful visualizations because they show the shape, range, and variability in a dataset. By knowing the shape of the data, we can understand the data’s statistical properties.\nAlthough data comes in many shapes, there are some distributions that are so common in nature that they are given their own name and come with many mathematical tools. If we are lucky (and we often are), the data can be reasonably approximated by one of these handful of mathematical distributions. What this means is that we can summarize a histogram by an equation. This also means we can have a pretty good sense of our data by knowing\n\nThe mathematical distribution it (roughly) follows\nThe values of the key parameters in this distribution.\n\nThis is remarkably powerful because we can then effectively summarize hundreds of data points with a few numbers, and because we can apply a whole bunch of mathematical machinery to generate sampling distributions and do statistics!!\n\nThis section focusses on the normal distribution. Other common statistical distributions include:\n\nBinomial: The number of “successes” in n trials. This is a natural distribution for e.g. the number of hybrid seeds out of n genotyped offspring.\nPoisson: The number of observations in a certain space or over a certain time. This is a natural distribution for e.g. the number of pollinator visits observed in a fifteen minute time interval.\n\nWe will revisit these distributions later in the book.\n\n\n\nThe normal distribution\n\n\n\n\n\n\n\n\n\nFigure 2: Comparing the normal and paranormal distributions. From Freeman (2006).\n\n\n\n\nPerhaps the most common and well-known distribution has a shape similar to a bell and is called the normal distribution (Figure 2). The normal distribution is characterized by a few key features:\n\nA single, central peak where the data are most frequent.\nSymmetry about the center line.\n\nThe shape of this distribution is described precisely by its probability density function (PDF). This equation is defined by two key parameters:\n\nThe population mean, \\(\\mu\\), which sets the center of the distribution.\nThe population standard deviation, \\(\\sigma\\), which sets the width (or spread) of the distribution.\n\n\\[f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\]\nWith these two parameters, we can fully characterize any normal distribution!\n\n\nSampling from a normal distribution\nThe equation obscures the fact that sampling involves random chance. The outcome of sampling isn’t predetermined, it’s uncertain. However just because it’s uncertain does not mean that all results are equally likely.\nI built the webapp below to encourage you to “walk the field,” pick Clarkia xantiana flowers, measure them, and see how a normal model captures that built-in randomness. Try to pick five, then ten then 30!\n\n\n\n\n\nWhat’s ahead\nHere is how we will wind through the normal distribution and learn a few other things on the journey:\n\nFirst, we’ll get a feel for the normal distribution, introducing the concept of a probability density function and how to calculate it in R.\nNext, we’ll explore the key properties of the normal distribution, using an interactive web app and R’s pnorm() function to calculate probabilities. We’ll also cover the Z-transformation, a key tool for standardizing data.\nThen, we’ll simulate data from a normal distribution to verify the mathematical properties of the normal to build a deeper intuition for the sampling process.\nWe’ll then discuss the Central Limit Theorem, the amazing principle that explains why the normal distribution is so common in statistics.\nNext, we’ll cover the practical skills of assessing if your data is “normal-ish” using visual tools like QQ plots.\n\nFinally we’ll explore common data transformations to make data normal.\n\nWe conclude, as always, with a chapter summary.\n\n\n\n\nFreeman, M. (2006). A visual comparison of normal and paranormal distributions. J Epidemiol Community Health, 60(1), 6.",
    "crumbs": [
      "15. Normal distribution"
    ]
  },
  {
    "objectID": "book_sections/normal/normal_intro.html",
    "href": "book_sections/normal/normal_intro.html",
    "title": "• 15. Normal Introduction",
    "section": "",
    "text": "Motivation: You are getting ready to conquer the normal distribution and just need to set a few things in place.\nLearning goals: By the end of this chapter you should be able to:\n\nIdentify the key features of a normal distribution and its parameters:\n\nThe center: Mean (\\(\\mu\\)).\n\nThe width: Standard deviation (\\(\\sigma\\)).\n\nExplain the concept of a probability density for continuous data and how it differs from a probability mass function for discrete data.\nUse R’s dnorm() function to find the probability density of a specific value from a normal distribution.\n\nKnow that the standard error, \\(\\sigma_\\bar{x}\\), is the the standard deviation of the sampling distribution.\n\nThe standard error of a sample from a normal distribution is \\(\\sigma_\\bar{x} = \\frac{\\sigma}{\\sqrt{n}}\\) where:\n\n\\(\\sigma = \\frac{\\Sigma(x_i-\\mu)^2}{n}\\) is the population standard deviation,\n\n\\(n\\) is the sample size.\n\n\n\n\n\n\n\nA normal-ish distribution\n\n\nCode to make a histogram with normal line.\ngc_rils &lt;- gc_rils|&gt; \n  mutate(log10_petal_area_mm = log10(petal_area_mm))\n\ngc_rils_summaries &lt;- gc_rils  |&gt; \n  summarise(mean_log10_petal_area = mean(log10_petal_area_mm, na.rm = TRUE),\n            sd_log10_petal_area   = sd(log10_petal_area_mm, na.rm = TRUE))\n\ngc_rils_mean_log10_petal_area &lt;- pull(gc_rils_summaries, mean_log10_petal_area)\ngc_rils_sd_log10_petal_area &lt;- pull(gc_rils_summaries, sd_log10_petal_area)\nthis_n &lt;- gc_rils |&gt; summarise(n = sum(!is.na(petal_area_mm))) |&gt; pull()\n\ngc_rils_mean_log10_petal_area &lt;- pull(gc_rils_summaries, mean_log10_petal_area)\ngc_rils_sd_log10_petal_area &lt;- pull(gc_rils_summaries, sd_log10_petal_area)\nthis_n &lt;- gc_rils |&gt; summarise(n = sum(!is.na(petal_area_mm))) |&gt; pull()\n\nggplot(gc_rils, aes(x = log10_petal_area_mm))+\n    geom_histogram(color = \"white\", binwidth = .06)+\n    stat_function(fun = function(x) { \n       this_n * .06 * dnorm(x,\n                           mean = gc_rils_mean_log10_petal_area ,\n                           sd = gc_rils_sd_log10_petal_area) },\n      color = \"red\", linewidth = 1, xlim = c(1.45,2.1))+\n  labs(x = expression(paste(\"log\"[10], \" petal area (mm)\")),\n       y = \"count\")+\n  theme(axis.title = element_text(size = 16),\n        axis.text = element_text(size = 16))\n\n\n\n\n\n\n\n\nFigure 1: A histogram showing the distribution of the log_10 transformed petal area. The red line shows the theoretical normal distribution with the same mean and standard deviation as the sample.\n\n\n\n\n\nPetal areas (even after being log transformed) aren’t perfectly normal, but they are close enough that the normal distribution is a useful approximation. Remembering that all models are wrong, but some are useful is key. Stats relies on useful but imperfect models - and it is our job as statisticians to recognize when a model is good enough and when it is downright inappropriate.\nTo see what this looks like in practice, let’s compare our flower data to a normal distribution fit by parameter estimates from our data. The similar shape of the histogram and the normal distribution plotted over it (in red), demonstrates that the \\(log_{10}\\)-transformed petal area of parviflora RILs (Figure 1) planted at GC is well-approximated by a normal distribution (with mean, \\(\\bar{x}\\) = 1.7815, and sd, s = 0.0998). We will therefore start with this example to understand properties of a normal distribution while picking up some probability theory and R tricks along the way.\n\n\n\n\n\n\nmean\nsd\n\n\n\n\n1.7815\n0.0998\n\n\n\n\n\ng_petal_summaries &lt;- gc_rils  |&gt; \n  summarise(mean = mean(log10_petal_area_mm, na.rm = TRUE),\n            sd   = sd(log10_petal_area_mm, na.rm = TRUE))\n\n\n\nFor the rest of this section we will deal with a population with a mean \\(\\mu\\) of 1.7815, and a standard deviation, \\(\\sigma\\) of 0.0998, rather than our sample. Note that I have gone back and forth on if I write mean and standard deviation with an English or Greek symbol… there is a method to the madness:\n\nGreek letters like \\(\\mu\\) and \\(\\sigma\\) refer to population parameters.\n\nEnglish symbols (typically \\(\\bar{x}\\) for mean, and \\(s\\) for the standard deviation), refer to sample estimates.\n\n\n\n\n\nProbability density functions\n\n\n\n\n\n\n\n\nFigure 2: The normal distribution that fits the log-10 transformed petal area data. The curve shows the probability density for any given value, pretending our sample means and standard deviations where actually parameters in a normal distribution. This raw probability density is proportional to the line in Figure 1, above, but is not scaled to match our sample size or the bin width of the histogram.\n\n\n\n\n\nContinuous distributions are a bit funny:\n\nThe probability of any particular number (evaluate to infinite digits) is zero\n\nBut some observations would be more surprising than others\n\nA flower a \\(log_{10}\\)-transformed petal area of 99 would shock me.\n\nA flower a \\(log_{10}\\)-transformed petal area of 1.873892 would be less surprising.\n\n\nTo address this we work, not with raw probabilities, but with probability densities – numbers that are proportional to a probability. The probability density of the Gaussian (aka normal) distribution is equals\n\\[f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\]\n\n\nNote You can use R’s dnorm() function to plug numbers into the Gaussian function for you! The d is for density and the norm is for normal!\nSo, the probability density of a flower with an area of 1.873892 is\n\\[f(x) = \\frac{1}{0.0998\\sqrt{2\\pi}} e^{-\\frac{1}{2}\\left(\\frac{1.873892- 1.7815}{0.0998}\\right)^2} =     2.604\\]\n\n\nvalue   &lt;- 1.873892\nmy_mean &lt;- 1.7815\nmy_sd   &lt;- 0.0998\ndnorm(x     =  value, \n      mean  =  my_mean, \n      sd    =  my_sd)\n\n[1] 2.604191\n\nThis means that rather than summing to one (as traditional “probability masses” for categorical or discrete variables do) the probability density integrates to one. Thus, unlike probability masses, probabilities densities (like the one above, and many points in Figure 2) can exceed one.\n\n\nThe Probability Density of a Sample Mean\nRemember that the standard deviation of the sampling distribution is called the standard error. To calculate the probability density of a sample mean, we substitute \\(\\sigma\\) (the variance of the normal distribution) with the the standard deviation of a normal distribution (i.e. the standard error of the mean. Mathematically, the standard error of the mean of a normal sampling distribution, \\(\\sigma_\\bar{x}\\), equals:\n\\[\\sigma_\\bar{x}=\\frac{\\sigma}{\\sqrt{n}}\\text{ where: }\\]\n\n\\(\\sigma=\\frac{\\Sigma(x_i-\\mu)^2}{n}\\)$ is the population standard error.\n\n\\(n\\) is the sample size.\n\nFor instance: the probability density that a random draw of size n from a normal distribution with mean 1.7815 of and sd of 0.0998 will have a sample mean of 1.8739 equals:\n\ndnorm(1.8739, mean =  1.7815, sd = 0.0998/sqrt(2)) = 2.399 for \\(n\\) of 2.\ndnorm(1.8739, mean = 1.7815, sd = 0.0998/sqrt(10)) = 0.174 for \\(n\\) of 10.\n\n\n\nI did some sneaky stuff above, and I want you to think about it for a minute.\nNote that I said “the \\(log_{10}\\)-transformed petal area of parviflora RILs (Figure 1) planted at GC is well-approximated by a normal distribution (with mean = 1.7815, and sd = 0.0998)”.\nThis is a mathematical approximation, not the true description of this population for two reasons.\n\nThe values of \\(\\mu\\) and \\(\\sigma\\) are estimates from the sample, not the true parameters.\nPetal area is not generated by a normal distribution - it is generated by genes, sunlight, nutrients and the like, so the word “approximated” is doing a lot of work here. This is the difference between a mathematical and phenomenological model.\nPutting aside the concern above - the mathematical description of the data as normal may not be quite right - to my eye the data are somewhat skewed.\n\nBut as long as we keep these caveats in our head using the normal distirbution to approximate these data is incredibly useful!",
    "crumbs": [
      "15. Normal distribution",
      "• 15. Normal Introduction"
    ]
  },
  {
    "objectID": "book_sections/normal/normal_properties.html",
    "href": "book_sections/normal/normal_properties.html",
    "title": "• 15. Normal Properties",
    "section": "",
    "text": "Exploring Probabilities with the Standard Normal Distribution\nNow that we have the basics of the normal distribution down, let’s consider some of its properties. While I don’t believe in “making” students memorize anything, these are basic facts that anyone who deals with statistics should be pretty comfortable with:\nWe briefly go over these properties below. As we do so, we will also review some basic probability theory.\nThese properties are true for any normal distribution, regardless of its mean or standard deviation. But to make them concrete, let’s start with a normal distribution with a mean of zero and a standard deviation of one. This is known as the Standard Normal Distribution. The standard normal is convenient because the values on the x-axis (like -2, -1, 1, 2) directly correspond to the number of standard deviations from the mean.\nWe can dust off our integral calculus to find the probability that a value of X falls within a range (see the margin). But if you (like me) would rather use a computer and simple arithmetic we can avoid calculus! I have created a webapp (below) to help get started with these ideas, after a bit of practice with the webapp, I’ll introduce some probability theory rules to help combine probabilities.",
    "crumbs": [
      "15. Normal distribution",
      "• 15. Normal Properties"
    ]
  },
  {
    "objectID": "book_sections/normal/normal_properties.html#the-z-transformation",
    "href": "book_sections/normal/normal_properties.html#the-z-transformation",
    "title": "• 15. Normal Properties",
    "section": "The Z-Transformation",
    "text": "The Z-Transformation\nThe web app is a great tool for understanding the standard normal distribution. But how does this help us with our flower data, which has a mean of 1.7815 and a standard deviation of 0.0998?\nThe answer is a powerful tool called the Z-transformation. It’s a simple formula that allows us to convert any value from any normal distribution into an equivalent value on the standard normal distribution. This means we can solve any normal distribution problem by converting it to the standard normal first.\nWe z-transform data by taking each value, subtracting the mean, and dividing by the standard deviation. So, if the \\(i^{th}\\) observation has a value of \\(x_i\\), its z-value is:\n\\[z_i=\\frac{x_i-\\mu}{\\sigma}\\]\nSo, although we just worked through the results thinking about the standard normal, they apply to any normal distribution!\n\nThe mathematical notation for a normal distribution is written as follows:\n\\[X \\sim \\mathcal{N}(\\mu,\\,\\sigma^{2})\\]\n\nSo the standard normal is written as: \\(X \\sim \\mathcal{N}(0,\\,1^2)\\).\nAnd a normal with a mean of \\(1.78\\), and an sd of 0.1 (i.e. a variance of \\(0.01\\)) is written as: \\(X \\sim \\mathcal{N}(1.78,\\,0.01)\\).",
    "crumbs": [
      "15. Normal distribution",
      "• 15. Normal Properties"
    ]
  },
  {
    "objectID": "book_sections/normal/noRmal.html",
    "href": "book_sections/normal/noRmal.html",
    "title": "• 15. Normal Simulations",
    "section": "",
    "text": "Simulating From Normal Distributions in R\nIn the previous section, we used a web app to calculate probabilities from a normal distribution. Here, we’ll take a different approach: simulation. We’ll use R’s rnorm() function to generate our own random data. This approach both builds intuition for the sampling process, and allows us to double check our math.",
    "crumbs": [
      "15. Normal distribution",
      "• 15. Normal Simulations"
    ]
  },
  {
    "objectID": "book_sections/normal/noRmal.html#simple-simulation-with-rnorm",
    "href": "book_sections/normal/noRmal.html#simple-simulation-with-rnorm",
    "title": "• 15. Normal Simulations",
    "section": "Simple Simulation with rnorm()",
    "text": "Simple Simulation with rnorm()\n\n\n\n\n\n\nWe previously saw that about 68% of a normal distribution fell within one standard deviation of the mean. We can validate this for ourselves by a simple simulation, using our log-transformed petal area (\\(X \\sim \\mathcal{N}(1.78,\\,0.01)\\)) as an example. To do so\n\nWe first simulate the data with rnorm() (see simulated data in the margin).\n\nWe then find the proportion of draws within one standard deviation (i.e. greater than 1.68 and less than 1.88, code and answer below:\n\n\nnormal_sim &lt;- tibble(x = rnorm(mean = 1.78, sd = 0.1, n = 1000) )\n\nnormal_sim |&gt;\n  mutate(within_sd = x &gt; 1.68 & x &lt; 1.88) |&gt;\n  summarise( prop_within_sd = mean(within_sd))\n\n# A tibble: 1 × 1\n  prop_within_sd\n           &lt;dbl&gt;\n1          0.691\n\n\nThis is close to our mathematical result, but sampling did set this a bit off our precise expectation of 0.6826895.\n\nrnorm() (and all _norm() functions) defaults to the standard normal, so rnorm(n=10) generate a sample of ten observations from a normal distirbution.\n\n\nMultiple samples from a normal\n\n\n\n\n\n\nWe know by now that the standard error is the standard deviation of the sampling distribution. I briefly informed you that for the normal the mathematical formula for the standard error of the mean is:\n\\[\\sigma_\\bar{x} = \\sigma/\\sqrt{n}\\text{ where: }\\]\n\n\\(\\sigma = \\frac{\\Sigma{(x_i-\\mu)}}{n}\\).\n\n\\(n\\) is the sample size.\n\nLet’s convince ourselves of this by simulation and then make sure that \\(\\approx 68\\%\\) of sample means, \\(\\bar{x}\\) are within one SE of the population mean \\(\\mu\\).\nTo do so we will set up a scenario in which our simulation results are “grouped”. Summarizing each sample with the mean generates a sampling distribution.\nNote for the simulation below:\n\nWe simulated from the standard normal distribution (\\(X \\sim \\mathcal{N}(0,\\,1)\\)).\n\nWe simulated one hundred thousand samples each of size nine.\n\n\nmu               &lt;- 0 \nsigma            &lt;- 1\nsample_size      &lt;- 9 \nn_samples        &lt;- 100000\n\nnormal_sampling_sim &lt;- tibble(\n  x = rnorm(mean = mu, sd = sigma, n = sample_size * n_samples),\n  group = rep(1:n_samples, each = sample_size)\n)\n\nsample_means &lt;- normal_sampling_sim |&gt;\n  group_by(group)   |&gt;\n  summarise(mean_x = mean(x))\n\n\n\n\n\n\n\n\n\nFigure 1: Simulating many samples of size 9 from a population with μ=0 and σ=1. A. The first 12 random samples. Note the variability in the shapes of the histograms and the sample means (red dashed lines). B. The sampling distribution of the mean, constructed from 100,000 sample means. Note that the spread of of data in each simulation (A), exceeds the spread of sample means in B.\n\n\n\n\n\nWe can calculate the standard deviation of the sample means in Figure 1 as show in the margin. You can see that the standard deviation of the sampling distirbution of the standard normal with a sample size of 9 is approximately \\(1/3\\), which reassuringly is the standard deviation (\\(\\sigma = 1\\)) divided by the square root of the sample size, \\(\\sqrt{n} = \\sqrt{9} =3\\).\n\n\nsample_means |&gt; \n  summarise(se = sd(mean_x))\n\n# A tibble: 1 × 1\n     se\n  &lt;dbl&gt;\n1 0.333\n\n\nMutliple samples from a normal within a range\nWe expect, for example, \\(\\approx 68\\%\\) of draws from the normal to be within one standard deviation of the mean. We similarly expect \\(\\approx 68\\%\\) of sample means to be within one standard error of the population mean (i.e. between \\(\\mu-\\frac{\\sigma}{\\sqrt{n}}\\) and \\(\\mu+\\frac{\\sigma}{\\sqrt{n}}\\)). We can confirm this was true of our simulation as follows:\n\nse    &lt;- sigma / sqrt(sample_size)\n\nsample_means |&gt; \n  mutate(within_one_se     = mean_x &gt; mu - se &    mean_x &lt; mu + se) |&gt;\n  summarise(prop_within_se = mean(within_one_se))\n\n# A tibble: 1 × 1\n  prop_within_se\n           &lt;dbl&gt;\n1          0.683",
    "crumbs": [
      "15. Normal distribution",
      "• 15. Normal Simulations"
    ]
  },
  {
    "objectID": "book_sections/normal/p_norm.html",
    "href": "book_sections/normal/p_norm.html",
    "title": "• 15. Normal Math",
    "section": "",
    "text": "Finding probabilities of a range with pnorm()\nI introduced a webapp that integrates the area under the standard normal curve for you, and have shown that you can use rnorm() to simulate a normal distribution. Here, I introduce how to use pnorm() to have R, rather than my webapp, calculate this area.\nThis is useful because we often want to calculate a p-value, which amounts to asking “How much of the null sampling distribution is more extreme than my observed observed test statistic?”",
    "crumbs": [
      "15. Normal distribution",
      "• 15. Normal Math"
    ]
  },
  {
    "objectID": "book_sections/normal/p_norm.html#finding-probabilities-of-a-range-with-pnorm",
    "href": "book_sections/normal/p_norm.html#finding-probabilities-of-a-range-with-pnorm",
    "title": "• 15. Normal Math",
    "section": "",
    "text": "Intro to pnorm()\n\n\n\n\n\n\n\n\nFigure 1: A normal distribution with a mean of 1.78 and a variance of 0.01. The shaded blue areas in the tails represent the probabilities of observing a value more than 1.5 standard deviations away from the mean. The left tail corresponds to pnorm(lower.tail = TRUE), while the right tail corresponds to pnorm(lower.tail = FALSE).\n\n\n\n\n\nWe can find the probability that a sample from a normal is greater than or less than some value of interest with R’s pnorm() function, which takes the arguments:\n\nq: The value of interest.\n\nmean: The population mean, \\(\\mu\\).\n\nsd: The population standard deviation, \\(\\sigma\\).\n\nlower.tail: Whether we want the upper or lower tail (see Figure 1).\n\nSet lower.tail = TRUE for the lower tail.\n\nSet lower.tail = FALSE for the upper tail.\n\n\nNow, try it yourself. Use the webR console below to calculate the probability that a random draw from our distribution (\\(X \\sim \\mathcal{N}(1.78,\\,0.01)\\)) is\n\n\nRemember the notation: \\(\\mathcal{N}(1.78,\\,0.01)\\) means a normally distributed random variable with mean 1.78 and variance 0.01 (i.e. standard deviation pod \\(\\sqrt{0.01}=0.1\\)).\n\nLess than 1.63 (1.5 sd less than the mean): .\nGreater than 1.93 (1.5 sd more than the mean): .\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nI give up\n\n.\nLess than 1.63: type pnorm(q = 1.63, mean = 1.78, sd = 0.1, lower.tail = TRUE) = 0.0668072.\nGreater than 1.93: type pnorm(q = 1.93, mean = 1.78, sd = 0.1, lower.tail = FALSE) = 0.0668072.\n\nIf you got these answers right, you should have noticed that they are the same. This makes sense because the normal distribution is symmetric and both values are 1.5 standard deviations away from the mean.",
    "crumbs": [
      "15. Normal distribution",
      "• 15. Normal Math"
    ]
  },
  {
    "objectID": "book_sections/normal/p_norm.html#simple-probability-rules",
    "href": "book_sections/normal/p_norm.html#simple-probability-rules",
    "title": "• 15. Normal Math",
    "section": "Simple Probability Rules",
    "text": "Simple Probability Rules\nThe pnorm() function is a great tool, but it has one limitation: it only calculates the probability from a single point out to one of the tails. This is useful, but it doesn’t directly answer other common questions, like:\n\nWhat’s the probability of an extreme outcome in either tail (e.g., very high OR very low)? We use this to find a two-tailed p-value.\n\nWhat’s the probability of an outcome falling in a specific range between two points?\n\nFortunately, by combining results from pnorm() with two simple rules from probability theory we can answer these sorts of questions. Below we’ll cover:\n\nHow to find the probability of this or that.\nHow to find the probability that X falls in a range.\n\n\nThe Addition Rule for Mutually Exclusive Events\nAbove we found that \\(6.68\\%\\) of the normal distribution is 1.5 standard deviations less than the population mean. We also found that \\(6.68\\%\\) of the normal distribution is 1.5 standard deviations greater than the population mean.\nBecause a single value cannot be in both the upper tail and the lower tail at the same time, these events are mutually exclusive. To find the total probability of either one OR the other happening, we simply add their individual probabilities.\n\\[P(A \\text{ or }B) = P(A) +P(B)\\] So the percent of samples from a normal are either 1.5 standard deviations more than or 1.5 standard deviations less than the mean equals \\(0.0668 + 0.0668 = 0.1336\\). Because these are symmetric we could have just multiplied \\(0.0668\\) by two. However this addition rule applies for any combination of mutually exclusive options.\n\nThe simple addition rule only applies to mutually exclusive events. For nonexclusive events we need to subtract off cases where both events happen to avoid double counting.\n\n\n\nThe Complement Rule\nWe just found that the probability of a value being more than one and a half standard deviations away from the mean is 0.1336. If we know the probability of an extreme outcome, what’s the probability of a “typical” outcome (i.e., not an extreme one)?\nWe can use the complement rule to answer this! Since the total probability of all possible outcomes must sum to 1, the probability of being within one and a half standard deviations is simply 1 minus the probability of being outside it.\nFor example, to find the probability of a value falling within 1.5 standard deviations of the mean for a standard normal distribution, we would calculate:\n\\[P(-1.5 &lt; X &lt; 1.5) = 1 - P(X &lt; -1.5 \\text{ or } X &gt; 1.5) = 1 - 0.1336 = 0.8664\\]",
    "crumbs": [
      "15. Normal distribution",
      "• 15. Normal Math"
    ]
  },
  {
    "objectID": "book_sections/normal/p_norm.html#the-z-test",
    "href": "book_sections/normal/p_norm.html#the-z-test",
    "title": "• 15. Normal Math",
    "section": "The Z-test",
    "text": "The Z-test\nWe will introduce many specific statistical tests in the following chapters. However, many of them can be approximated by a simple Z-test. The Z-test evaluates the null hypothesis (\\(H_0\\)) that a normally distributed test statistic is equal to a specific value.\nTo conduct a Z-test we:\n\nFind our Z-statistic: Z is the number of standard errors between our observation and the value proposed under the null hypothesis. We find it as \\(Z=\\frac{\\bar{x} - \\mu_0}{\\sigma_\\bar{x}}\\), where:\n\n\\(\\bar{x}\\) is the parameter estimate.\n\n\\(\\mu_0\\) is the parameter value proposed by the null model.\n\n\\(\\sigma_\\bar{x}\\) is the standard error.\n\n\nFind the two-tailed p-value as 2 * pnorm(q = abs(Z), lower.tail = FALSE).\nReject \\(H_0\\) is \\(p &lt; \\alpha\\) (where \\(\\alpha\\) is traditionally 0.05). Otherwise, fail to reject \\(H_0\\).\n\n\nFinding the Z statistic should remind you of the Z-transform, but rather than transforming each data point, we transform our data summary. As such we subtract the proposed value under then null, \\(\\mu_0\\), from our observed value, \\(\\bar{x}\\), and divide buy the standard error (i.e. the standard deviation of the sampling distribution)\n\\[Z=\\frac{\\bar{x}-\\mu_0}{\\sigma_\\bar{x}}\\]",
    "crumbs": [
      "15. Normal distribution",
      "• 15. Normal Math"
    ]
  },
  {
    "objectID": "book_sections/normal/is_it_normal.html",
    "href": "book_sections/normal/is_it_normal.html",
    "title": "• 15. Is It Normal?",
    "section": "",
    "text": "Is it normal?\nMany standard statistical approaches rely, to some extent, on normal data (or more specifically, a normally distributed sampling distribution of residuals). It is therefore often important to know if our data (or at least the residuals of a linear model) are normally distributed, as this influences how much faith we have in the results of a given statistical procedure.\nWhile there are ways to formally test the null hypothesis that data come from a normal distribution, we rarely use these because deviations from normality can be most critical when we have the least power to detect them. For this reason,we typically rely on visual inspection rather than null hypothesis significance testing to assess whether data are approximately normal.",
    "crumbs": [
      "15. Normal distribution",
      "• 15. Is It Normal?"
    ]
  },
  {
    "objectID": "book_sections/normal/is_it_normal.html#quantile-quantile-plots-and-the-eye-test",
    "href": "book_sections/normal/is_it_normal.html#quantile-quantile-plots-and-the-eye-test",
    "title": "• 15. Is It Normal?",
    "section": "“Quantile-Quantile” plots and the eye test",
    "text": "“Quantile-Quantile” plots and the eye test\nA QQ (aka “quantile-quantile”) plot is a useful tool to help us visually evaluate if data are roughly normal. It does so by comparing the quantiles of your data against the theoretical quantiles you would expect if your data came from some an ideal version of a specified distribution (in this case a normal distribution). If your data normal, the points will fall along a straight line.\nThe QQ-plot of petal area in parviflora RILs planted at site GC (Figure 2.3) reveals that the points are fairly close to the predicted line, although both the small and large values are slightly larger than expected. Is this a big deal? Is this deviation surprising? To answer that, we need to understand the variability we expect from a normal distribution.\n\ngc_rils |&gt;\n  ggplot(aes(sample = petal_area_mm))+\n  geom_qq()+\n  geom_qq_line()+\n  labs(x = \"Theoretical quantiles\",\n       y = \"Observed petal area\",\n       title = \"Normal QQ plot of petal area among parviflora RILs\")\n\n\n\n\n\n\n\nFigure 2: A quantile-quantile plot of petal area in parviflora RILs.\n\n\n\n\n\n\n\nMaking a QQ plot in R: We can create a QQ-plot using the geom_qq() function and add a line with geom_qq_line(). Here, we map our quantity of interest onto the sample attribute.\n\nWhat normal distributions look like\nI’m always surprised by how easily I can convince myself that a sample doesn’t come from a normal distribution. Try hitting the Generate a sample from the normal distribution button in the app below a few times, and experiment with the sample size to get a sense of the variability in what samples from a normal distribution can look like.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| column: page-right\n#| standalone: true\n#| viewerHeight: 900\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(bslib)\n\nui &lt;- fluidPage(\n  theme = bs_theme(bootswatch = \"flatly\"),\n  titlePanel(\"Getting a feel for normal distributions\"),\n  \n  fluidRow(\n    column(\n      12,\n      div(\n        style = \"margin-bottom: 10px;\",\n        actionButton(\"go\", \"Generate a sample from the normal distribution\"),\n        br(), br(),\n        tags$label(\"Sample Size:\", `for` = \"n\", style = \"font-weight:600;\")\n      ),\n      sliderInput(\n        \"n\", label = NULL, min = 10, max = 100, value = 24, step = 1, ticks = TRUE, width = \"100%\"\n      )\n    )\n  ),\n  \n  hr(),\n  h3(textOutput(\"subtitle\")),\n  br(),\n  \n  # 2 x 2 plot grid\n  fluidRow(\n    column(\n      6,\n      h4(\"Histogram\"),\n      plotOutput(\"hist\", height = \"220px\")\n    ),\n    column(\n      6,\n      h4(\"Density plot\"),\n      plotOutput(\"dens\", height = \"220px\")\n    )\n  ),\n  fluidRow(\n    column(\n      6,\n      h4(\"quantile-quantile plot\"),\n      plotOutput(\"qq\", height = \"220px\")\n    ),\n    column(\n      6,\n      h4(\"Cumulative distribution\"),\n      plotOutput(\"ecdf\", height = \"220px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  # Re-sample ONLY when the button is clicked (and use current n)\n  sample_rv &lt;- eventReactive(input$go, {\n    rnorm(input$n)\n  }, ignoreInit = TRUE)\n  \n  # Initialize once so there is something to show before first click\n  observeEvent(TRUE, {\n    if (is.null(isolate(sample_rv()))) {\n      isolate({\n        # seed-free initial draw; changes when button is pressed\n        assign(\"._init_x\", rnorm(isolate(input$n)), envir = .GlobalEnv)\n      })\n    }\n  }, once = TRUE)\n  \n  x &lt;- reactive({\n    z &lt;- sample_rv()\n    if (is.null(z)) get(\"._init_x\", envir = .GlobalEnv) else z\n  })\n  \n  output$subtitle &lt;- renderText({\n    paste0(\"A sample of size \", length(x()), \" from the standard normal distribution\")\n  })\n  \n  output$hist &lt;- renderPlot({\n    ggplot(data.frame(x = x()), aes(x)) +\n      geom_histogram(color = \"black\", fill = \"grey40\", alpha = 0.7, bins = max(6, round(sqrt(length(x()))))) +\n      labs(x = \"x\", y = \"count\") +\n      theme_minimal()\n  }, res = 150)\n  \n  output$dens &lt;- renderPlot({\n    ggplot(data.frame(x = x()), aes(x)) +\n      geom_density(fill = \"grey60\", alpha = 0.5) +\n      labs(x = \"x\", y = \"density\") +\n      theme_minimal()\n  }, res = 150)\n  \n  output$qq &lt;- renderPlot({\n    ggplot(data.frame(x = x()), aes(sample = x)) +\n      stat_qq(size = 1.6) +\n      stat_qq_line() +\n      labs(x = \"theoretical\", y = \"sample\") +\n      theme_minimal()\n  }, res = 150)\n  \n  output$ecdf &lt;- renderPlot({\n    ggplot(data.frame(x = x()), aes(x)) +\n      stat_ecdf(geom = \"step\", linewidth = 0.9) +\n      coord_cartesian(ylim = c(0, 1)) +\n      labs(x = \"x\", y = \"y\") +\n      theme_minimal()\n  }, res = 150)\n}\n\nshinyApp(ui, server)\n\n\nExamples of a sample not from a normal distribution\nLet’s compare the samples from a normal distribution, in our shinyapp above, to cases in which the data are not normal. For example,\n\nFigure 3 A-D makes it clear that across the three Iris species, petal length is bimodal. -Figure 3 E-H makes it clear that across all mammals the distribution of body weights are exponentially distributed.\n\nThese examples are a bit extreme. Over the term, we’ll get practice in visually assessing if data are normal-ish.\n\n\n\n\n\n\n\n\nFigure 3: A-D. The distribution of petal lengths for the combined iris dataset. The histogram and density plot clearly show a bimodal distribution with two distinct groups, which causes the points on the QQ plot to form an S-curve rather than a straight line. E-H. The distribution of mammal body weights. All four plots reveal a strong right-skew, characteristic of an exponential-like distribution. This is seen in the L-shaped histogram, the long right tail of the density plot, and the pronounced upward curve of the points in the QQ plot.",
    "crumbs": [
      "15. Normal distribution",
      "• 15. Is It Normal?"
    ]
  },
  {
    "objectID": "book_sections/normal/clt.html",
    "href": "book_sections/normal/clt.html",
    "title": "• 15. The Normal is Common",
    "section": "",
    "text": "Why Normal Distributions Are Common\nOne amazing thing about the world is just how frequently normal distributions occur. The reason for this is that whenever a value results from adding up MANY INDEPENDENT factors, that value will follow a normal distribution, regardless of the underlying distribution of these individual factors. For example, your height is influenced by many genes in your genome, as well as numerous environmental factors, all contributing to this outcome.\nAn important consequence of this is that the sampling distribution of means tends to be normally distributed, provided the sample size isn’t too small. This principle, known as the Central Limit Theorem, is very useful in statistics. It allows us to create reasonable statistical models of sample means by assuming normality, even when the underlying data may not be perfectly normal.",
    "crumbs": [
      "15. Normal distribution",
      "• 15. The Normal is Common"
    ]
  },
  {
    "objectID": "book_sections/normal/clt.html#why-normal-distributions-are-common",
    "href": "book_sections/normal/clt.html#why-normal-distributions-are-common",
    "title": "• 15. The Normal is Common",
    "section": "",
    "text": "A Galton board! At every peg, a bead has a 50/50 chance of bouncing left or right. The final position of the bead in a bin at the bottom is the sum of all these random left and right steps. Most often paths right and left even out and the bead lands in the center, but not always! This ultimately generates a normal distribution.\n\n\nThe Central Limit Theorem is crucial for statistics because many of the statistical analyses we perform, which assume normality, are still valid even if the underlying data are not perfectly normal. This central limit theorem is remarkably useful because it means we can use statistical tests that assume normality (like the t-test) to make inferences about the mean, even if our raw data isn’t quite normally distributed.\n\n\nHow Large Must a Sample Be for Us to Trust the Central Limit Theorem?\nThe Central Limit Theorem assures us that with a sufficiently large sample size, the sampling distribution of means will be normal, regardless of the distribution of the underlying data points. But how large is sufficiently large? The answer depends on how far from normal the initial data are. The less normal the original data, the larger the sample size needed before the sampling distribution becomes normal.\nThe webapp below is a simulation of the sampling distribution to help you build an intuition for the Central Limit Theorem. It lets you draw samples from four different variables from our parviflora RILs plants at GC.\n\nThe top row of plots always shows the shape of the original data.\nThe bottom row shows the sampling distribution of the mean, which is built from the averages of 1000 different samples.\n\nUse the Sample Size (n) slider for each variable, and evaluate how large n needs to be before the points form a straight line, signaling that the sampling distribution has become approximately normal.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| column: page-right\n#| standalone: true\n#| viewerHeight: 1000\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(cowplot)\n\n# --- UI Definition ---\nui &lt;- fluidPage(\n    titlePanel(\"The Central Limit Theorem in Action\"),\n    sidebarLayout(\n        sidebarPanel(\n            selectInput(\"var\", \"Population Distribution:\",\n                        choices = c(\"Petal Area\" = \"petal_area_mm\",\n                                    \"Prop. Hybrid\" = \"prop_hybrid\",\n                                    \"Mean Visits\" = \"mean_visits\",\n                                    \"Pink Flowers\" = \"pink_flowers\")),\n            selectInput(\"n\", \"Sample Size (n):\",\n                        choices = c(\"2\", \"5\", \"10\", \"25\", \"50\", \"100\"),\n                        selected = \"25\"),\n            hr(),\n            helpText(\"We take 1000 random samples and calculate the mean for each.\")\n        ),\n        mainPanel(\n            plotOutput(\"distPlot\", height = \"600px\")\n        )\n    )\n)\n\n# --- Server Logic ---\nserver &lt;- function(input, output) {\n    \n    # Load data once\n    dataset &lt;- reactive({\n        ril_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_rils.csv\"\n        df &lt;- readr::read_csv(ril_link) %&gt;%\n              mutate(growth_rate = ifelse(growth_rate == \"1.8O\", \"1.80\", growth_rate),\n                     growth_rate = as.numeric(growth_rate),\n                     visited = mean_visits &gt; 0,\n                     pink_flowers = as.numeric(petal_color == \"pink\")) %&gt;%\n              filter(location == \"GC\") %&gt;%\n              select(petal_area_mm, pink_flowers, mean_visits, prop_hybrid) %&gt;%\n              drop_na()\n        df\n    })\n\n    population_dist &lt;- reactive({\n        req(input$var, dataset())\n        tibble(x = dataset()[[input$var]])\n    })\n\n    sampling_dist &lt;- reactive({\n        req(population_dist(), input$n)\n        pop_vec &lt;- population_dist()$x\n        n_val &lt;- as.numeric(input$n)\n        \n        means &lt;- replicate(1000, {\n            mean(sample(pop_vec, size = n_val, replace = TRUE))\n        })\n        tibble(mean_x = means)\n    })\n\n    output$distPlot &lt;- renderPlot({\n        pop_data &lt;- population_dist()\n        samp_dist &lt;- sampling_dist()\n        \n        # Population Plots\n        pop_hist &lt;- ggplot(pop_data, aes(x = x)) +\n            geom_histogram(bins = 30, color = \"white\", fill = \"pink\") +\n            labs(x = \"Observed Values\", title = \"Actual Data (Population)\") +\n            theme_minimal(base_size = 14)\n\n        pop_qq &lt;- ggplot(pop_data, aes(sample = x)) +\n            geom_qq(color = \"pink\") +\n            geom_qq_line(color = \"pink\") +\n            labs(title = \"Actual Data QQ\") +\n            theme_minimal(base_size = 14)\n\n        # Sampling Plots\n        samp_hist &lt;- ggplot(samp_dist, aes(x = mean_x)) +\n            geom_histogram(bins = 30, color = \"white\", fill = \"#3b82f6\") +\n            labs(x = \"Sample Means\", title = \"Sampling Distribution\") +\n            theme_minimal(base_size = 14)\n\n        samp_qq &lt;- ggplot(samp_dist, aes(sample = mean_x)) +\n            geom_qq(color = \"#3b82f6\") +\n            geom_qq_line(color = \"#3b82f6\") +\n            labs(title = \"Sampling Dist QQ\") +\n            theme_minimal(base_size = 14)\n\n        plot_grid(pop_hist, pop_qq, samp_hist, samp_qq, ncol = 2)\n    })\n}\n\nshinyApp(ui = ui, server = server)\n\n\nFor each of the populations below, use the app to find the smallest sample size (n) where the sampling distribution of the mean becomes approximately normal (i.e., the QQ plot is a straight line).\n\nQ1. What is the minimum sample size for the petal area data? 2525100\n\n\nExplanation\n\nAt n=25, the QQ plot is reasonably straight, showing the CLT has taken effect. For smaller sample sizes, the QQ plot still shows some curvature. I couldn’t tell if the rright answer was 10, 25 or 50 so I split the difference\n\n\nQ2. What is the minimum sample size for the proportion pink data? 251025\n\n\nExplanation\n\nThe sampling distribution only starts to look continuous and normal-like when the sample size is large enough. At n=25, the QQ plot straightens out nicely.\n\n\nQ3. What is the minimum sample size for the highly skewed pollinator visits data? 2525100\n\n\nExplanation\n\nThe original data is very skewed, so a large sample size is needed for the CLT to work. At n=25, the sampling distribution is still visibly skewed, but by n=100, it becomes much more symmetric and bell-shaped.",
    "crumbs": [
      "15. Normal distribution",
      "• 15. The Normal is Common"
    ]
  },
  {
    "objectID": "book_sections/normal/transform.html",
    "href": "book_sections/normal/transform.html",
    "title": "• 15. Make It Normal",
    "section": "",
    "text": "Motivating scenario: After looking at your data you can tell it’s pretty far from normal. You know that simply ignoring this violation could lead to incorrect conclusions. What do you do? Data transformation is valid way to change the shape of your data so that it better meets the assumptions of your statistical model.\nLearning goals: By the end of this chapter you should be able to:\n\nExplain the primary reasons for transforming data in statistical analysis.\n\nList the key principles that make a data transformation legitimate.\n\nIdentify common transformations used to correct for right skew, left skew, and to analyze proportions.\n\nEvaluate whether a transformation made the data normal enough.\n\n\n\nTransforming Data\nBecause the normal distribution is so common, and because the Central Limit Theorem is so useful, many statistical approaches are built on the assumption of some form of normality. However, sometimes data are too far from normal to be modeled as if they are normal. Other times, certain characteristics of the data’s distribution may break other assumptions of statistical tests. When this happens, we have a few options:\n\nWe can permute and bootstrap!\n\nWe can transform the data to better meet our assumptions.\n\nWe can use or develop tools to model the data based on their actual distribution.\n\nWe can use robust and/or nonparametric approaches.\n\nWe have already discussed option 1 at length, and will return to options 3 and 4 later in the term. Here we visit option 2, transformation, in which we change the shape of our data (We already coverred this some earlier in the book).\n\nRules for Legitimate Transformations\nThere is nothing inherently “natural” about the linear scale, so transforming data to a different scale is perfectly valid. In fact, we should estimate and test hypotheses on a meaningful scale. Often, an appropriate transformation will result in data that are more “normal-ish.” Effective transformations are guided by the characteristics of the data, the processes that generated them, and the specific questions being addressed, ensuring that the transformation makes sense in context.\nRules for Transforming Data:\n\nLet biology guide you: Often, you can determine an appropriate transformation by considering a mathematical model that describes your data. For example, if values naturally grow exponentially, a log transformation may be appropriate.\n\nApply the same transformation consistently to each individual in the dataset.\n\nTransformed values must have a one-to-one correspondence with the original values. For example, don’t square values if some are less than 0 and others greater than 0.\n\nTransformed values must maintain a monotonic relationship with the original values. This means that the order of the values should be preserved after the transformation—larger values in the original data should remain larger after the transformation.\n\nConduct your statistical tests AFTER you settle on the appropriate transformation.\n\nBe cautious not to bias your results by inadvertently losing data points. This can happen, for example, if a log transformation fails on zero or negative values, or when extreme values are removed during the transformation process.\n\n\n\nCommon Transformations\nThere are several common transformations that can make data more normal, depending on their initial shape:\n\n\n\nName\nFormula\nWhat type of data?\n\n\n\n\nLog\n\\(Y'=\\log_x(Y + \\epsilon)\\)\nRight skewed\n\n\nSquare-root\n\\(Y'=\\sqrt{Y+1/2}\\)\nRight skewed\n\n\nReciprocal\n\\(Y'=1/Y\\)\nRight skewed\n\n\nArcsine\n\\(\\displaystyle p'=arcsin[\\sqrt{p}]\\)\nProportions\n\n\nSquare\n\\(Y'=Y^2\\)\nLeft skewed\n\n\nExponential\n\\(\\displaystyle Y'=e^Y\\)\nLeft skewed\n\n\n\n\nTransformation example: The log transformation\nWe have seen that the normal distribution arises when we add up a bunch of things. If we multiply a bunch of things, we get an exponential distribution. Because adding logs is like multiplying untransformed data, a log transform makes exponential data look normal.\nTake our distribution of the petal area of parviflora RILs we’ve been using all chapter. You will note that we have been using the log-scaled data. We have done so because petal area is initially far from normal (Fig. Figure 1.1 A,C), but after a log transformation it is much closer to normal (Fig. Figure 1.1 B,D) and would likely have a normal sampling distribution for relatively modest sample sizes.\n\n# Tranfsorm\ngc_rils &lt;-gc_rils |&gt;\n    mutate(log10_petal_area = log10(petal_area_mm))\n\n\n\nCode to make the plots below\nlibrary(patchwork)\nmean_val_raw &lt;- mean(gc_rils$petal_area_mm, na.rm = TRUE)\nsd_val_raw &lt;- sd(gc_rils$petal_area_mm, na.rm = TRUE)\n\nraw_hist &lt;- ggplot(gc_rils, aes(x = petal_area_mm)) +\n    geom_histogram(aes(y = after_stat(density)), bins = 11, color = \"white\", fill = \"gray\") +\n    stat_function(fun = dnorm,\n        args = list(mean = mean_val_raw, sd = sd_val_raw),\n        color = \"red\", linewidth = 1.2) +\n    labs(title = \"Hist. of Petal Area with Normal Curve\", x = \"Petal Area (mm)\",y = \"Density\") +\n    scale_x_continuous(limits = c(10,130))+\n  theme(axis.text = element_text(size = 14),\n        axis.title = element_text(size = 18),\n        title = element_text(size = 18))\n\n# log10 transform\nmean_val_log10 &lt;-mean(gc_rils$log10_petal_area, na.rm = TRUE)\nsd_val_log10 &lt;- sd(gc_rils$log10_petal_area, na.rm = TRUE)\n\nlog10_hist &lt;- ggplot(gc_rils, aes(x = log10_petal_area)) +\n    geom_histogram(aes(y = after_stat(density)), bins = 11, color = \"white\", fill = \"gray\") +\n    stat_function(fun = dnorm,\n        args = list(mean = mean_val_log10, sd = sd_val_log10),\n        color = \"red\", linewidth = 1.2) +\n    labs(title = \"Hist. of log10(Petal Area) w. Normal Curve\", x = \"log10(Petal Area) (mm)\",y = \"Density\") +\n    scale_x_continuous(limits = c(1.4,2.2))+\n  theme(axis.text = element_text(size = 14),\n        axis.title = element_text(size = 18),\n        title = element_text(size = 18))\n\nraw_qq &lt;- ggplot(gc_rils, aes(sample = petal_area_mm))+\n  geom_qq()+\n  geom_qq_line()+\n  labs(title = \"QQ plot of Petal Area\", y = \"Petal Area (mm)\",x = \"Theoretical quantile\") +\n  theme(axis.text = element_text(size = 14),\n        axis.title = element_text(size = 18),\n        title = element_text(size = 18))\n\nlog10_qq  &lt;- ggplot(gc_rils, aes(sample = log10_petal_area))+\n  geom_qq()+\n  geom_qq_line()+\n  labs(title = \"QQ plot of log10(Petal Area)\", y = \"log10(Petal Area) (mm)\",x = \"Theoretical quantile\") +\n  theme(axis.text = element_text(size = 14),\n        axis.title = element_text(size = 18),\n        title = element_text(size = 18))\nraw_hist + raw_qq + log10_hist  + log10_qq+plot_annotation(tag_levels  = \"A\")\n\n\n\n\n\n\n\n\nFigure 1.1: A. The histogram of the raw petal area data shows a slight right skew, and the theoretical normal curve (red line) is not a great fit. B. The corresponding QQ plot also shows this deviation from normality, as the points form a distinct curve away from the straight line. C. After a log10 transformation, the histogram of the data is much more symmetric and bell-shaped, better matching the overlaid normal curve. D. The QQ plot of the log10 transformed data looks better.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.2: A meme to contrast different approaches to statistical analysis, based on perceived complexity versus wisdom. Permutation and permutation (when properly reflecting the structure of our data are the solustion to both simple and complex stats problems.)\n\n\n\n\n\nBe careful when log-transforming!! All data with a value of zero or less will disappear. For this reason, we often use a log1p transform, which adds one to each number before logging them.\n\n\n\n\nWhen transformation fails\nNot all data can be made to be normal. If you are worried that even after transformation your data are too far from normal for standard linear models, don’t despair! We can build modles that better meet the data, permute/bootstrap (Figure 1.2), or find a better test.",
    "crumbs": [
      "15. Normal distribution",
      "• 15. Make It Normal"
    ]
  },
  {
    "objectID": "book_sections/normal/normal_summary.html",
    "href": "book_sections/normal/normal_summary.html",
    "title": "• 15. Normal Summary",
    "section": "",
    "text": "Chapter summary\nLinks to: Summary. Chatbot tutor. Questions. Glossary. R packages. R functions. More resources.\nThe Normal Distribution is a symmetric, bell-shaped curve defined by its mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)). Its importance in statistics comes from it’s frequent use in statistics, which is justified by the Central Limit Theorem, which states that the regardless of the distribution of the population parameter, the sampling distribution of the mean will be approximately normal as the sample size gets large. We can standardize any normal distribution using a Z-transformation to calculate Z-scores, which allows us to find probabilities using the Probability Density Function of the Standard Normal Distribution. We assess if data are normal byusing a Quantile-Quantile (QQ) Plot and can apply transform data to make it normal.",
    "crumbs": [
      "15. Normal distribution",
      "• 15. Normal Summary"
    ]
  },
  {
    "objectID": "book_sections/normal/normal_summary.html#normal_summary_chapter-summary",
    "href": "book_sections/normal/normal_summary.html#normal_summary_chapter-summary",
    "title": "• 15. Normal Summary",
    "section": "",
    "text": "Chatbot tutor\n\nPlease interact with this custom chatbot (link here). I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you got what you needed from it.",
    "crumbs": [
      "15. Normal distribution",
      "• 15. Normal Summary"
    ]
  },
  {
    "objectID": "book_sections/normal/normal_summary.html#normal_summary_practice-questions",
    "href": "book_sections/normal/normal_summary.html#normal_summary_practice-questions",
    "title": "• 15. Normal Summary",
    "section": "Practice Questions",
    "text": "Practice Questions\nTry these questions! By using the R environment you can work without leaving this “book”. I even pre-loaded all the packages you need!\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nSETUP: The weights of singleton (that is, not twins) babies born in the U.S. are (roughly) normally distributed with a mean of 3.339 kg with a standard deviation of 0.573 kg.\n\nQ1. Roughly how many standard deviations away from the mean is a 5kg baby? \n\n\nExplanation\n\nWe calculate the Z-score: \\(Z = (5 - 3.339) / 0.573 = 1.661 / 0.573 \\approx 2.9\\). This is roughly 3 standard deviations.\n\n\nQ2. What is the name of the procedure you did to find the answer above? The Central Limit TheoremThe q-transformThe log-transformThe Z-transform\n\n\nExplanation\n\nThe Z-transform is the procedure of taking a value, subtracting the mean, and dividing by the standard deviation to find out how many standard deviations away it is.\n\n\nQ3. Use the appropriate _norm() function to find the probability density that a baby is 5 kg.\n\n\n\nExplanation\n\nWe use the dnorm() function for probability density: dnorm(x = 5, mean = 3.339, sd = 0.573), which equals 0.0105.\n\n\nQ4. The answer above describes the proportion of babies that will weigh 5 kg at birth. TRUEFALSE\n\n\nExplanation\n\nFalse. Probability density is not a direct probability or proportion. For a continuous variable, the probability of any exact value is zero. The density is the height of the curve, and the area under the curve gives the probability.\n\n\nQ5. Use the appropriate _norm() function to find the probability that a baby is greater than 5 kg. \n\n\nExplanation\n\nWe use the pnorm() function for cumulative probability. To find the area in the upper tail (greater than), we set lower.tail = FALSE: pnorm(q = 5, mean = 3.339, sd = 0.573, lower.tail = FALSE), which equals 0.0018.\n\n\nQ6. What percentage of babies are between three and four kg? 40%50%60%70%\n\n\nExplanation\n\nWe find the area below 4 kg and subtract the area below 3 kg: pnorm(4, 3.339, 0.573) - pnorm(3, 3.339, 0.573) = 0.875 - 0.277 = 0.598, or about 60%.\n\n\nQ7. Imagine we took five babies born in a given hospital and calculated their mean weight. The standard deviation of this sampling distribution would equal: 0.110.150.25\n\n\nExplanation\n\nThe standard deviation of the sampling distribution is the standard error, calculated as \\(\\sigma / \\sqrt{n}\\). So, \\(0.573 / \\sqrt{5} = 0.256\\).",
    "crumbs": [
      "15. Normal distribution",
      "• 15. Normal Summary"
    ]
  },
  {
    "objectID": "book_sections/normal/normal_summary.html#normal_summary_glossary-of-terms",
    "href": "book_sections/normal/normal_summary.html#normal_summary_glossary-of-terms",
    "title": "• 15. Normal Summary",
    "section": "📊 Glossary of Terms",
    "text": "📊 Glossary of Terms\n\n\nNormal Distribution: A continuous probability distribution characterized by a symmetric, bell-shaped curve. It is defined by its mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)). Also known as the Gaussian distribution.\nProbability Density Function (PDF): A function for continuous random variables where the area under the curve between two points represents the probability of the variable falling within that range. The total area under the curve is 1, but the density at a single point can be greater than 1.\nProbability Mass Function (PMF): A function for discrete random variables that gives the probability of a specific outcome. The sum of all probabilities from a PMF is 1.\n\nQuantile-Quantile (QQ) Plot: A plot of the quantiles of the observed data against the theoretical quantiles of a perfect normal distribution. This is used to assess if a dataset is approximately normally distributed – if the data are normal, the points fall along a straight line.\n\nSampling Distribution (of the mean): The probability distribution of a statistic (like the mean) obtained from a large number of random samples drawn from a specific population.\n\nStandard Normal Distribution: A normal distribution with \\(\\mu=0\\), and \\(\\sigma=1\\).\n\nCentral Limit Theorem (CLT): A fundamental theorem in statistics which states that the sampling distribution of the mean of a sufficiently large number of samples will be approximately normally distributed, regardless of the shape of the original population’s distribution.\n\n\nParameters & Estimates\n\nMean (\\(\\mu\\)): A measure of central tendency, calculated as the average of all values in a dataset. For a normal distribution, the mean is the center and peak of the curve.\nVariance (\\(\\sigma^2\\)): The spread of data points around the mean: \\(\\frac{\\sum(x_i-\\mu)}{n}\\).\nStandard Deviation (\\(\\sigma\\)): The square root of the variance.\nStandard Error (\\(\\sigma_\\bar{x}\\)): The standard deviation of the sampling distribution. For the sample mean, of a normally distributed variable, it is calculated as \\(\\sigma_\\bar{x} =\\frac{\\sigma}{\\sqrt{n}}\\).\n\n\n\nProbability Rules\n\nMutually Exclusive Events: Two events that cannot occur at the same time. For example, a coin flip cannot be both heads and tails.\n\nAddition Rule: A probability rule stating that for mutually exclusive events, the probability of one event (A) or another event (B) occurring is the sum of their individual probabilities. \\(P(A \\text{ or } B) = P(A) + P(B)\\).\nComplement Rule: A probability rulw stating that the probability of an event not occurring is equal to 1 minus the probability that the event does occur. \\(P(\\text{not } A) = 1 - P(A)\\).\n\n\n\nTransformations & Tests\n\nData Transformation: The process of applying a mathematical function to each point in a dataset (e.g., taking the logarithm or square root) to change the mean, variance and/or shape of the data’s distribution.\nZ-score: A value that indicates how many standard deviations an observation is from the mean of its distribution. It is calculated using the Z-transformation.\nZ-transformation: Converting a value, \\(x_i\\), from any normal distribution (with mean \\(\\mu\\) and variance, \\(\\sigma^2=1\\)) into a Z-score, \\(z_i\\) from the “standard normal distribution” (with \\(\\mu=0\\) and \\(\\sigma^2 = 1\\)): \\(z_i = (x_i - \\mu) / \\sigma\\).\nZ-test: A test of the null hypothesis that a population’s true mean equals its proposed null value, \\(\\mu_0\\). To do we find the distance (in standard errors) between our estimate, \\(\\bar{x}\\), and its null value \\(\\mu_0\\), as \\(Z=\\frac{\\bar{x}-\\mu_0}{\\sigma_\\bar{x}}\\), where \\(\\sigma_\\bar{x}\\) is the standard error. We then find the two-tailed p-value as 2 * pnorm(q=abs(Z), sd =1, lower.tail = TRUE).",
    "crumbs": [
      "15. Normal distribution",
      "• 15. Normal Summary"
    ]
  },
  {
    "objectID": "book_sections/normal/normal_summary.html#normal_summary_key-r-functions",
    "href": "book_sections/normal/normal_summary.html#normal_summary_key-r-functions",
    "title": "• 15. Normal Summary",
    "section": "🛠️ Key R Functions",
    "text": "🛠️ Key R Functions\n\n\ndnorm(): An R function that calculates the density (the height of the curve) of a normal distribution at a specific point.\n\npnorm(): An R function that calculates the cumulative probability (the area under the curve from \\(-\\infty\\) to a given value) for a normal distribution. It finds \\(P(X \\le q)\\).\n\nqnorm(): An R function that calculates the quantile for a given cumulative probability in a normal distribution. It is the inverse of pnorm() and is used to find critical values.\n\nrnorm(): An R function used to generate random numbers from a specified normal distribution.\ngeom_qq(): A ggplot2 function that creates the scatter plot of points for a Quantile-Quantile plot (note in aes, set sample = VARIABLE_OF_INTEREST).\n\ngeom_qq_line(): A ggplot2 function that adds the straight reference line to a Quantile-Quantile plot, representing where the points would fall if the data were perfectly normal.",
    "crumbs": [
      "15. Normal distribution",
      "• 15. Normal Summary"
    ]
  },
  {
    "objectID": "book_sections/normal/normal_summary.html#normal_summary_additional-resources",
    "href": "book_sections/normal/normal_summary.html#normal_summary_additional-resources",
    "title": "• 15. Normal Summary",
    "section": "Additional resources",
    "text": "Additional resources\n\nVideos:\n\nThe Normal Distribution: Crash Course Statistics #19: A clear description of the normal disitrbution, how it arises, its properties, and the central lmit theorem. This is a nice summary of the key concepts in this chapter.\nBut what is the Central Limit Theorem? from 3Blue1Brown’s youtube page]: An approachable introduction to more technical aspects of the normal distribution and the central limit theorm. This is the first video in his youtube playlist on the central limit theorem.\nKhan Academy Central Limit Theorem.",
    "crumbs": [
      "15. Normal distribution",
      "• 15. Normal Summary"
    ]
  },
  {
    "objectID": "book_sections/t.html",
    "href": "book_sections/t.html",
    "title": "16. The t distribution",
    "section": "",
    "text": "The Dilemma\nThis allowed us to summarize the distance between an estimated mean, \\(\\bar{x}\\), and its null value, \\(\\mu_0\\), in standardized units (standard errors), regardless of the mean and variance. This is super useful, because e.g. we can conduct null hypothesis significance testing and the like.\nHowever, there’s a problem: We usually have only a sample and not the whole population (that’s why we’re doing statistics after all!). So have an estimate of the standard deviation \\(s\\), and not its true parameter value \\(\\sigma\\). This means our standard error, too, is an estimate associated with uncertainty. Unfortunately, the Z-distribution does not account for this additional uncertainty.",
    "crumbs": [
      "16. The t distribution"
    ]
  },
  {
    "objectID": "book_sections/t.html#the-dilemma",
    "href": "book_sections/t.html#the-dilemma",
    "title": "16. The t distribution",
    "section": "",
    "text": "We previously explored samples drawn from a normal distribution. A key takeaway was the Z-transformation: .\n\n\n\nFor a single observation, \\(z = \\frac{x - \\mu}{\\sigma}\\).\n\nFor the mean of a sample of size \\(n\\), \\(Z = \\frac{\\overline{x} - \\mu_0}{\\sigma / \\sqrt{n}}\\).",
    "crumbs": [
      "16. The t distribution"
    ]
  },
  {
    "objectID": "book_sections/t.html#the-solution",
    "href": "book_sections/t.html#the-solution",
    "title": "16. The t distribution",
    "section": "The Solution",
    "text": "The Solution\nDon’t despair! The t-distribution can rescue us!!!\n\nLike the standard normal (“Z”) distribution, the t-distribution is unimodal and symmetric.\n\nBut, to account for the additional uncertainty that comes from using the sample standard deviation (\\(s\\)) instead of the true population value (\\(\\sigma\\)), the t-distribution has “fatter tails” than the Z distribution.\n\nBecause the tails are “fatter”, a larger proportion of its area in the tails beyond \\(\\pm2\\) (greater than 4.55%, compared to exactly 4.55% for the Z-distribution).",
    "crumbs": [
      "16. The t distribution"
    ]
  },
  {
    "objectID": "book_sections/t.html#degrees-of-freedom",
    "href": "book_sections/t.html#degrees-of-freedom",
    "title": "16. The t distribution",
    "section": "Degrees of freedom",
    "text": "Degrees of freedom\n\n\n\n\n\n\n\n\nFigure 1: Comparing the t (black) and standard normal (Z, red) distributions for different degrees of freedom.\n\n\n\n\n\nNow you might be wondering:\n\nHow “fat” are the tails of the t-distribution? and\n\nWhat proportion of samples from a t-distribution are more than two standard errors away from the population mean?\n\nThe answers to these questions depend on which t-distribution we’re talking about. There are many each associated with a certain number of degrees of freedom.\nThe degrees of freedom reflect the number of observations that can vary before all values are determined, which is related to our uncertainty in our estimate – the more the degrees of freedom, the less uncertainty! As such, tails get less “fat” and the t-distribution looks more and more like the standard normal (z) distribution as the degrees of freedom increase (Figure 1).",
    "crumbs": [
      "16. The t distribution"
    ]
  },
  {
    "objectID": "book_sections/t.html#t-as-a-common-test-statistic",
    "href": "book_sections/t.html#t-as-a-common-test-statistic",
    "title": "16. The t distribution",
    "section": "t as a Common Test Statistic",
    "text": "t as a Common Test Statistic\nSince most sampling distributions are normal, but we rarely know the true population standard deviation, t-values are common in applied statistics. Each time we encounter a t-value, it tells us how many standard errors our estimate is away from the hypothesized parameter under the null hypothesis.\nPut simply t is the distance (in standard errors) between our estimate and its proposed value under the null (incorporating uncertainty in our estimate of the standard deviation)\n\nYou may see Z-tests in the literature. Z tests are never the “right” thing to do, because we never have a population, but in some cases they aren’t meaningfully “wrong”. As sample sizes get large, the t and z distributions converge and so they will give essentially the same answer. For convenience you may see people use the z test in such cases, and that’s fine by me.",
    "crumbs": [
      "16. The t distribution"
    ]
  },
  {
    "objectID": "book_sections/t.html#whats-ahead",
    "href": "book_sections/t.html#whats-ahead",
    "title": "16. The t distribution",
    "section": "What’s Ahead",
    "text": "What’s Ahead\nIn this chapter we introduce the t-distribution.\n\nWe begin by introducing a non-Clarkia dataset to motivate our discussion.\nNext, we review standard summaries of single continuous variables, and consider the assumptions required to justify using the t-distribution to model such data.\nWe then calculate 95% confidence intervals for data modeled by the t-distribution, and show how the “one sample” t-test can be used to evaluate the null hypothesis that a sample comes from a population with mean $_0$. After working through the math and logic of this “one-sample t-test,” we see how R can do most of this heavy lifting for us.\nFinally, we work through the “paired” t-test, a particularly useful version of the one-sample test that asks whether the mean difference between paired observations under different treatments differs from zero (i.e., no difference).\nAs always, we conclude with a chapter summary.",
    "crumbs": [
      "16. The t distribution"
    ]
  },
  {
    "objectID": "book_sections/t/t_example.html",
    "href": "book_sections/t/t_example.html",
    "title": "• 16. t: Example Data",
    "section": "",
    "text": "Set up and scientific hypothesis\nAs the climate warms, the habitats suitable for many species are moving. Species can respond in several ways: they can adapt to the new conditions, tolerate them, or move to track their preferred environment. If they can’t do any of these, they may go extinct locally. A study (Chen et al. (2011)) combined data for over thirty taxonomic groups to address a key question: ’Do species, on average, move to higher elevations to keep up with a changing climate?’”",
    "crumbs": [
      "16. The t distribution",
      "• 16. t: Example Data"
    ]
  },
  {
    "objectID": "book_sections/t/t_example.html#hypotheses",
    "href": "book_sections/t/t_example.html#hypotheses",
    "title": "• 16. t: Example Data",
    "section": "Hypotheses",
    "text": "Hypotheses\n\nNull Hypothesis (\\(H_0\\)): The true mean elevational shift (\\(\\mu\\)) is zero. Any observed shift from our sample is due to random chance (\\(H_0: \\mu=0\\)).\n\nAlternative Hypothesis (\\(H_A\\)): The true mean elevational shift (\\(\\mu\\)) is not zero. Species are, on average, shifting their elevation (\\(H_A: \\mu \\neq 0\\)).\n\nNote that although the scientific hypothesis was one-tailed, it is common practice to test the two-tailed alternative hypothesis, and a one-tailed test raises suspicions.",
    "crumbs": [
      "16. The t distribution",
      "• 16. t: Example Data"
    ]
  },
  {
    "objectID": "book_sections/t/t_example.html#the-data",
    "href": "book_sections/t/t_example.html#the-data",
    "title": "• 16. t: Example Data",
    "section": "The data",
    "text": "The data\nThe data , are available here and represent the rate of upward or downward movement of a species’ range, measured in meters per decade. Each data point is the mean elevation shift for a particular taxonomic group in a specific geographic region (e.g., plants in the Green Mountains in Vermont - Figure 1). A positive value indicates a species group has moved uphill to a higher elevation, while a negative value means it has moved downhill to a lower elevation.\n\n\nCode\nlibrary(tidyr)\nlibrary(dplyr)\nrange_shift_file &lt;- \"https://whitlockschluter3e.zoology.ubc.ca/Data/chapter11/chap11q01RangeShiftsWithClimateChange.csv\"\nrange_shift &lt;- read_csv(range_shift_file) |&gt;\n  mutate(uphill = elevationalRangeShift &gt; 0)|&gt;\n  separate(taxonAndLocation, into = c(\"taxon\", \"location\"), sep = \"_\")\n\n\n\n\n\n\n\n\n\n\n\n\nChen, I.-C., Hill, J. K., Ohlemüller, R., Roy, D. B., & Thomas, C. D. (2011). Rapid range shifts of species associated with high levels of climate warming. Science, 333(6045), 1024–1026. https://doi.org/10.1126/science.1206432",
    "crumbs": [
      "16. The t distribution",
      "• 16. t: Example Data"
    ]
  },
  {
    "objectID": "book_sections/t/summaries_4_t.html",
    "href": "book_sections/t/summaries_4_t.html",
    "title": "• 16. Data summaries for t",
    "section": "",
    "text": "Data visualization\nWe have previously considered standard summaries of univariate data. Because the \\(t\\)-distribution assumes a normal distribution, we focus on standard parametric estimates. These include:\nI calculate these below:\nAs a reminder\nSo, our observed Cohen’s D of 1.28 means that this effect is quite strong!\nWe will develop a few visualizations of our data. For now, I present a simple histogram. Figure 1, clearly shows that most species have moved uphill. It also allows us to evaluate if the data are normalish, as assumed by the t distribution. We spend the next section thinking harder about assumptions of the t distribution, and if our data.\nCode to make our histogram\nrange_shift |&gt;\n ggplot(aes(x = elevationalRangeShift ))+\n geom_histogram(color = \"white\",\n                breaks = seq(-20,110,10))+\n  geom_vline(color = \"red\", lty = 2, xintercept = 0)+\n  labs(x = \"Per decade increase in altitude (meters).\")\nFigure 1: A histogram showing the distribution of observed elevational range shifts (in meters per decade) for 30 taxonomic groups. The vertical red line indicates a shift of zero, the value specified by the null hypothesis. Data from Chen et al. (2011).",
    "crumbs": [
      "16. The t distribution",
      "• 16. Data summaries for t"
    ]
  },
  {
    "objectID": "book_sections/t/summaries_4_t.html#data-visualization",
    "href": "book_sections/t/summaries_4_t.html#data-visualization",
    "title": "• 16. Data summaries for t",
    "section": "",
    "text": "Chen, I.-C., Hill, J. K., Ohlemüller, R., Roy, D. B., & Thomas, C. D. (2011). Rapid range shifts of species associated with high levels of climate warming. Science, 333(6045), 1024–1026. https://doi.org/10.1126/science.1206432",
    "crumbs": [
      "16. The t distribution",
      "• 16. Data summaries for t"
    ]
  },
  {
    "objectID": "book_sections/t/t_assumptions.html",
    "href": "book_sections/t/t_assumptions.html",
    "title": "• 16. Assumptions of t",
    "section": "",
    "text": "Assumptions in statistical models\nModeling uncertainty and testing null hypotheses requires an appropriate sampling distribution. But remember our challenge - we are stuck with a single sample so we don’t have a sampling distribution. To make this distribution, we must make some assumptions. For resampling methods like bootstrapping and permutation, we assume the data are independent and collected without bias. Parametric methods, like the t-test, use a well-defined statistical distribution (here, the t-distribution) as a shortcut to get a sampling distribution. This shortcut is only valid if its own assumptions are met. When using the t-distribution, we assume:",
    "crumbs": [
      "16. The t distribution",
      "• 16. Assumptions of t"
    ]
  },
  {
    "objectID": "book_sections/t/t_assumptions.html#assumptions-in-statistical-models",
    "href": "book_sections/t/t_assumptions.html#assumptions-in-statistical-models",
    "title": "• 16. Assumptions of t",
    "section": "",
    "text": "Data are independent.\n\nData are collected without bias.\n\nThe mean is an appropriate summary of the data. AND\n\nData are normally distributed.",
    "crumbs": [
      "16. The t distribution",
      "• 16. Assumptions of t"
    ]
  },
  {
    "objectID": "book_sections/t/t_assumptions.html#what-to-do-when-we-violate-assumptions",
    "href": "book_sections/t/t_assumptions.html#what-to-do-when-we-violate-assumptions",
    "title": "• 16. Assumptions of t",
    "section": "What to Do When We Violate Assumptions",
    "text": "What to Do When We Violate Assumptions\nIf our data meet assumptions, we can trust our inference. If data do not meet assumptions, the validity of our inferences is no longer guaranteed. Rather we must think hard about our data and what we know about our distribution to evaluate whether we can trust our inference (i.e. if our inferences are robust to violations of assumptions) or not.\nThe robustness of our inference depends on which assumptions are broken, and how severe any such violations are. For example, because the central limit theorem states any sampling distribution approaches the normal distribution as sample sizes get large the t-distribution is robust to modest deviations from normality. Here’s what to do if our data violate different assumption:\n\nBias is very difficult to address and is best managed by designing a better study. However, if you fully know the bias of the data, you can try an advanced technique to model this bias.\nWhether the mean is a meaningful summary is a biological question, and a different summary could be evaluated by other methods. For example you could bootstrap the data and generate some other summary of the data if you want.\nNon-independent data can be modeled, but such models are beyond the scope of this chapter.\nThe normality assumption is the easiest to address. If this assumption is violated, we can:\n\nIgnore it (if the violation is minor) because the central limit theorem helps us. OR\n\nTransform the data to an appropriate scale. OR\n\nUse bootstrapping to estimate uncertainty and/or conduct a binomial test, treating values greater than \\(\\mu_0\\) as “successes” and less than \\(\\mu_0\\) as “failures” (covered in future chapters).",
    "crumbs": [
      "16. The t distribution",
      "• 16. Assumptions of t"
    ]
  },
  {
    "objectID": "book_sections/t/t_assumptions.html#evaluating-assumptions-for-our-data",
    "href": "book_sections/t/t_assumptions.html#evaluating-assumptions-for-our-data",
    "title": "• 16. Assumptions of t",
    "section": "Evaluating Assumptions for our Data",
    "text": "Evaluating Assumptions for our Data\n\nAre the data biased? Hopefully not, but this depends on the study design. For example, we might want to know if species had as much opportunity to increase in elevation as to decrease. How were the species selected? etc etc…\nIs the mean a meaningful summary of the data? My sense is yes, but take a look and judge for yourself.\nAre the data independent? I’m not sure about this. We see that some locations and some taxa are there more than once (Figure 1). My sense is this means the data are not actually independent. What if there’s something unique about low-elevation regions in the UK? This isn’t necessarily a reason to stop the analysis, but it’s worth considering.\n\n\n\n\n\n\n\n\n\nFigure 1: A tile plot showing the number of observations for each combination of taxon and geographic location in the dataset. Each colored cell represents a unique study group. The concentration of data points in certain locations (e.g., the UK) and for certain taxa (e.g., plants) highlights the potential non-independence among observations.\n\n\n\n\n\n\nIs the sample (or more specifically, the sampling distribution) normal? My sense is that the data are normal enough! But you can view the data and decide for yourself. As a guide, the image below displays nine quantile-quantile plots – one shows our data while the other eight were generated with rnorm(). Can you easily tell which one is from the real data? I sure can’t, so this seems normal enough to me!\n\n\n\n# Code for qq plot \n\nrange_shift|&gt;\n  ggplot(aes(sample = elevationalRangeShift))+\n  geom_qq()+\n  geom_qq_line()",
    "crumbs": [
      "16. The t distribution",
      "• 16. Assumptions of t"
    ]
  },
  {
    "objectID": "book_sections/t/t_assumptions.html#lfg",
    "href": "book_sections/t/t_assumptions.html#lfg",
    "title": "• 16. Assumptions of t",
    "section": "LFG",
    "text": "LFG\nAlthough the data may not be fully independent, it roughly meets the assumptions of the t-distribution. Now that we’ve evaluated our assumptions let’s get to statin!",
    "crumbs": [
      "16. The t distribution",
      "• 16. Assumptions of t"
    ]
  },
  {
    "objectID": "book_sections/t/t_CI.html",
    "href": "book_sections/t/t_CI.html",
    "title": "• 16. Uncertain-t",
    "section": "",
    "text": "Quantifying Uncertainty\nA major goal of statistics is not just to estimate parameters, but to explicitly communicate the uncertainty in these estimates. This section introduces the concepts and calculations to present confidence intervals for estimates of the mean.",
    "crumbs": [
      "16. The t distribution",
      "• 16. Uncertain-t"
    ]
  },
  {
    "objectID": "book_sections/t/t_CI.html#quantifying-uncertainty",
    "href": "book_sections/t/t_CI.html#quantifying-uncertainty",
    "title": "• 16. Uncertain-t",
    "section": "",
    "text": "Assuming a t-distribution, and data that conform to its assumptions, of course.",
    "crumbs": [
      "16. The t distribution",
      "• 16. Uncertain-t"
    ]
  },
  {
    "objectID": "book_sections/t/t_CI.html#the-standard-error",
    "href": "book_sections/t/t_CI.html#the-standard-error",
    "title": "• 16. Uncertain-t",
    "section": "The Standard Error",
    "text": "The Standard Error\nWe know that the standard error is the standard deviation of the sampling distribution. We also saw that the standard error of the normal distribution is a parameter that equals:\n\\[\\sigma_\\bar{x}=\\frac{\\sigma}{\\sqrt{n}} = \\frac{\\sqrt{\\Sigma{(x_i-\\mu)^2}/n}}{\\sqrt{n}}\\]\nOf course we don’t know \\(\\sigma\\), so to find the standard error of the t-distribution we replace the unknown population standard deviation, \\(\\sigma\\), with our estimate of it, \\(s\\):\n\\[s_\\bar{x}=\\frac{s}{\\sqrt{n}} =  \\frac{\\sqrt{\\Sigma{(x_i-\\bar{x})^2}/(n-1)}}{\\sqrt{n}}\\]\n\n\n\n\n\n\n\nFinding the SE of range shift data\n\n\nmean\nsd\nn\nse\n\n\n\n\n39.33\n30.66\n31\n5.51\n\n\n\n\n\n\n\n# summarize data\nrange_shift_summary &lt;- range_shift |&gt;\n  summarise(mean     = mean(elevationalRangeShift),\n            sd       = sd(elevationalRangeShift),\n            n        = n(),\n            se = sd / sqrt(n))",
    "crumbs": [
      "16. The t distribution",
      "• 16. Uncertain-t"
    ]
  },
  {
    "objectID": "book_sections/t/t_CI.html#degrees-of-freedom",
    "href": "book_sections/t/t_CI.html#degrees-of-freedom",
    "title": "• 16. Uncertain-t",
    "section": "Degrees of Freedom",
    "text": "Degrees of Freedom\nAs you can see the only difference between the standard error of the normal distribution (\\(\\sigma_\\bar{x}\\)) and the standard error of the t distribution (\\(s_\\bar{x}\\)) is that we replace the sample standard deviation, \\(\\sigma\\), with its estimate, \\(s\\). As you can see, the main differences between the formula for the population standard deviation (\\(\\sigma\\)) and the sample standard deviation (\\(s\\)) are:\n\nWe use the known population mean (\\(\\mu\\)) for \\(\\sigma\\) and the estimated sample mean (\\(\\bar{x}\\)) for \\(s\\).\n\nWe divide by \\(n\\) when calculating \\(\\sigma\\), but by \\(n-1\\) when calculating \\(s\\).\n\nIt turns out these two differences are related!!!\nCalculating the mean \\(\\bar{x}\\) from our data means that if we know the mean and all but one observation, we could confidently find the last value with simple algebra. This is why we divide by \\(n-1\\) instead of \\(n\\) – we only have \\(n -1\\) observations that provide information beyond the mean. This concept is directly related to the degrees of freedom!\nThe degrees of freedom indicate how many individual data points we need to know after building our model, to know the rest of the data points. For example, if you have a sample of size:\n\n\\(n = 1\\), and I tell you the sample mean, you know every data point, so there are zero degrees of freedom.\n\n\\(n = 2\\), and I tell you the sample mean, you need one more data point to fill in the rest, leaving one degree of freedom.\n\n\\(n = 3\\), and I tell you the sample mean, you need two more data points to fill in the rest, so there are two degrees of freedom.\n\nThus, when estimating a sample mean \\(\\overline{x}\\), from a sample of size, \\(n\\), the degrees of freedom equal the sample size minus one: \\(\\text{df}_t = n-1\\). So, for our case:\n\n\n\n\n\n\n\nRange shift summary with df!\n\n\nmean\nsd\nn\nse\ndf\n\n\n\n\n39.33\n30.66\n31\n5.51\n30\n\n\n\n\n\n\n\n# summarize data\nrange_shift_summary &lt;- range_shift_summary  |&gt;\n  mutate(df = n -1)\n\nNow that we have our degrees of freedom, we know which t-distribution we’re working with (Figure 1)!\n\n\nPlotting \\(t_{30}\\).\nt_30 &lt;- tibble(t = seq(-5,5,.001))|&gt;\n  mutate(prob_density = dt(x = t, df = 30))\n\nggplot(t_30, aes(x = t, y = prob_density)) +\n  geom_area(fill = \"white\", color = \"black\")+\n  labs(title = \"The t-distribution with 30 degrees of freedom\", \n       y = \"Probability density\")+\n  scale_x_continuous(breaks = seq(-4,4,2))\n\n\n\n\n\n\n\n\nFigure 1: The probability density function for a t-distribution with 30 degrees of freedom. The t-score is on the x-axis, and the probability density is on the y-axis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Critical values for the t-distribution with x degrees of freedom (α= 0.05). The dashed red line shows the Z critical value (i.e. df = ∞).",
    "crumbs": [
      "16. The t distribution",
      "• 16. Uncertain-t"
    ]
  },
  {
    "objectID": "book_sections/t/t_CI.html#calculating-a-confidence-interval",
    "href": "book_sections/t/t_CI.html#calculating-a-confidence-interval",
    "title": "• 16. Uncertain-t",
    "section": "Calculating a Confidence Interval",
    "text": "Calculating a Confidence Interval\nBecause the t statistic measures how many standard errors our estimate is from the true parameter value, we can find a \\(1-\\alpha\\)% confidence interval by:\n\nFinding the critical t value, \\(t_{\\alpha/2,\\text{ df = }30}\\) that captures the middle \\(1-\\alpha\\) of the t distribution.\n\nBringing this CI to the scale of our data by multiplying it by our estimate of the standard error, \\(s_\\bar{x}\\),\n\nReporting the \\((1-\\alpha)\\%\\text{ CI }\\) as \\(\\overline{x} \\pm t_{\\alpha/2} \\times s_\\bar{x}\\), where \\(t_{\\alpha/2}\\) is the critical two-tailed t value for a specified \\(\\alpha\\) level, which we can find R using the qt() function.\n\nCritical values for a 95% confidence interval with two to twenty five degrees of freedom are shown in Figure 2. Note that this approaches 1.96 (the Z-critical value) \\(df \\to\\infty\\).\n\n\n\n\nNote: We divide \\(\\alpha\\) by 2 because a two-sided CI splits the remaining area equally across both tails.\n\nWorked example\nSo to find the 95% Confidence interval for our example:\n\n95% CI with 30 df = \\(\\overline{x} \\pm t_{.05/2,\\text{ df = } 30} \\times s_\\bar{x}\\).\n95% CI for our data = \\(39.33 \\pm t_{.05/2,\\text{ df = } 30} \\times 5.51\\).\n\n\n\n\n\\(\\bar{x} = 39.33\\).\n\n\\(s_\\bar{x} = 5.51\\).\n\nBecause we know that roughly 95% of the Z is between two and negative two, and because the t-distribution approaches the Z distribution as we have more degrees of freedom, we can guesstimate the 95% CI as \\(39.33 \\pm 2 \\times 5.51\\). i.e. The 95% CI is roughly between 28 and 50.\nMore precisely the qt() function finds a critical value of qt(0.025, df= 30, lower.tail = FALSE) = 2.042. So:\n\n95% CI for our data = \\(39.33 \\pm 2.042 \\times 5.51 = 39.33 \\pm 11.25\\) i.e. between 28.1 and 50.6.\n\n\nalpha &lt;- 0.05\nrange_shift_summary &lt;- range_shift_summary  |&gt;\n  mutate(df = n-1,\n         crit_val = qt(p = alpha/2, df = df, lower.tail = FALSE),\n         lower_CI = mean - se * crit_val,\n         upper_CI = mean + se * crit_val)\n\n\n\n\n\n\n\nRange shift summary with 95% CI\n\n\nmean\nsd\nn\nse\ndf\ncrit_val\nlower_CI\nupper_CI\n\n\n\n\n39.33\n30.66\n31\n5.51\n30\n2.04\n28.08\n50.58\n\n\n\n\n\n\n\n\nThe qt() function\nqt() takes the arguments, p and df, and finds the critical value that separates the lower p tail from the rest of the t distribution with df degrees of freedom. The optional argument lower.tail tells R if we’re coming in from the upper or lower tail. Because the t-distribution is symmetric this simply changes the sign of qt()’s output.\nThe q_() function is available for many common distributions. For example qnorm(.025, lower.tail = FALSE), returns 1.96, the Z value that marks the upper 2.5% of the Z distribution.",
    "crumbs": [
      "16. The t distribution",
      "• 16. Uncertain-t"
    ]
  },
  {
    "objectID": "book_sections/t/t_CI.html#comparison-to-the-bootstrap-ci",
    "href": "book_sections/t/t_CI.html#comparison-to-the-bootstrap-ci",
    "title": "• 16. Uncertain-t",
    "section": "Comparison to the bootstrap CI",
    "text": "Comparison to the bootstrap CI\nWhen our sample size is not too small the 95% confidence interval from the t-distribution will be quite close to that generated by bootstrapping (see below).\nSo why use the t-distribution? That’s a good question! The best answer is “tradition”. The second best answer is that the t-distribution-based confidence interval is faster, reproducible, and less computationally demanding than the bootstrap-based confidence interval.\n\n\n\n\n\n\n\nt vs. Bootstrapped uncertainty\n\n\ndistribution\nse\nlowerCI\nupperCI\n\n\n\n\nt\n5.51\n28.08\n50.58\n\n\nbootstrap\n5.37\n29.03\n50.00\n\n\n\n\n\n\n\nlibrary(infer)\na &lt;- 0.05\nrange_shift_boot_summary &lt;-  range_shift                  |&gt;\n  specify(response = elevationalRangeShift)          |&gt;  \n  generate(reps = 10000, type = \"bootstrap\")         |&gt;\n  summarise(est_shift = mean(elevationalRangeShift)) |&gt;\n  summarise(se  = sd(est_shift),\n        lower_CI = quantile(est_shift, probs = a/2),\n        upper_CI = quantile(est_shift, probs = 1-a/2))",
    "crumbs": [
      "16. The t distribution",
      "• 16. Uncertain-t"
    ]
  },
  {
    "objectID": "book_sections/t/t_CI.html#interpreting-confidence-intervals",
    "href": "book_sections/t/t_CI.html#interpreting-confidence-intervals",
    "title": "• 16. Uncertain-t",
    "section": "Interpreting Confidence Intervals",
    "text": "Interpreting Confidence Intervals\nWe have already considered the interpretation of confidence intervals. But it is worth revisiting, because the confidence interval is a weird concept. If we repeatedly sampled from a population and built a \\(1-\\alpha\\) confidence interval from each sample, then about \\(1-\\alpha\\) of those intervals would contain the true mean. Feel free to revisit the confidence interval webapp from before.\nThe tricky thing is that we don’t have a population, we have a sample, so we do not know the probability that a confidence interval contains the true parameter value. In fact in a frequentist perspective (which we take on for most of this book), the question “What is the probability that a confidence interval contains the true parameter” is somewhat incoherent and cannot be answered. The true parameter is either inside or outside the confidence interval—there is no probability about it.\n\n\nIn a Bayesian framework, this question is coherent and can be answered, but doing so requires a different set of equations, assumptions, and worldviews, and will not generally result in the same answer as found by a confidence interval.\nSo we do this strange thing – we use our sample estimate and standard error to approximate the sampling distribution. Then we mark off the middle \\(1-\\alpha\\) of that distribution and call it our \\(1-\\alpha\\) confidence interval. We then repeat the magic words “We are 95% confident that the true population mean lies within our 95% confidence interval”.\n\nA 95% interval is NOT a range with a 95% chance of containing the true mean..\n\nWrong: “There is a 95% chance the true mean is in this interval.”\n\nRight: “Our method of constructing intervals will contain the true mean about 95% of the time.”",
    "crumbs": [
      "16. The t distribution",
      "• 16. Uncertain-t"
    ]
  },
  {
    "objectID": "book_sections/t/t_CI.html#visualizing-confidence-intervals",
    "href": "book_sections/t/t_CI.html#visualizing-confidence-intervals",
    "title": "• 16. Uncertain-t",
    "section": "Visualizing Confidence Intervals",
    "text": "Visualizing Confidence Intervals\n\n\n\n\n\n\n\n\n\nFigure 3: Change in the elevation of 31 species. Data from Chen et al. (2011). code here. The error bar represents the t-based 95% confidence interval.\n\n\n\n\nWe have previously used ggplot’s stat_summary() function to plot 95% bootstrap confidence intervals with stat_summary(fun.data = \"mean_cl_boot\"). We do basically the same thing for the t -based confidence interval, but replace \"mean_cl_boot\" with\"mean_cl_normal\". Figure 3 is a slight elaboration of this code:\n\nlibrary(ggplot2)\nlibrary(Hmisc)\nggplot(range_shift, aes(x = 0, y =elevationalRangeShift))+\n    geom_jitter()+\n    stat_summary(fun.data = \"mean_cl_normal\", geom = \"errorbar\", width = .2)+\n    scale_x_continuous(limits = c(-1,1))\n\n\n\n\n\nChen, I.-C., Hill, J. K., Ohlemüller, R., Roy, D. B., & Thomas, C. D. (2011). Rapid range shifts of species associated with high levels of climate warming. Science, 333(6045), 1024–1026. https://doi.org/10.1126/science.1206432",
    "crumbs": [
      "16. The t distribution",
      "• 16. Uncertain-t"
    ]
  },
  {
    "objectID": "book_sections/t/one_sample_t_test.html",
    "href": "book_sections/t/one_sample_t_test.html",
    "title": "• 16. One sample t-test",
    "section": "",
    "text": "The one sample t-test\nThis feels weird. I would be shocked if you ever use the one-sample t-test in your research. That said, I feel good about teaching it in this course because understanding how the one sample t-test works will provide you the foundation needed to understand the more complex analyses that you will likely perform (or read about). So lets do it!\nTo conduct a one sample t-test we:",
    "crumbs": [
      "16. The t distribution",
      "• 16. One sample t-test"
    ]
  },
  {
    "objectID": "book_sections/t/one_sample_t_test.html#the-one-sample-t-test",
    "href": "book_sections/t/one_sample_t_test.html#the-one-sample-t-test",
    "title": "• 16. One sample t-test",
    "section": "",
    "text": "State the null and alternative hypotheses. This means you must say what \\(\\mu_0\\) should be under the null.\n\nCalculate the t-value as the difference between the estimated mean \\(\\bar{x}\\) and its hypothesized value under the null \\(\\mu_0\\) divided by the sample standard error, \\(s_\\bar{x}\\). That is, \\(t = \\frac{\\bar{x}-\\mu_0}{s_\\bar{x}}\\).\n\nFind the p-value as the probability, under the null t-distribution, of observing a value at least as extreme as the one from our data (i.e. the proportion of the t-distribution as or more extreme than our t).\n\nYou can do this with pt(): P-value = pval &lt;- 2*pt(q = abs(t_value), df=these_df, lower.tail=FALSE).\n\nWe multiply by two because this is a two-tailed test.\n\n\nReject the null if our p-value is less than \\(\\alpha\\). Otherwise, fail to reject the null hypothesis.\n\nNote that when our t-value is greater than the critical t-value, the p-value is less than \\(\\alpha\\) and we reject the null.\n\n\nWrite up your results.",
    "crumbs": [
      "16. The t distribution",
      "• 16. One sample t-test"
    ]
  },
  {
    "objectID": "book_sections/t/one_sample_t_test.html#worked-example",
    "href": "book_sections/t/one_sample_t_test.html#worked-example",
    "title": "• 16. One sample t-test",
    "section": "Worked example",
    "text": "Worked example\nLet’s apply the one-sample t test to our elevational range shift data set.\n\nStep 1. State statistical hypotheses\nTo start, let’s state the null and alternative hypotheses as we did when introducing the data:\n\nNull Hypothesis (\\(H_0\\)): The true mean elevational shift (\\(\\mu\\)) is zero. Any observed shift from our sample is due to random chance (\\(H_0: \\mu=0\\)).\n\nAlternative Hypothesis (\\(H_A\\)): The true mean elevational shift (\\(\\mu\\)) is not zero. Species are, on average, shifting their elevation (\\(H_A: \\mu \\neq 0\\)).\n\n\nIs the null really \\(\\mu = 0\\)?\nFor the sake of our learning goals, we’ll assume the null expectation is \\(\\mu_0 = 0\\) (i.e. species are equally likely to increase or decrease elevation). But that’s not necessarily a biologically realistic null without more evidence.\n\nIf all species started at low elevations, they would have nowhere to go but up.\n\nIf all species started at high elevations, it would all be downhill.\n\nIn a real analysis, I’d want to model the expected change in elevation based on the available habitat area, and use that as my null.\n\n\n\nStep 2. Calculate the t-value\nTo calculate the t-value, we simply follow the equation:\n\\[t = \\frac{\\bar{x}-\\mu_0}{s_\\bar{x}} = \\frac{39.33 - 0}{5.51} = 7.14\\]\n\n\n\n\n\n\n\nSummary for one-sample t-test\n\n\nStat\nValue\n\n\n\n\nmean\n39.33\n\n\nsd\n30.66\n\n\nn\n31.00\n\n\nse\n5.51\n\n\ndf\n30.00\n\n\nt\n7.14\n\n\n\n\n\n\n\nmu_0  &lt;- 0 # set the null\n\nrange_shift_summary &lt;- range_shift |&gt;\n  summarise(mean     = mean(elevationalRangeShift),\n            sd       = sd(elevationalRangeShift),\n            n        = n(),\n            se       = sd / sqrt(n),\n            df       = n - 1,\n            t        = (mean - mu_0) / se) \n\n\n\nStep 3. Find the p-value\n\n\nPlotting \\(t_{30}\\) and out t-value.\nt_30 &lt;- tibble(t = seq(-9,9,.005))|&gt;\n  mutate(prob_density = dt(x = t, df = 30))\n\n\nggplot(t_30, aes(x = t, y = prob_density)) +\n  geom_area(fill = \"white\", color = \"black\")+\n  labs(title = \"Comparing our t to the t-distribution\", \n       y = \"Probability density\")+\n  scale_x_continuous(breaks = seq(-8,8,4))+\n  geom_vline(xintercept = c(-7.14,7.14), color = \"red\", linewidth = 1.2,linetype= 2)+\n  theme(axis.title = element_text(size = 21),\n        axis.text = element_text(size = 21), \n        title = element_text(size = 19))\n\n\n\n\n\n\n\n\n\nFigure 1: The probability density function for a t-distribution with 30 degrees of freedom. The t-score is on the x-axis, and the probability density is on the y-axis. Dashed red lines show the value in our data.\n\n\n\n\nWe don’t need R to tell us this what we’ll do to the null, Figure 1 makes it clear that the p-value is very small. More generally,\n\nWhen t is less than 1.96 (the critical Z-value) we will fail to reject the null.\n\nWhen t is greater than 2.5 (and our sample size is greater than seven) we will reject the null at the traditional \\(\\alpha\\) value of 0.05. Here t is much greater than two and a half (it’s seven) and our sample size is way more than seven (it’s thirty one) – we will reject the null.\n\nWhen t is between 1.96 and 2.5 we may or may not reject the null depending on the degrees of freedom and the value of t.\n\nBut let’s find the p-value anyways:\n\n# Our p-value\n2 * pt(7.14, df= 30, lower.tail = FALSE) \n\n\n\n[1] \"6.08e-08\"\n\n\n\n\nStep 4. Make a decision\nOur p-value is minuscule. We reject the null hypothesis. Only one in fifteen million samples from the null distribution would have generated a t-statistic with an absolute value of 7.14 or more.\n\n\nStep 5. Write up\nLet’s work on writing these results. A write up should include the biological motivation, study design, sample summaries, effect sizes, test statistics, p-values, and what we do to the null. Here is a short version:\nOn average, species have shifted their range to higher elevations, with a mean shift of 39.3 meters, a standard error of 2.21 meters, and a 95% confidence interval between 28.1 and 50.6 meters. Although there is some variability among species (sd = 30.7), this variability is overshadowed by the overall trend toward higher elevations (Cohen’s d = 1.28 – A very large effect size). Such a large shift is highly unlikely under the null hypothesis (t = 7.14, df = 30, p = \\(6.06 \\times 10^{-8}\\)). This result is not driven by the UK samples, which make up half of the dataset - we observe a similar result (mean = 48.6 meters, 95% confidence interval between 27.5 and 69.7 meters) after excluding them.",
    "crumbs": [
      "16. The t distribution",
      "• 16. One sample t-test"
    ]
  },
  {
    "objectID": "book_sections/t/one_sample_t_test_in_R.html",
    "href": "book_sections/t/one_sample_t_test_in_R.html",
    "title": "• 16. One sample t-test in R",
    "section": "",
    "text": "Stats in R\nFigure 1: R can make things easy! Image made with ChatGPT.\nWe just worked through the calculations for a confidence interval and a hypothesis test step by step. But R can do this for us in a single line. Now that you know how it works and what R is doing, I can show you how to have R do this work for you.",
    "crumbs": [
      "16. The t distribution",
      "• 16. One sample t-test in R"
    ]
  },
  {
    "objectID": "book_sections/t/one_sample_t_test_in_R.html#stats-in-r",
    "href": "book_sections/t/one_sample_t_test_in_R.html#stats-in-r",
    "title": "• 16. One sample t-test in R",
    "section": "",
    "text": "“What does a one-sample t-test in R mean”, you ask, realizing we just did that. Let me explain…",
    "crumbs": [
      "16. The t distribution",
      "• 16. One sample t-test in R"
    ]
  },
  {
    "objectID": "book_sections/t/one_sample_t_test_in_R.html#t.test",
    "href": "book_sections/t/one_sample_t_test_in_R.html#t.test",
    "title": "• 16. One sample t-test in R",
    "section": "t.test()",
    "text": "t.test()\nThe t.test() function is the easiest way to run a one-sample t-test in R. There are two relevant arguments:\n\nx: A vector of observations.\n\nmu: \\(\\mu_0\\) – i.e. the expected value of x under the null hypothesis.\n\n\n\nNote: Because x must be a vector, I pull() it out of its tibble.\n\nt.test(x = pull(range_shift,elevationalRangeShift) , mu = 0)\n\n\n    One Sample t-test\n\ndata:  pull(range_shift, elevationalRangeShift)\nt = 7.1413, df = 30, p-value = 0.00000006057\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 28.08171 50.57635\nsample estimates:\nmean of x \n 39.32903 \n\n\n\n\nTidying up R model outputs\nThe output of t-test() includes:\n\nThe test statistic, t.\n\nThe degrees of freedom.\n\nThe p-value.\n\nThe 95% confidence interval.\n\nThe sample estimate (in this case the mean).\n\nWhile all of these results match our calculations (and are faster and easier to do), there is one problem – they are a mess. That is, they are stored in a format that you can read, but it’s not easy to process with a computer. The tidy() function in the broom package offers a convenient format:\n\nlibrary(broom)\nt.test(x = pull(range_shift,elevationalRangeShift) , mu = 0) |&gt;\n  tidy() \n\n\n\n\n\n\n\nt.test() output piped through broom's tidy() function\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n39.33\n7.14\n6.06e-08\n30\n28.08\n50.58\nOne Sample t-test\ntwo.sided\n\n\n\n\n\n\n\n\n\nThe statistic here is the t-value. The statistic column will contain the value of whichever test statistic your analysis produces.",
    "crumbs": [
      "16. The t distribution",
      "• 16. One sample t-test in R"
    ]
  },
  {
    "objectID": "book_sections/t/one_sample_t_test_in_R.html#the-general-linear-model-framework-in-r",
    "href": "book_sections/t/one_sample_t_test_in_R.html#the-general-linear-model-framework-in-r",
    "title": "• 16. One sample t-test in R",
    "section": "The general linear model framework in R",
    "text": "The general linear model framework in R\nAn alternative approach to the t.test() function is to use the R’s framework for linear models. Remember that a linear model predicts the value of the response variable of the \\(i^{th}\\) observation \\(\\hat{Y_i}\\) as a function of things in our model. For a one sample t-test, our model only contains the mean, which we represent as the “intercept”, \\(a\\):\n\\[\\hat{Y_i} = a\\]\nOf course, actual observations \\(Y_i\\), will deviate from model predictions. This deviation, known as the “residual” is shown as \\(e_i\\):\n\\[Y_i = \\hat{Y_i} + e_i\\] \\[Y_i =  a+e_i\\]\nThe lm() function in R builds such linear models. If we are only modelling the mean, the syntax is:\n\nlm(y ~ 1) if y is a vector, or\n\nlm(y ~ 1, data = dataset) if y is a column in a tibble (or dataframe)\n\nHere the 1 ells R to fit the simplest possible model: an intercept-only model. The estimate for this intercept will be the overall mean of our response variable:\n\nlm(elevationalRangeShift ~ 1, data = range_shift)\n\n\nCall:\nlm(formula = elevationalRangeShift ~ 1, data = range_shift)\n\nCoefficients:\n(Intercept)  \n      39.33  \n\n\n\n\nThis may seem like overkill for a one-sample t-test, but the linear model framework generalizes to regression, ANOVA, and more. Seeing a one-sample t-test as a linear model prepares us for the models we’ll meet next.\n\n\nsummary() provides p-values and test statistics\nlm() just returns the mean, but it actually calculates so much more! To access information like the standard error, t-value, p value (reported as Pr(&gt;|t|)), and degrees of freedom, pipe the output to summary():\n\nlm(elevationalRangeShift ~ 1, data = range_shift) |&gt;\n  summary()\n\n\nCall:\nlm(formula = elevationalRangeShift ~ 1, data = range_shift)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-58.629 -23.379  -3.529  24.121  69.271 \n\nCoefficients:\n            Estimate Std. Error t value     Pr(&gt;|t|)    \n(Intercept)   39.329      5.507   7.141 0.0000000606 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.66 on 30 degrees of freedom\n\n\n\n\n\ntidy() cleans up the output\nAs above, we can use broom’s tidy() function to turn this output into a more convenient format:\n\nlibrary(broom)\nlm(elevationalRangeShift ~ 1, data = range_shift) |&gt;\n  tidy()\n\n# A tibble: 1 × 5\n  term        estimate std.error statistic      p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 (Intercept)     39.3      5.51      7.14 0.0000000606\n\n\n\n\n\n\n\n\n\n\nFitted values and residuals from augment()\n\n\nelevationalRangeShift\n.fitted\n.resid\n\n\n\n\n58.9\n39.329\n19.571\n\n\n7.8\n39.329\n-31.529\n\n\n108.6\n39.329\n69.271\n\n\n44.8\n39.329\n5.471\n\n\n11.1\n39.329\n-28.229\n\n\n\n\n\n\n\n\naugment() shows residuals (and more)\nbroom’s augment() function shows each observation’s the actual value, predicted value (.fitted) and residual (.resid). I’m only showing the first three columns because they are the only ones we’ve covered so far, and I only show the first five rows to save space.\n\nlibrary(broom)\nlm(elevationalRangeShift ~ 1, data = range_shift) |&gt;\n  augment()|&gt;\n  select(1:3)|&gt;\n  slice_head(n = 5)",
    "crumbs": [
      "16. The t distribution",
      "• 16. One sample t-test in R"
    ]
  },
  {
    "objectID": "book_sections/t/paired_t_test.html",
    "href": "book_sections/t/paired_t_test.html",
    "title": "• 16. The “Paired” t-test",
    "section": "",
    "text": "Paired t-test\nA common use of the one-sample t-test is to compare groups when there are natural pairs in each group. These pairs should be similar in every way, except for the difference we are investigating.",
    "crumbs": [
      "16. The t distribution",
      "• 16. The \"Paired\" t-test"
    ]
  },
  {
    "objectID": "book_sections/t/paired_t_test.html#paired-t-test",
    "href": "book_sections/t/paired_t_test.html#paired-t-test",
    "title": "• 16. The “Paired” t-test",
    "section": "",
    "text": "We cannot just pair random individuals and call it a paired t-test.\nFor example, suppose we wanted to test the idea that more money leads to more problems. We give some people $100,000 and others $1 and then measure their problems using a quantitative, normally distributed scale.\n\nIf we randomly gave twenty people $100,000 and twenty people $1, we could not just randomly form 20 pairs and conduct a paired t-test.\n\nHowever, we could pair people by background (e.g., find a pair of waiters at similar restaurants, give one $100k and the other $1, then do the same for a pair of professors, a pair of hairdressers, a pair of doctors, and a pair of programmers, etc., until we had twenty such pairs). In that case, we could conduct a paired t-test.",
    "crumbs": [
      "16. The t distribution",
      "• 16. The \"Paired\" t-test"
    ]
  },
  {
    "objectID": "book_sections/t/paired_t_test.html#paired-t-test-example",
    "href": "book_sections/t/paired_t_test.html#paired-t-test-example",
    "title": "• 16. The “Paired” t-test",
    "section": "Paired t-test Example:",
    "text": "Paired t-test Example:\nI miss our parviflora plants too much - let’s get back to them. Recall that Brooke created “lines” (RILs) of parviflora plants. Although each RIL differed from all the others, each RIL can be replicated (by self fertilization), so Brooke planted genetically identical genotypes at each location.\nWe can therefore test if the hybridization rate differed across locations. Here we focus on two locations – Lower Breckenridge (LB), and Sawmill Road (SR). The data are presented in Figure 1. Panels A and B show the same data in unpaired and paired form; panel C shows the distribution of within-RIL difference.\nBelow I introduce the paired t-test, which tests the null hypothesis that the mean difference between pairs is zero. To do so, we simply run a one sample t-test on the difference in values for each pair and test against the null of zero. This means our degrees of freedom equals the number of pairs minus one. Because this “pairing” accounts for differences across pairs and the paired t-test provides high statistical power to reject a false null hypothesis by focusing exclusively on differences within pairs.\n\n\nThe next chapter introduces the two-sample t-test. Although the two-sample t-test is less powerful than the paired t-test it is useful because it can be applied when data are not paired.\n\n\n\n\n\n\n\n\nFigure 1: (A) Unpaired view: Hybridization proportion for each RIL at Lower Breckenridge (LB) and Sawmill Road (SR), with means ± 95% confidence intervals shown. (B) Paired view: Each line connects the same RIL grown at both sites. (C) The distribution of differences in hybridization rate for each RIL between sites (SR – LB). The red horizontal error bar shows the 95% confidence interval of the mean difference.\n\n\n\n\n\n\nEvaluating assumptions\nBecause a paired t-test is simply a one-sample t-test on the differences in each pair, all our assumptions apply to the distribution of difference within pairs.\nWe know that data are independent, and collected without bias (way to go Brooke!). I also think that the mean seems like a reasonable summary of the data.\nHowever, I know that the differences in each pair are not perfectly normal because:\n\nThe original data are bounded between zero and one - so differences are bounded between negative one and one.\n\nThe data are discrete – it’s one proportion minus another (each usually in increments of one eighth, as we usually assayed eight offspring).\n\nIt also looks like there are an excess of zeros (i.e. the data are “zero inflated”) because many RILs made zero hybrids at both sites.\n\nBut we’re not looking for a perfect normal distribution. We want data to be normal enough so that we believe in our statistics. Because the t-test is robust to minor violations of assumptions of normality, and because our qq-plot (Figure 2) doesn’t look so bad, we can move on and do some stats.\n\nggplot(wide_lb_sr, aes(sample = diffs))+\n  geom_qq(size = 4)+\n  geom_qq_line()+\n  theme(axis.text = element_text(size = 24),title  = element_text(size = 24),\n        axis.title = element_text(size = 24),subtitle  = element_text(size = 18))+\n  labs(title = \"QQ-plot\", subtitle = \"Difference in hybridization\",\n       x = \"Theoretical quantile\", y= \"Observed (SR - LB)\")\n\n\n\n\n\n\n\n\nFigure 2: QQ-plot of differences in hybridization (Sawmill Road – Lower Breckenridge).\n\n\n\n\n\n\nSummarizing data\nWhile you are free to report means and confidence intervals for each “treatment” in a paired t-test, we are actually focused on the difference in pairs. In the tibble below, I have calculated this as:\n\\[\\text{diffs} = \\text{Prop hybrid}_\\text{ Sawmill Road} - \\text{Prop hybrid}_\\text{ Lower Breckenridge}\\]\nSo, we can now calculate the relevant summary stats:\n\ndiffs_summaries &lt;- wide_LB_SR  |&gt;\n  summarise(n = n(),\n            mean_diff = mean(diffs), \n            sd_diffs  = sd(diffs),\n            cohens_D  = (mean_diff - 0) / sd_diffs,\n            \n            se_diffs  = sd_diffs / sqrt(n),\n            t         = (mean_diff - 0) /  se_diffs)\n\n\n\n\n\n\n\n\n\nn\nmean_diff\nsd_diffs\ncohens_D\nse_diffs\nt\n\n\n\n\n80\n-0.046\n0.312\n-0.147\n0.035\n-1.317\n\n\n\n\n\n\n\n🛑STOP🛑 Before we conduct a null hypothesis significance test you should immediately notice two things:\n\nAny effect, if real is not particularly strong - a Cohen’s D value is “tiny” (0.01 – 0.20).\n\nThis will not be significant at the \\(\\alpha = 0.05\\) level – results will never be statistically significant when t is less than 1.96.\n\nBut let’s move on to a formal test because we’re doing it.\n\n\nThe paired t-test in R\nIf your data are formatted like mine (see margin), the t.test function provides two equivalent ways to conduct a paired t-test. Both approaches give the same result, since they are just two ways of formulating the same test. I show you how to do them below:\n\nThe one sample version: t.test(x = DIFFERENCES, mu = 0).\n\nThe paired version: t.test(x = CONDITION ONE, y = CONDITION TWO, paired = TRUE).\n\nFor this version the \\(i^{th}\\) entry of x and y should refer to the same pair (pair, i).\n\n\n\n\n\n\n\n\n\nData for paired t-test\n\n\nril\nLB\nSR\ndiffs\n\n\n\n\nA1\n0.125\n0.000\n-0.125\n\n\nA100\n0.375\n0.250\n-0.125\n\n\nA106\n0.000\n0.000\n0.000\n\n\nA111\n0.000\n0.125\n0.125\n\n\n\n\n\n\n\nOne sample t-test versionPaired t-test version\n\n\n\n\nt.test(x = pull(wide_LB_SR, diffs))\n\n\n    One Sample t-test\n\ndata:  pull(wide_LB_SR, diffs)\nt = -1.3171, df = 79, p-value = 0.1916\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.11535736  0.02348236\nsample estimates:\n mean of x \n-0.0459375 \n\n\n\n\n\n\n\nt.test(x      = pull(wide_LB_SR, SR), \n       y      = pull(wide_LB_SR, LB), \n       paired = TRUE)\n\n\n    Paired t-test\n\ndata:  pull(wide_LB_SR, SR) and pull(wide_LB_SR, LB)\nt = -1.3171, df = 79, p-value = 0.1916\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.11535736  0.02348236\nsample estimates:\nmean difference \n     -0.0459375 \n\n\n\n\n\n\nBecause our p-value exceeds 0.05 we fail to reject the null.\n\nConcept check: We failed to reject the null. This means\n\n The null is true The null is false The null is more likely to be true than false There is a p = 0.19 chance that the null is false There is a p = 0.19 chance that our results are due to sampling error Our sample did not provide strong evidence against the null hypothesis.",
    "crumbs": [
      "16. The t distribution",
      "• 16. The \"Paired\" t-test"
    ]
  },
  {
    "objectID": "book_sections/t/t_summary.html",
    "href": "book_sections/t/t_summary.html",
    "title": "• 16. t Summary",
    "section": "",
    "text": "Chapter summary\nLinks to: Summary. Chatbot tutor. Questions. Glossary. R packages. R functions. More resources.\nThe t-distribution is a symmetric, bell-shaped curve defined by its degrees of freedom. Like the standard normal deviate, Z, the t-value measures the distance (in units of standard errors) between a sample estimate and a hypothesized population parameter. Compared to the Z-distribution, the t-distribution has “fatter tails,” reflecting the extra uncertainty from estimating the standard deviation.\nWe use the t-distribution to quantify uncertainty and to conduct a one-sample t-test of the null hypothesis that an approximately normally distributed sample came from a population with mean \\(\\mu_0\\). A particularly useful application is the paired t-test. Here, each natural pair is split across two conditions, and we calculate differences in the response variable within each pair. The test then evaluates whether these differences are consistent with a null mean difference of \\(\\mu_0=0\\).",
    "crumbs": [
      "16. The t distribution",
      "• 16. t Summary"
    ]
  },
  {
    "objectID": "book_sections/t/t_summary.html#t_summary_chapter-summary",
    "href": "book_sections/t/t_summary.html#t_summary_chapter-summary",
    "title": "• 16. t Summary",
    "section": "",
    "text": "Chatbot tutor\n\nPlease interact with this custom chatbot (link here). I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you got what you needed from it.",
    "crumbs": [
      "16. The t distribution",
      "• 16. t Summary"
    ]
  },
  {
    "objectID": "book_sections/t/t_summary.html#t_summary_practice-questions",
    "href": "book_sections/t/t_summary.html#t_summary_practice-questions",
    "title": "• 16. t Summary",
    "section": "Practice Questions",
    "text": "Practice Questions\nTry these questions! By using the R environment you can work without leaving this “book”. I even pre-loaded all the packages you need!\n\nPart 1: Concepts\n\nQ1. The difference between a t-value and a z-value is:\n\n The z assumes the mean is zero, the t does not The z assumes that samples come from a normal distribution, the t does not The z assumes that we know the true population standard deviation, the t assumes we only have an estimate\n\n\nQ2. If our sample is not perfectly normal:\n\n We cannot use t-based tests, as the assume normality, and are not useful if data aren't normal We can use t-based tests so long as the sample is sufficiently large, and the deviations from normality aren't too big, so the central limit theorem bails us out. This has no impact on t - that's why we us a t-distribution instead of a z distribution\n\n\nQ3. The t and z distributions will never be the exact same. But when will they be nearly identical?\n\n When the sample size is large When the standard deviation is small When data are normal Never, these are very different distributions - this is akin to asking when an orange and apple are similar.\n\n\nQ4. Which is the best summary of the effect size for normally distributed data?\n\n The p-value The difference between the parameter and its null value The difference between the parameter and its null value, standardized by the standard deviation The difference between the parameter and its null value, standardized by the standard error\n\n\n\n\n\nPart 2: Example\n\n\n\n\n\n\n\n\n\nFigure 2: Paired differences in hyena “giggles” by dominance status. Each pair comprises one dominant and one subordinate hyena. Left: raw paired measurements with lines connecting each pair. Right: the within-pair difference (subordinate − dominant); points above 0 indicate more giggling by subordinates. The black point shows the mean difference with a 95% CI. Most pairs show positive differences, consistent with subordinates giggling more on average. (Data redrawn from Mathevon et al. 2010.)\n\n\n\n\n\n\n\n\nEnjoy this video of hyena laughter.\n\nSetup. We’re not sure about the function of laughter, but there are some ideas that it could function to show dominance or subordinates. Mathevon and colleagues looked into this idea in hyenas. They found natural pairs of hyenas hanging out and noted which of the two was dominant and which was subordinate. They provided a measure of giggling (big number means more giggles). Because these data come naturally paired, we can conduct a t-test on the difference between pairs by testing the null hypothesis that this difference is zero. The data are available here, plotted in Figure 2, loaded below, and presented in Table 1:\n\n\n\n\n\n\n\nTable 1\n\n\npair\ndominant\nsubordinate\ndiffs\n\n\n\n\n1\n0.384\n0.507\n0.123\n\n\n2\n0.386\n0.569\n0.183\n\n\n3\n0.252\n0.235\n-0.017\n\n\n4\n0.226\n0.415\n0.189\n\n\n5\n0.323\n0.436\n0.113\n\n\n6\n0.287\n0.451\n0.164\n\n\n7\n0.303\n0.399\n0.096\n\n\n8\n0.317\n0.220\n-0.097\n\n\n9\n0.277\n0.338\n0.061\n\n\n\n\n\n\n\nlibrary(stringr);  library(dplyr); library(readr); library(ggplot2)\nlink &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/chap12q23HyenaGiggles.csv\"\nhyenas&lt;-read_csv(link) |&gt;\n rename_all( str_remove, \"IndividualGiggleVariation\") |&gt;\n mutate(diffs = subordinate - dominant)\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nQ5. Make a qq-plot. Then evaluate it in relation to the normality assumption.\n\n The data are perfectly normal The data likely deviate somewhat from normality, but a t-test is probably ok The data deviate strongly from normality and any test assuming normality cannot be trusted\n\n\n\nClick here for code if you’re stuck\n\n\nggplot(hyenas, aes(sample = diffs))+\n  geom_qq() +\n  geom_qq_line()\n\n\n\n\n\n\n\n\n\n\nFor the next few questions, fill in the blanks to find:\n- The standard deviation of the differences (sd_diff).\n- Cohen’s d (cohens_d).\n- The standard error (se_diff).\n- The t-value (t).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nQ6. The difference between in laughter between subordinate and dominant hyenas is ___ standard errors away from the zero. .\n\n\nExplanation\n\nThis is the t-value!\n\n\n**Q7. The 95% CI is calculated as the mean ± ___ times the critical t-value at 𝛼= 0.05. Cohen’s dThe standard deviationThe variancetThe standard error1.96\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nQ8. To find this two tailed, 95% confidence interval, you must find the critical t-value. For this case the critical t-value equals (return the absolute value) .\n\n\nClick here for help\n\nWe use the qt()function to find the critical t-value. There are two key arguments\n\np: which equals \\(\\alpha/2\\). Because we’re looking for the 95% confidence interval, \\(\\alpha = 1.00 -0.95 = 0.05\\). So p = \\(\\alpha/2 =0.025\\).\ndf: The degrees of freedom, which equals \\(n - 1\\). Because \\(n=9\\), we have eight degrees of freedom.\n\nqt() also takes an optional argument, lower.tail. Setting this to FALSE returns a poistive number.\n\n\nReturn above to try yourself, or click here for code\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nQ9. Given the answers to Q7 and Q8 do you think the difference between subordinates and dominants is statistically significant at the \\(\\alpha = 0.05\\) level?\n\n yes because t is greater than the critical value no because t is greater than the critical value we can't know this until we find our p-value\n\n\nQ10. Therefore, according to traditional statistical practice, we\n\n Proceed as if the null is false (interpret results accordingly), while recognizing that there is a real chance we're wrong Proceed as if the null is true (interpret results accordingly), while recognizing that there is a real chance we're wrong Reserve all judgement until we prove that the null is true (or false)\n\n\nQ11. Say you found a p-value of p. Which would be a correct interpretation of p?\n\n p is the probability that the null hypothesis generated the data (or something more extreme). p is the probability that the null hypothesis is true. p is the probability that a random sample from the null hypothesis would be as or more extreme than the data\n\n\nQ12. Say I had data from another two dominant individuals and another two subordinate individuals, but they were not from a pair. Would it be legit to randomly pair them and do a paired t-test? YesNo",
    "crumbs": [
      "16. The t distribution",
      "• 16. t Summary"
    ]
  },
  {
    "objectID": "book_sections/t/t_summary.html#t_summary_glossary-of-terms",
    "href": "book_sections/t/t_summary.html#t_summary_glossary-of-terms",
    "title": "• 16. t Summary",
    "section": "📊 Glossary of Terms",
    "text": "📊 Glossary of Terms\n\n\nt-distribution: A continuous probability distribution, similar to the normal distribution but with heavier tails to account for uncertainty when the population standard deviation is unknown. Defined by its degrees of freedom.\nDegrees of Freedom (df): The number of independent values in a calculation that are free to vary. For a one-sample t-test, \\(df = n-1\\).\nCritical Value (\\(t\\_{\\alpha/2, df}\\)): The cutoff from the t-distribution that marks the boundary for rejecting the null hypothesis at a chosen significance level \\(\\alpha\\).\nEffect Size: A quantitative measure of the magnitude of a phenomenon; here it describes how far the sample mean is from the null hypothesis mean, in standardized units.\n\nCohen’s d: A standardized effect size calculated as the mean difference divided by the sample standard deviation, \\(d = (\\bar{x} - \\mu\\_0)/s\\).\n\nt-value: The distance in estimated standard errors between a sample estimate of the mean \\(\\bar{x}\\), and its hypothesized value under the null hypothesis, \\(\\mu_0\\).\n\nt is calculated as \\(\\frac{\\bar{x} - \\mu_0}{s_\\bar{x}}\\), where \\(s_\\bar{x}\\) is the estimate of the standard error (below).\nt is the test statistic for a t-test.\n\n\nStandard Error (SE): An estimate of the standard deviation of the sampling distribution, calculated as \\(s_\\bar{x}=\\frac{s}{\\sqrt{n}}\\) for the mean of a normal or t-distribution.\n\nOne-Sample t-test: A test used to compare the mean of a single sample to a hypothesized population mean.\n\nPaired t-test: A test comparing the means of two related samples (e.g., before vs. after, or paired individuals), by analyzing the distribution of their differences.",
    "crumbs": [
      "16. The t distribution",
      "• 16. t Summary"
    ]
  },
  {
    "objectID": "book_sections/t/t_summary.html#t_summary_key-r-functions",
    "href": "book_sections/t/t_summary.html#t_summary_key-r-functions",
    "title": "• 16. t Summary",
    "section": "🛠️ Key R Functions",
    "text": "🛠️ Key R Functions\n\n\npt(): Finds cumulative probabilities for a t-distribution (area under the curve up to a value).\nqt(): Finds critical values (quantiles) for a t-distribution, e.g. for confidence intervals.\nt.test(): Performs one-sample, two-sample, or paired t-tests in R.\n\nOne sample” t.test(x = your_vector, mu = mu0).\n\nYou may need to pull() your vector from a column in a tibble.\n\n\\(\\mu_0\\) is the value of \\(\\mu\\) under the null hypotheses\n\nPaired: t.test(x = your_vector_of_diffs, mu = 0). OR t.test(x = vector_treat_a,y = vector_treat_b, paired = TRUE).\n\nlm(): Fits linear models; an intercept-only model (lm(y ~ 1)) is equivalent to a one-sample t-test.\n\ngeom_qq() and geom_qq_line(): Create QQ plots to visually check whether data are close enough to normal. - Remember to set aes(sample = THING) instead of aes(x = THING).",
    "crumbs": [
      "16. The t distribution",
      "• 16. t Summary"
    ]
  },
  {
    "objectID": "book_sections/t/t_summary.html#t_summary_additional-resources",
    "href": "book_sections/t/t_summary.html#t_summary_additional-resources",
    "title": "• 16. t Summary",
    "section": "Additional resources",
    "text": "Additional resources\n\nVideos:\n\nThe Normal Distribution: Crash Course Statistics #19: A clear description of the normal disitrbution, how it arises, its properties, and the central lmit theorem. This is a nice summary of the key concepts in this chapter.\nBut what is the Central Limit Theorem? from 3Blue1Brown’s youtube page]: An approachable introduction to more technical aspects of the normal distribution and the central limit theorm. This is the first video in his youtube playlist on the central limit theorem.\nKhan Academy Central Limit Theorem.",
    "crumbs": [
      "16. The t distribution",
      "• 16. t Summary"
    ]
  },
  {
    "objectID": "book_sections/two_ts.html",
    "href": "book_sections/two_ts.html",
    "title": "17. A binary explanatory variable",
    "section": "",
    "text": "The t-distribution\nWe can use the t-distribution to evaluate the null hypothesis that two samples come from the same statistical population.",
    "crumbs": [
      "17. A binary explanatory variable"
    ]
  },
  {
    "objectID": "book_sections/two_ts.html#the-t-distribution",
    "href": "book_sections/two_ts.html#the-t-distribution",
    "title": "17. A binary explanatory variable",
    "section": "",
    "text": "Paired comparisons: When data are naturally paired, we can use a one-sample t-test on the differences between members of each pair. This design provide us great statistical power to reject false null hypotheses because pairs “soak up” variability unrelated to treatment. This gives us more sensitivity to detect real effects.\nUnpaired comparisons: In many cases, natural pairing is impractical or impossible. For example, there is no natural “pairing” in comparisons between pink and white parviflora RILs. In these situations, we test whether the means of two independent samples differ.",
    "crumbs": [
      "17. A binary explanatory variable"
    ]
  },
  {
    "objectID": "book_sections/two_ts.html#our-path",
    "href": "book_sections/two_ts.html#our-path",
    "title": "17. A binary explanatory variable",
    "section": "Our Path",
    "text": "Our Path\nWe will return to our Clarkia RILs and compare pollinator visitation on to pink and white flowered RILs at site Sawmill Road. We might think that pink flowers attract more pollinator visits than white flowers (our scientific hypothesis). Recasting this idea as a (two-tailed) statistical hypotheses:\n\nNull hypothesis: The mean number of pollinators visits to pink and white flowered parviflora RILs are the same.\n\nAlternative hypothesis: The mean number of pollinators visits to pink and white flowered parviflora RILs differ.",
    "crumbs": [
      "17. A binary explanatory variable"
    ]
  },
  {
    "objectID": "book_sections/two_ts/2t_assumptions.html",
    "href": "book_sections/two_ts/2t_assumptions.html",
    "title": "• 17. Two-t Assumptions",
    "section": "",
    "text": "Assumptions of linear models\nWe’ve already seen the basic assumptions when modeling a single continuous variable with the t-distribution: data should be\nFor comparing two samples, we clarify that:\nLinear models, like a two-sample t-test also assume:",
    "crumbs": [
      "17. A binary explanatory variable",
      "• 17. Two-t Assumptions"
    ]
  },
  {
    "objectID": "book_sections/two_ts/2t_assumptions.html#assumptions-of-linear-models",
    "href": "book_sections/two_ts/2t_assumptions.html#assumptions-of-linear-models",
    "title": "• 17. Two-t Assumptions",
    "section": "",
    "text": "Independent.\nCollected without bias.\n\nSummarized appropriately by the mean, and\n\nApproximately normal.\n\n\n\nNormality refers specifically to normality of residuals – that is, the differences between observations and their predicted values. or a one sample analysis, the predicted value was simply the overall mean so normality of the raw implied that residuals were normal. For a two-sample analysis, each group has its own mean. Normality of residuals means data are approximately normal within each group. The combined data may look non-normal (even bimodal), and that’s fine, as long as each group individually is roughly normal.\n\n\n\nHomoscedasticity: This is a fancy way of saying that variance is independent of the predicted value, \\(\\hat{Y}\\). For two-sample analysis, this means that we are assuming equal variance in each group.",
    "crumbs": [
      "17. A binary explanatory variable",
      "• 17. Two-t Assumptions"
    ]
  },
  {
    "objectID": "book_sections/two_ts/2t_assumptions.html#evaluating-assumptions-for-our-data-set",
    "href": "book_sections/two_ts/2t_assumptions.html#evaluating-assumptions-for-our-data-set",
    "title": "• 17. Two-t Assumptions",
    "section": "Evaluating assumptions for our data set",
    "text": "Evaluating assumptions for our data set\n\n\nLoading and processing data\nril_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_rils.csv\"\nSR_rils &lt;- readr::read_csv(ril_link) |&gt;\n  filter(location == \"SR\") |&gt;\n  select(ril, petal_color, mean_visits) |&gt;\n  filter(!is.na(mean_visits), !is.na(petal_color))\n\n\nThese data are independent (we use only one value per RIL – the mean across plants of a given RIL genotype), collected without bias (nice job Brooke!), and the mean seems like a good summary. So let’s look into the assumptions of normality and equal variance:\n\n4. Data are far from normal, but transformation helps.\nA look at the raw data shows that pollinators visits are far from normal – for both petal color morphs, many RILs have nearly no visits, while some have many visits (Figure 1 A). After log-transformation\\(^*\\) (Figure 1 B), the data becomes closer to normal – however, the many white flowers receiving no pollinator visits \\(^\\$\\) remain, resulting a modest (but good enough for most stats) deviation from normality.\n\n\n\\(^*\\) or more specifically log(x+0.2) transformation. \n\\(^\\$\\) this is called “zero inflation” – a common challenge for some types of statistical analyses.\n\n\nCode for log-transformed plots.\nlibrary(patchwork)\na &lt;- ggplot(SR_rils, aes(x = mean_visits, fill = petal_color))+\n  geom_histogram(color = \"black\", bins =8)+\n  theme(legend.position = \"none\")+\n  scale_fill_manual(values = c(\"pink\", \"white\"))+\n  labs(title =\"Raw data (linear scale)\", x = \"log(.2 + mean visits)\")+\n  facet_wrap(~petal_color)+\n  coord_cartesian(ylim = c(0,30))\n\nb &lt;- ggplot(SR_rils, aes(x = log(.2+mean_visits), fill = petal_color))+\n  geom_histogram(color = \"black\", bins =8)+\n  theme(legend.position = \"none\")+\n  scale_fill_manual(values = c(\"pink\", \"white\"))+\n  labs(title =\"Transformed data (log+.2 scale)\", x = \"log(.2 + mean visits)\")+\n  facet_wrap(~petal_color)+\n  coord_cartesian(ylim = c(0,30))\n\na+b+plot_layout(axis_titles = \"collect_y\") + plot_annotation(tag_levels = \"A\")\n\n\n\n\n\n\n\n\nFigure 1: Evaluating the normality assumptions for the raw (A), and log transformed (B) data. The “log” transform is actually log(+0.2) transformation because the data contains zeros, and adding one does not make the data particularly normal. This transformation makes the data more consistent with the normality assumption.\n\n\n\n\n\n\n\n5. Variance is similar among groups\nComparing the variance in (log (x+.2) transformed) pollinator visits, reveals remarkably similar variance between groups. Thus we satisfy the equal variance assumption of linear models.\n\n\n\n\n\n\n\n\n\npetal_color\nvar_visits\n\n\n\n\npink\n0.627\n\n\nwhite\n0.768\n\n\n\n\n\n\n\n# Transform data\nSR_rils  &lt;- SR_rils                          |&gt; \n  mutate(log_visits = log(.2 + mean_visits))\n\n# Compare variance\nSR_rils                                      |&gt; \n  group_by(petal_color)                      |&gt;\n  summarise(var_visits = var(log_visits))\n\n\nWhen to worry about unequal variance.\nLinear models are remarkably robust to the assumption of equal variance. You can feel confident that differences in variance will have limited influence on your results until the larger variance is more than four times larger than the smaller variance.",
    "crumbs": [
      "17. A binary explanatory variable",
      "• 17. Two-t Assumptions"
    ]
  },
  {
    "objectID": "book_sections/two_ts/2t_assumptions.html#what-to-do-when-data-violate-assumptions",
    "href": "book_sections/two_ts/2t_assumptions.html#what-to-do-when-data-violate-assumptions",
    "title": "• 17. Two-t Assumptions",
    "section": "What to do when data violate assumptions",
    "text": "What to do when data violate assumptions\nLater we’ll briefly touch on advanced techniques for dealing with data that are non-independent or and/or poorly described by the mean. Biased data are even harder. So here we will focus on solutions for non-normal data and data with unequal variance.\n\n\n\n\n\n\n\n\n\nFigure 2: A stats meme! At the novice and expert ends of the experience distribution, both agree that bootstrapping for uncertainty and permuting for NHST are useful tools. The average biostatistician, stuck in the middle, frets over which modeling approach is appropriate.\n\n\n\n\n\nWhen data are not normal: We can\n\nTransform the data (as above).\n\nBootstrap to estimate uncertainty and permute to test null hypotheses (Figure 2). OR\n\nUse a rank-based “non-parametric” test. Such tests are more formally a test for difference in medians. They work by ranking the data and comparing the observed distribution of rankings between groups to the null. We can do this in R as follows, which leads to serious rejection of the null hypothesis:\n\n\n\nwilcox.test(log_visits ~ petal_color, data = SR_rils, exact = FALSE)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  log_visits by petal_color\nW = 2127.5, p-value = 0.00001143\nalternative hypothesis: true location shift is not equal to 0\n\n\n\nWhen variance is unequal: We can use the “Welch’s t-test”, which does not assume equal variance. In fact, this is the default in R’s t.test() function. Here we use the standard t-test because the math is easier and is consistent with results in a broader linear model framework.",
    "crumbs": [
      "17. A binary explanatory variable",
      "• 17. Two-t Assumptions"
    ]
  },
  {
    "objectID": "book_sections/two_ts/2t_calcs.html",
    "href": "book_sections/two_ts/2t_calcs.html",
    "title": "• 17. Two-t Calculations",
    "section": "",
    "text": "Estimates\nPlotting the data\nlibrary(ggforce)\nggplot(SR_rils, aes(x = petal_color,\n                    y = log_visits,\n                    fill = petal_color))+\n  geom_sina(pch = 21, size = 7)+\n  scale_fill_manual(values = c(\"pink\", \"white\"))+\n  stat_summary(fun.data   = \"mean_cl_normal\", linewidth = 3)+\n  stat_summary(geom = \"line\", linewidth = 1, linetype = 2,aes(group = 1))+\n  theme(legend.position = \"none\",\n        axis.text  = element_text(size = 26),\n        axis.title = element_text(size = 26))+\n  labs(y = \"log (0.2 + visits)\")\n\n\n\n\n\n\n\n\n\nFigure 1: Each point shows an individual RIL’s mean pollinator visits on a log(visits + 0.2) scale. Flower colors means and 95% confidence intervals are plotted as thick black bars, with a dashed line connecting the group means for comparison.\nHere’s a brief refresher of our previously introduced standard summaries of associations between a categorical explanatory variable and a continuous response.\nBecause we are calculating statistics on transformed data, we should summarise the transformed data. Later in this section we will learn how to back-transform.",
    "crumbs": [
      "17. A binary explanatory variable",
      "• 17. Two-t Calculations"
    ]
  },
  {
    "objectID": "book_sections/two_ts/2t_calcs.html#estimates",
    "href": "book_sections/two_ts/2t_calcs.html#estimates",
    "title": "• 17. Two-t Calculations",
    "section": "",
    "text": "Estimating summaries of each group:\nWe can summarise within group means, and variances (or even 95% CI if we want etc) as we saw in the previous chapter. (Some of) these high-level summaries are presented in Figure 1 and calculated below:\n\n\n\n\n\n\n\n\n\npetal_color\nMEAN\nVAR\nN\n\n\n\n\npink\n0.366\n0.627\n57\n\n\nwhite\n-0.445\n0.768\n50\n\n\n\n\n\n\n\ncolor_visit_summaries &lt;- SR_rils |&gt;\n  group_by(petal_color)|&gt;\n  summarise(MEAN = mean(log_visits),\n            VAR        = var(log_visits),\n            N                 = n())\n\n\n\nEstimating summaries of differences:\nWe would also like to summarise the data jointly, including the variance, the difference in groups means, and a standardized summary of this difference:\n\nThe pooled variance: To both estimate the effect size as Cohen’s D, and estimate uncertainty we need to calculate the variance. But we have two groups, so we need something like “the average variance within each group.” The pooled variance, \\(s^2_p\\) – the variance in each group weighted by their degrees of freedom and divided by the total degrees of freedom is this average (see margin for hand calculation).\n\n\n\n\\[\n\\begin{align}\ns^2_p &= \\frac{df_1 \\times s^2_1 + df_2 \\times s^2_2}{df_1+df_2} \\\\\n&= \\frac{df_\\text{pink} \\times s^2_\\text{pink} + df_\\text{white} \\times s^2_\\text{white}}{df_\\text{pink}+df_\\text{white}} \\\\\n&= \\frac{(57-1)\\times 0.627 + (50-1)\\times 0.768}{(57-1)+(50-1)} \\\\\n&= 0.693\n\\end{align}\n\\]\n\nThe difference in means: To find this simply subtract one from the other: \\(\\text{mean}\\_\\text{diff}= 0.366 - (-0.455)= 0.811\\). \nCohen’s D as the difference in means weighted by the pooled standard deviation. Cohen’s D \\(=\\frac{\\text{mean diff}}{s_p}\\) \\(=\\frac{0.811}{\\sqrt{0.693}}\\) \\(= 0.974\\). This is a large effect size!!!\n\nWe can calculate these global summaries from summaries of each petal color (above):\n\n\n\n\n\n\n\n\n\nsummary\nestimate\n\n\n\n\nmean_diff\n0.811\n\n\npooled_var\n0.693\n\n\npooled_sd\n0.832\n\n\ncohens_D\n0.975\n\n\n\n\n\n\n\ncolor_visit_summaries|&gt;\n    summarise(mean_diff  = diff(MEAN) |&gt; abs(), \n              pooled_var = sum((N-1)*(VAR)) / (sum(N)-2),\n              pooled_sd  = sqrt(pooled_var),\n              cohens_D   = mean_diff / pooled_sd)",
    "crumbs": [
      "17. A binary explanatory variable",
      "• 17. Two-t Calculations"
    ]
  },
  {
    "objectID": "book_sections/two_ts/uncertain2t.html",
    "href": "book_sections/two_ts/uncertain2t.html",
    "title": "• 17. Uncertain-2t",
    "section": "",
    "text": "Estimating Uncertainty in Mean Difference\nA “point” estimate is just that – our informed guess of a parameter value. Point estimates come from samples, so we should always pair them with an uncertainty estimate. To estimate uncertainty in the difference in means we find the “pooled standard error” and then go to our t-distribution to turn this into a confidence interval.\n\\[SE_{\\overline{x_1}-\\overline{x_2}} = \\sqrt{s_p^2 \\Big(\\frac{1}{n_1} + \\frac{1}{n_2}\\Big)}\\]\n\\[\n\\begin{align}\ndf_\\text{total} &= df_1+df_2 \\\\\n&= (n_1 - 1) + (n_2-1)\\\\\n&= n_{total}-2\n\\end{align}\n\\]\nLet’s calculate the standard error and 95% confidence interval for the estimate of the mean difference “by hand” in R:\nthing\nvalue\n\n\n\n\nmean_diff\n0.811\n\n\npooled_var\n0.693\n\n\ndf_total\n105.000\n\n\nse\n0.161\n\n\ncrit_t\n1.983\n\n\nlower_95_CI\n0.492\n\n\nupper_95_CI\n1.131\ncolor_visit_summaries|&gt;\n  mutate(DF = N-1)|&gt;\n  summarise(mean_diff   = diff(MEAN) |&gt; abs(),\n            pooled_var  = sum((N-1)*(VAR)) / (sum(N)-2),\n            df_total    = sum(DF),\n            se          = sqrt(pooled_var*(sum(1/N))),\n            crit_t      = qt(p = 0.025, \n                             df = df_total, \n                             lower.tail = FALSE),\n            lower_95_CI = mean_diff - se * crit_t,\n            upper_95_CI = mean_diff + se * crit_t)\nNote these summaries are interesting but are pretty tough to interpret as i dont know of anyone who thinks on the log(x+0.2) scale. A",
    "crumbs": [
      "17. A binary explanatory variable",
      "• 17. Uncertain-2t"
    ]
  },
  {
    "objectID": "book_sections/two_ts/uncertain2t.html#estimating-uncertainty-in-mean-difference",
    "href": "book_sections/two_ts/uncertain2t.html#estimating-uncertainty-in-mean-difference",
    "title": "• 17. Uncertain-2t",
    "section": "",
    "text": "Remember: The equation for the pooled variance: \\[s^2_p = \\frac{\\text{df}_1 \\times s_1^2+ \\text{df}_2 \\times s_2^2}{\\text{df}_\\text{total}}\\].\n\n\\(s_i^2\\): The variance within group \\(i\\).\n\n\\(\\text{df}_i\\): The degrees of freedom in group \\(i\\).\n\n\\(\\text{df}_\\text{total}\\): The total degrees of freedom (defined in text below).\n\n\n \\(SE_{\\overline{x_1}-\\overline{x_2}}\\) for our example:\n\\[\n\\begin{align}\nSE_{\\overline{x_1}-\\overline{x_2}} &= \\sqrt{s_p^2 \\Big(\\frac{1}{n_1} + \\frac{1}{n_2}\\Big)}\\\\\n&= \\sqrt{0.693\\times\\Big({\\frac{1}{57}+\\frac{1}{50}\\Big)}}\\\\\n&=0.161\n\\end{align}\n\\]\n\nThe standard error for the difference in means is a strange looking equation. Recall that $s^2_p is the “pooled variance” and using this value implies that we believe that variance is similar within each group:\n\n\n\nThe 95% confidence interval of the difference in means: Is pretty straightforward. Now that we have our standard error, we simply multiply it by \\(t_{\\text{crit, }{\\alpha=0.05\\text{, }df_\\text{total}}}\\).\n\nThe total degrees of freedom is the sum of the degrees of freedom for each estimate:\n\n\n\n\n\n\n\n\nWe have done all our calculations on log(x+0.2) transformed data. Interpretation of these results on the original linear scale is tricky, and cannot be achieved without some additional assumptions.",
    "crumbs": [
      "17. A binary explanatory variable",
      "• 17. Uncertain-2t"
    ]
  },
  {
    "objectID": "book_sections/two_ts/uncertain2t.html#an-alternative-bootstrap-to-quantify-uncertainty",
    "href": "book_sections/two_ts/uncertain2t.html#an-alternative-bootstrap-to-quantify-uncertainty",
    "title": "• 17. Uncertain-2t",
    "section": "An alternative: Bootstrap to quantify uncertainty",
    "text": "An alternative: Bootstrap to quantify uncertainty\nEverything above is statistically valid—the data meet the assumptions of the analysis. The bummer, as we noted, is that the results are awkward to interpret. As scientists, we balance values differently - I usually prioritize clarity over perfection. As such I often presentplainly interpretable summaries over technically perfect but opaque ones.\nBelow, I bootstrap (see the section on uncertainty for review) the untransformed data to obtain a 95% confidence interval without making assumptions about equal variance or normality. The 95% bootstrap CI ranges from 0.50 to 1.52. This interval is roughly the mean \\(\\pm 2 \\times \\text{Bootstrapped SE}\\), suggesting that in this case, results are robust to violations of the assumption of normality.\nIn a later section you will see that this is basically we would get from a version of the t-test that does not assume equal variance (we assume unequal variance because on a linear scale the variancesare very different!). This further suggests tha the t-machinery is pretty robust to violations of normality assumptions.\n\n\n\n\n\n\n\n\n\nFigure 1: Bootstrap sampling distribution of the difference in mean visits (pink − white) on the original (linear) scale. The histogram shows 5,000 bootstrapped estimates of the mean difference; the red vertical lines mark the 95% percentile CI (here ≈ 0.50 to 1.52).\n\n\n\n\n\n\n\n\n\n\n\nBootstrapped SEs and CI's\n\n\nse\nlower_95\nestimate\nupper_95\n\n\n\n\n0.246\n0.562\n1.026\n1.531\n\n\n\n\n\n\n\nlibrary(infer)\n\n# SUMMARIZE THE DATA\nraw_diff &lt;- SR_rils %&gt;%\n  specify(mean_visits ~ petal_color) |&gt;            # response ~ explanatory\n  calculate(stat = \"diff in means\",                # difference in group means\n            order = c(\"pink\", \"white\"))\n\n# BOOTSTRAP\nboot_SR_visits &lt;-  SR_rils %&gt;%\n  specify(mean_visits ~ petal_color) |&gt;            # specify response ~ explanatory\n  generate(reps = 5000, type = \"bootstrap\") |&gt;     # resample\n  calculate(stat = \"diff in means\",                # calculate difference in means\n            order = c(\"pink\", \"white\"))\n\n# SUMMARIZE THE BOOTSTRAP\nbootsummary &lt;- boot_SR_visits |&gt;\n  summarise(\n    se = sd(stat),\n    lower_95 = quantile(stat, prob = 0.025),\n    estimate = pull(raw_diff ),\n    upper_95 = quantile(stat, prob = 0.975)\n    )",
    "crumbs": [
      "17. A binary explanatory variable",
      "• 17. Uncertain-2t"
    ]
  },
  {
    "objectID": "book_sections/two_ts/nhs2t.html",
    "href": "book_sections/two_ts/nhs2t.html",
    "title": "• 17. Two-sample t-test",
    "section": "",
    "text": "Testing the null of equal means\nIn the previous section, we calculated a difference in \\(\\text{log}(\\text{visits} + 0.2)\\) between pink and white flowers at site SM of \\(0.366- (-0.445) = 0.811\\). We also quantified uncertainty about this estimate as a standard error of 0.161. Recalling the equation for t:\n\\[t = \\frac{\\bar{x}-\\mu_0}{s_\\bar{x}}  = \\frac{(\\bar{x_1}-\\bar{x_2})-0}{s_\\overline{x_1-x_2}}\\] In this case\n\\[t = \\frac{0.366- (-0.445)}{0.161}=5.04\\]\nSince we had 105 degrees of freedom, our critical t is going to be close to 1.96. Because our observed t-value of \\(\\approx 5\\) is way bigger than our critical t-value of \\(\\approx 2\\), we strongly reject the null hypothesis that pink and white-flowered Clarkia xantiana subspecies parviflora RILs at site SM are visited equally by pollinators.\nWe can use the pt() function to quantify exactly how rarely the null would generate a result this or more extreme as follows:\n2 * pt(q = 5.037267, df = 105, lower.tail = FALSE) = 2e-06",
    "crumbs": [
      "17. A binary explanatory variable",
      "• 17. Two-sample t-test"
    ]
  },
  {
    "objectID": "book_sections/two_ts/nhs2t.html#testing-the-null-of-equal-means",
    "href": "book_sections/two_ts/nhs2t.html#testing-the-null-of-equal-means",
    "title": "• 17. Two-sample t-test",
    "section": "",
    "text": "More precisely, crit-t = qt(0.025, df= 102,lower.tail = F) = $1.983",
    "crumbs": [
      "17. A binary explanatory variable",
      "• 17. Two-sample t-test"
    ]
  },
  {
    "objectID": "book_sections/two_ts/nhs2t.html#a-two-sample-t-test-in-r",
    "href": "book_sections/two_ts/nhs2t.html#a-two-sample-t-test-in-r",
    "title": "• 17. Two-sample t-test",
    "section": "A two sample t-test in R",
    "text": "A two sample t-test in R\nWe can use the “formula” syntax in the t.test() function to have R test this null for us. Note that we set var.equal = TRUE. We see that this provides p-values and 95% confidence intervals identical to what we calculated ourselves:\n\nt.test(log_visits ~ petal_color, data = SR_rils, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  log_visits by petal_color\nt = 5.0312, df = 105, p-value = 0.000002024\nalternative hypothesis: true difference in means between group pink and group white is not equal to 0\n95 percent confidence interval:\n 0.4916664 1.1312798\nsample estimates:\n mean in group pink mean in group white \n          0.3661857          -0.4452874 \n\n\nAgain, we can make this output tidy / easier to process in R with the tidy() function in the broom package:\n\n\nFor some reason that I don’t understand, tidy() labels the column with the “degrees of freedom” as “parameter”. smdh.\n\nlibrary(broom)\nt.test(log_visits ~ petal_color, data = SR_rils, var.equal = TRUE)|&gt;\n  tidy()\n\n\n\n\n\n\n\n\n\nestimate\nestimate1\nestimate2\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n0.811\n0.366\n-0.445\n5.031\n2.0e-6\n105\n0.492\n1.131\nTwo Sample t-test\ntwo.sided\n\n\n\n\n\n\n\nIf our data were untransformed, or if the transformation led to a clean biological interpretation we would be done. Our (transformed) data met test assumptions, we got interesting results etc. However, I have no idea how to interpret \\(\\text{log}(\\text{visits} + 0.2)\\). So, for example the standard error and 95% confidence interval around our estimated mean difference are not easy to interpret. In such cases we have to use our understanding of biology, our value on clear communication, and our understanding of statistics and statistical assumptions to present a responsible and interpretable analysis. Below, I provide one potential route which builds on our bootstrap results.\n\nA two sample t-test is often robust\nNow that we know we can reject the null when data are transformed to meet assumptions of the two-sample t-test, and we have an estimate of the bootstrap 95% confidence interval, we could complement these analyses with an analysis of the untransformed data. This step is not always necessary or reliable - but we are using our brains to tell a coherent story.\nFirst let’s calculate our summary statistics:\n\n\n\n\n\n\n\n\n\nsummary\nestimate\n\n\n\n\nmean_diff\n1.026\n\n\npooled_var\n1.834\n\n\npooled_sd\n1.354\n\n\ncohens_D\n0.758\n\n\n\n\n\n\n\ncolor_visit_summaries &lt;- SR_rils |&gt;\n  group_by(petal_color)|&gt;\n  summarise(MEAN = mean(mean_visits),\n            VAR        = var(mean_visits),\n            N                 = n()) |&gt;\n    summarise(mean_diff  = diff(MEAN) |&gt; abs(), \n              pooled_var = sum((N-1)*(VAR)) / (sum(N)-2),\n              pooled_sd  = sqrt(pooled_var),\n              cohens_D   = mean_diff / pooled_sd)\n\nNow let’s use R to test the null hypothesis and estimate confidence intervals.For fun let’s compare results from an analysis assuming equal variance to one that does not:\n\nbind_rows(\n  t.test(mean_visits ~ petal_color, data = SR_rils, var.equal = TRUE)|&gt; tidy(),\n  t.test(mean_visits ~ petal_color, data = SR_rils, var.equal = FALSE)|&gt; tidy()\n)\n\n\n\n\n\n\n\n\n\nestimate\nestimate1\nestimate2\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n1.026\n1.759\n0.733\n3.908\n1.65e-4\n105.0\n0.505\n1.546\nTwo Sample t-test\ntwo.sided\n\n\n1.026\n1.759\n0.733\n4.047\n1.61e-4\n89.6\n0.522\n1.529\nWelch Two Sample t-test\ntwo.sided\n\n\n\n\n\n\n\n\nThe Welch’s two sample t-test does not assume equal variance, and in practice is universally better than the standard two-sample t-test (that’s why R has it as a default).\n\\[t = \\frac{\\overline{X}_1 - \\overline{X}_2}{\\sqrt{\\frac{s_1^2}{N_1} + \\frac{s_2^2}{N_2}}}\n\\text{, and degrees of freedom: } \\text{df} \\approx \\frac{\\left(\\frac{s_1^2}{N_1} + \\frac{s_2^2}{N_2}\\right)^2}{\\frac{s_1^4}{N_1^2 \\times df_1} + \\frac{s_2^4}{N_2^2 \\times df_2}}\\]\nHowever the standard two-sample t-test is often good enough. It also has the benefit of being much like all linear modelling efforts, and is simpler mathematically, so we usually teach the standard two-sample t-test. In practice the difference between these rarely matters, except for when the variance between groups is massively different",
    "crumbs": [
      "17. A binary explanatory variable",
      "• 17. Two-sample t-test"
    ]
  },
  {
    "objectID": "book_sections/two_ts/nhs2t.html#writing-up-results",
    "href": "book_sections/two_ts/nhs2t.html#writing-up-results",
    "title": "• 17. Two-sample t-test",
    "section": "Writing up results",
    "text": "Writing up results\nNow we can write up our results. Note this takes some thinking because we had to make some decisions. Here’s my attempt:\nAt the Sawmill Road site, pink-flowered Clarkia xantiana ssp. parviflora RILs received, on average, more pollinators during a 15-minute observation than white-flowered RILs (mean pink = 1.76, mean white = 0.73). The mean difference of 1.03 visits was statistically significant (Welch’s t = 4.05, df = 89.6, p = 0.00016) and associated with a moderate-to-large effect size (Cohen’s D = 0.75). Results were robust to the right-skew in the data: a log(x + 0.2)-transformed analysis yielded an even stronger signal (t = 5.03, df = 106, p = 0.000002), and bootstrap confidence intervals closely matched analytic ones. We reject the null and conclude that pink-flowered plants attract more pollinators than white-flowered plants at this site.",
    "crumbs": [
      "17. A binary explanatory variable",
      "• 17. Two-sample t-test"
    ]
  },
  {
    "objectID": "book_sections/two_ts/2t_summary.html",
    "href": "book_sections/two_ts/2t_summary.html",
    "title": "• 17. Two t Summary",
    "section": "",
    "text": "Chapter summary\nLinks to: Summary. Chatbot tutor. Questions. Glossary. R functions. More resources.\nWe can naturally build from our description and analysis of a single sample to the more common scenario of comparing two samples. When data meet the assumptions of independence, lack of bias, being well summarized by the mean, normal residuals, and equal variance between groups, we can use the standard t-test machinery (with a slightly different calculation for the standard error) to test null hypotheses and estimate uncertainty. When group variances differ, we can use Welch’s t-test for unequal variance.",
    "crumbs": [
      "17. A binary explanatory variable",
      "• 17. Two t Summary"
    ]
  },
  {
    "objectID": "book_sections/two_ts/2t_summary.html#2t_summary_chapter-summary",
    "href": "book_sections/two_ts/2t_summary.html#2t_summary_chapter-summary",
    "title": "• 17. Two t Summary",
    "section": "",
    "text": "Chatbot tutor\n\nPlease interact with this custom chatbot (link here). I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you got what you needed from it.",
    "crumbs": [
      "17. A binary explanatory variable",
      "• 17. Two t Summary"
    ]
  },
  {
    "objectID": "book_sections/two_ts/2t_summary.html#2t_summary_practice-questions",
    "href": "book_sections/two_ts/2t_summary.html#2t_summary_practice-questions",
    "title": "• 17. Two t Summary",
    "section": "Practice Questions",
    "text": "Practice Questions\nTry these questions! By using the R environment you can work without leaving this “book”. I even pre-loaded all the packages you need!\n\nSETUP: There are plenty of reasons to choose your partner carefully. In much of the biological world a key reason is “evolutionary fitness” - presumably organisms evolve to choose mates that will help them make more (or healthier) children. This could, for example explain Kermit’s resistance in one of the more complex love stories of our time, as frogs and pigs are unlikely to make healthy children..\nTo evaluate this this idea Swierk & Langkilde (2019), identified a males top choice out of two female wood frogs and then had them mate with the preferred or unpreferred female and counted the number of hatched eggs.\nThe R code below loads the data It is meant to be loaded onto this console below too, but if something goes wrong, just paste this into your R console outside of this book\n\n\nlibrary(dplyr);    library(readr);  library(ggplot2);   library(janitor)\nfrog_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/biostat/master/data/Swierk_Langkilde_BEHECO_1.csv\"\nfrogs &lt;- read_csv(frog_link) |&gt;\n  clean_names()\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ1. Complete the code above, what pattern do you see?\n\n Similar numbers of hatched eggs in preferred and nonpreferred treatments Way more hatched eggs in the preferred treatment Way more hatched eggs in the nonpreferred treatment\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Histograms of the number of hatched eggs across treatments. (A) Distribution when pooling both treatments together. (B) Distributions shown separately for the nonpreferred and the preferred treatments.\n\n\n\n\n\n\nQ2. Consider Figure 2, above. Which plot is more useful to evaluate the normality assumption of the two sample t-test?\n\n A) Both treaments combined. B) Seperate data by treatment. It depends, you should integrate information from both. Nether plot is appropriate\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ3. Consider the output of the code above. Should you feel comfortable assuming homoscedasticity for a two sample t-test.\n\n Yes No Maybe I don't know (I need to run a statistical test to evaluate this assumption) Can you repeat the question?\n\n\nQ4. For now we’ll go on with a t-test approach, regardless. So find the pooled variance in the R interface above .\n\n\nClick for pooled variance equation\n\n\\[s^2_p = \\frac{\\text{df}_1 \\times s^2_1 + \\text{df}_2 \\times s^2_2}{\\text{df}_1+\\text{df}_2} \\text{, and df}_i = n_i-1\\]\n\n\n\nClick for worked answer\n\n\nWe can copy and paste R output into a calculator (that calculator might be R). I think this is best for understanding.\n\n\nfrogs |&gt; \n  group_by(treatment)|&gt;\n  summarise(MEAN = mean(hatched_eggs),\n            VAR  = var(hatched_eggs),\n            N    = n())\n\n# A tibble: 2 × 4\n  treatment     MEAN    VAR     N\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1 nonpreferred  414. 56118.    29\n2 preferred     345. 67412.    27\n\n# Pooled variance\n((56118*(29-1)) +(67412*(27-1))) / (29 + 27 - 2)\n\n[1] 61555.85\n\n\nOR\n\nWe do this in one long workflow in R. I think this is the best practice for getting exact answers.\n\n\nfrogs |&gt; \n    group_by(treatment)|&gt;\n    summarise(MEAN = mean(hatched_eggs),\n              VAR  = var(hatched_eggs),\n              N    = n())|&gt;\n    summarise(pooled_var = sum((N-1)*VAR) / (sum(N)-2) )\n\n# A tibble: 1 × 1\n  pooled_var\n       &lt;dbl&gt;\n1     61556.\n\n\n\n\nQ5. Given the answers above, characterize this effect size.\n\n Not worth reporting (Cohen's D is less than 0.01) Tiny (Cohen's D is between 0.01 and 0.20) Small (Cohen's D is between 0.20 and 0.50) Medium (Cohen's D is between 0.50 and 0.80) Large (Cohen's D is between 0.80 and 1.10) Very large (Cohen's D is between 1.20 and 2.00) Huge (Cohen's D is greater than 2.00)\n\n\nQ6. From this Cohens D value, we conclude that:\n\n We should reject the null hypothesis We should fail to reject the null hypothesis The null is false The null is true Even if statistically significant, this effect size would be practically negligible.\n\n\nQ7 State the null hypothesis\n\n\n\n\nQ8 State the alternative hypothesis\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nQ9. Which values are NOT IN the 95% confidence interval or the difference (i.e. preferred - nonpreferred) select all that apply:\n\n -92 -42 2 52 102 152 202 252\n\n\nQ10) The output of the code above shows a p-value of \\(\\approx 0.30\\), so we reject the null hypothesis. The output also shows a t-value of \\(\\approx 1\\). If all I knew was that p-value would I be able to reject the null at \\(\\alpha = 0.05\\)? YesNoCan’t tell you without the degress of freedom.\n\nQ11. We fail to reject the null hypothesis. This means the null is TRUEFALSEI dont know.\n\nQ12. If you had to make a bet, the safer bet in this case is that the null hypothesis is TRUEFALSE",
    "crumbs": [
      "17. A binary explanatory variable",
      "• 17. Two t Summary"
    ]
  },
  {
    "objectID": "book_sections/two_ts/2t_summary.html#2t_summary_glossary-of-terms",
    "href": "book_sections/two_ts/2t_summary.html#2t_summary_glossary-of-terms",
    "title": "• 17. Two t Summary",
    "section": "📊 Glossary of Terms",
    "text": "📊 Glossary of Terms\n\n\nAssumptions\n\nIndependence: Each observation must be independent of others; in two-sample designs, each group’s values should not influence the other.\nUnbiased Sampling: Data should be collected without systematic bias so that results generalize to the population.\nNormality of Residuals: Within each group, the distribution of residuals (observed – group mean) should be approximately normal.\nHomoscedasticity (Equal Variance): The spread of values should be roughly the same in each group. In practice, differences smaller than a 4:1 ratio usually have little effect.\n\n\n\nSummaries & Estimates\n\nGroup Mean (\\(\\bar{x}\\)): The average of all observations in a group.\nVariance (\\(s^2\\)): The spread of observations within a group, calculated as \\(s^2 = \\frac{\\sum (x_i - \\bar{x})^2}{n-1}\\).\nPooled Variance ($s_p^2$): A weighted average of group variances used in the standard two-sample t-test: \\(s_p^2 = \\frac{df_1 s_1^2 + df_2 s_2^2}{df_1 + df_2}\\)\nDifference in Means (\\(\\Delta \\bar{x}\\)): The difference between the two group averages, \\(\\Delta \\bar{x} = \\bar{x}_1 - \\bar{x}_2\\).\nCohen’s D: A standardized measure of effect size for two means: \\(d = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_p}\\)\n\n\n\nThe Two-Sample t-Test\n\nTest Statistic (t): Measures how many standard errors separate the observed difference from the null hypothesis difference (usually 0): \\(t = \\frac{(\\bar{x}_1 - \\bar{x}_2)}{s_{\\overline{x_1 - x_2}}}\\)\nDegrees of Freedom (df): For the standard test with equal variance: \\(df = (n_1 - 1) + (n_2 - 1) = n_1 + n_2 - 2\\)\nWelch’s t-Test: A version of the t-test that does not assume equal variance. The denominator uses each group’s variance scaled by its sample size, and \\(df\\) are approximated with the Welch–Satterthwaite equation.\nWilcoxon Rank-Sum Test: A non-parametric alternative to the two-sample t-test that compares the ranks of values between groups, effectively testing for differences in medians.",
    "crumbs": [
      "17. A binary explanatory variable",
      "• 17. Two t Summary"
    ]
  },
  {
    "objectID": "book_sections/two_ts/2t_summary.html#2t_summary_key-r-functions",
    "href": "book_sections/two_ts/2t_summary.html#2t_summary_key-r-functions",
    "title": "• 17. Two t Summary",
    "section": "🛠️ Key R Functions",
    "text": "🛠️ Key R Functions\n\n\nFunctions\n\nt.test(): Performs a one-sample, two-sample, or paired t-test. Use the formula syntax t.test(y ~ group, data = df, var.equal = TRUE) for the standard two-sample test. By default, Welch’s test is used (var.equal = FALSE).\nwilcox.test(): Performs the Wilcoxon rank-sum test (Mann–Whitney U test), a non-parametric alternative that compares medians between two groups.\nbroom::tidy(): Converts test outputs (like from t.test()) into tidy data frames, making it easier to report and manipulate results.\n\n\n\nSyntax\n\nTwo-sample t-test (equal variance assumed)\n\n\nt.test(y ~ group, data = df, var.equal = TRUE)\n\n\nTwo-sample Welch’s t-test (default in R, no equal variance assumption)\n\n\nt.test(y ~ group, data = df)\n\n\nWilcoxon rank-sum test (non-parametric alternative)\n\n\nwilcox.test(y ~ group, data = df, exact = FALSE)",
    "crumbs": [
      "17. A binary explanatory variable",
      "• 17. Two t Summary"
    ]
  },
  {
    "objectID": "book_sections/two_ts/2t_summary.html#2t_summary_additional-resources",
    "href": "book_sections/two_ts/2t_summary.html#2t_summary_additional-resources",
    "title": "• 17. Two t Summary",
    "section": "Additional resources",
    "text": "Additional resources\n\nVideos:\n\nCrash Course Statistics #27 T test.\nKahn Academy: Two sample t-test.\n\nOther resources:\n\nDatanovia: t-test\n\n\n\n\n\n\nSwierk, L., & Langkilde, T. (2019). Fitness costs of mating with preferred females in a scramble mating system. Behavioral Ecology, 30(3), 658–665. https://doi.org/10.1093/beheco/arz001",
    "crumbs": [
      "17. A binary explanatory variable",
      "• 17. Two t Summary"
    ]
  },
  {
    "objectID": "book_sections/f.html",
    "href": "book_sections/f.html",
    "title": "18. F this!",
    "section": "",
    "text": "The utility of \\(F\\)\nIn this chapter, we focus on the simplest case, comparing two groups, to better undertand what the F statistic represents (or What the \\(f\\) is going on!) We’ll connect this to concepts you already know (t-tests, sampling distributions), work through the math of partitioning variance, learn how to run ANOVA in R, and end by showing how these methods are all part of a unified linear model framework\nIn the coming chapters you will see that this view of “partitioning variance” has broad utility. In addition to its utility in partitioning variability within and among two groups, the approach in this chapter can be applied to\nWe will work through these applications in coming chapters!",
    "crumbs": [
      "18. F this!"
    ]
  },
  {
    "objectID": "book_sections/f.html#the-utility-of-f",
    "href": "book_sections/f.html#the-utility-of-f",
    "title": "18. F this!",
    "section": "",
    "text": "More than two groups.\n\nA numeric explanatory variable.\n\nMultiple explanatory variables, and even\nExplanatory variables whose interaction influences the response variable.",
    "crumbs": [
      "18. F this!"
    ]
  },
  {
    "objectID": "book_sections/f.html#data-and-hypotheses-for-this-section",
    "href": "book_sections/f.html#data-and-hypotheses-for-this-section",
    "title": "18. F this!",
    "section": "Data and hypotheses for this section",
    "text": "Data and hypotheses for this section\nIn the previous section, we saw that at Sawmill Road, pink parviflora RILs attracted more pollinators than did white parviflora RILs. We cared about this because we want to know if petal color influences hybridization – a question we will address later.\nBut there is a bigger motivating question than what happens in an experiment. That is, we want to know what has actually happened (and what is happening) in nature. In this chapter we get at the question of “does petal color influence hybridization?” in a somewhat backwards way. Instead of asking if pink flowers set more hybrid seed than white flowers, we analyze sequence data from pink and white flowered parviflora found in natural hybrid zones and ask if one petal color is associated with more introgression of xantiana ancestry.\n\n\nIntrogression is the incorporation of ancestry from one lineage into the genome of another, this happens by hybridization and subsequent backcrossing.",
    "crumbs": [
      "18. F this!"
    ]
  },
  {
    "objectID": "book_sections/f.html#whats-ahead",
    "href": "book_sections/f.html#whats-ahead",
    "title": "18. F this!",
    "section": "What’s ahead",
    "text": "What’s ahead\nIn this section\n\nWe go through the mathemagical foundation of partitioning variance and how this relates to our understanding populations, and samples as draws from the sampling distribution.\nWe then show how to calculate the sum of squares error and sums of squares model and related measures.\nNext, we introduce \\(r^2\\) as the “effect size” in such a model, and provide guidance on how to interpret this common summary.\nWe then show how to use this partitioning of variance to test the null hypothesis that all groups represent samples from the same population.\nBefore the chapter summary, we reflect a bit on correlation, causation and artifacts that may influence the patterns that we observe.",
    "crumbs": [
      "18. F this!"
    ]
  },
  {
    "objectID": "book_sections/f/from_sampling.html",
    "href": "book_sections/f/from_sampling.html",
    "title": "• 18. F the ratio of variance",
    "section": "",
    "text": "Predicting \\(\\sigma_\\bar{x}\\) from \\(\\sigma\\)\nRemember our “statistical” view of where data come from. We imagine that the data we observe are a sample of size \\(n\\) from a population with a true mean \\(\\mu\\) and standard deviation \\(\\sigma_x\\). Although we can’t know these true parameters, we estimate them using the sample mean \\(\\bar{x}\\) and standard deviation \\(s\\)\nWe envision the distribution of sample means we would get by repeatedly sampling from the same population as the sampling distribution. If the population is normally distributed with standard deviation \\(\\sigma\\), then the standard deviation of the sample means (aka the standard error) is:\n\\[\\sigma_{\\overline{x}} = \\frac{\\sigma}{\\sqrt{n}}\\]\nFigure 1: The F distirbution is magic.\nSquaring both sides gives the variance of the sampling distribution of the mean as the population variance divided by the sample size:\n\\[\\sigma_{\\overline{x}}^2 = \\frac{\\sigma_x^2}{n}\\]\nMultiplying both sides by \\(n\\), reveals the variance among sample means (for the same population) times the sample size should equal the variance in the population:\n\\[\\sigma_{\\overline{x}}^2 \\times n  = \\sigma_x^2\\]",
    "crumbs": [
      "18. F this!",
      "•  18. F the ratio of variance"
    ]
  },
  {
    "objectID": "book_sections/f/from_sampling.html#predicting-sigma_barx-from-sigma",
    "href": "book_sections/f/from_sampling.html#predicting-sigma_barx-from-sigma",
    "title": "• 18. F the ratio of variance",
    "section": "",
    "text": "Parameters and estimates in an \\(F\\)\nSo we can turn these ideas into parameters to estimate.\n\n\n\n\n\n\n\n\n\nSource\nParameter\nEstimate\nNotation\n\n\n\n\nModel\n\\(n \\times \\sigma_\\bar{x}^2\\)\nMean squares model\n\\(\\text{MS}_\\text{ Model}\\)\n\n\nError\n\\(\\sigma^2_x\\)\nMean squares error\n\\(\\text{MS}_\\text{ Error}\\)\n\n\nTotal\n\\(n \\times \\sigma_\\bar{x}^2 + \\sigma^2_x\\)\nMean squares total\n\\(\\text{MS}_\\text{ Total}\\)\n\n\n\nWe can use these values to calculate the ratio of variance among and within groups, \\(F\\). Notice that when our two samples come from the same population, we expect \\(F\\) to equal one (save some sampling error).\n\\[F = \\frac{\\text{MS}_\\text{Model}}{ \\text{MS}_\\text{Error} }\\]\n\nIn one-way ANOVA, what I call “Mean squares model” (\\(\\text{MS}_\\text{Model}\\)) is often called “Mean squares group” ( \\(\\text{MS}_\\text{Groups}\\)). I use mean squares model to highlight that this extends beyond ANOVA to regression and other linear models.\n\n\n\nImplications for NHST:\nThe derivations above ASSUMES that samples are\n\nIf all groups are drawn from the same population (the null hypothesis), then this equality holds: the variance among group means is exactly what we’d expect from sampling error.\nIf groups come from different populations (the alternative), then the variance among group means will be larger than \\(\\sigma^2\\).\n\nIn the next section we will see how we can use this framework to test the null that all samples come from the same statistical population.",
    "crumbs": [
      "18. F this!",
      "•  18. F the ratio of variance"
    ]
  },
  {
    "objectID": "book_sections/f/f_calcs.html",
    "href": "book_sections/f/f_calcs.html",
    "title": "• 18. F Calculations",
    "section": "",
    "text": "Visualizing components of variation\nLet’s look at the association between admixture proportion of natural parviflora plants in the SR hybrid zone, and petal color. The motivating idea is – if white petals do not attract pollinators (as seen in the previous chapter), white plants will have had less admixture (because they would not receive xantiana pollen).\nCode\nlibrary(broom)\nlibrary(patchwork)\nlibrary(tidyverse)\n\n\n# ---- Load & prepare data ----\nclarkia_hz &lt;- read_csv(hz_link)|&gt; \n  select(-weighted)|&gt;\n  rename(admix_proportion = `cutoff0.9`) |&gt;\n  filter(!is.na(admix_proportion))|&gt;\n  clean_names() |&gt;\n  mutate(site = if_else(site == \"SAW\", \"SR\", site)) |&gt;\n  filter(subsp==\"P\")|&gt;\n  filter(site == \"SM\") |&gt;\n  mutate(tmp = as.numeric(factor(petal_color)) + admix_proportion,\n         id = factor(id),\n         id = fct_reorder(id, tmp))\n\n# Fit model\nsm_lm &lt;- lm(admix_proportion ~ petal_color, data = clarkia_hz)\n\n# Add augment info\nplot_dat &lt;- augment(sm_lm) |&gt;\n  mutate(id = row_number(),\n         mean_admix = mean(admix_proportion, na.rm = TRUE)) |&gt;\n  mutate(tmp = as.numeric(factor(petal_color)) + admix_proportion,\n         id = factor(id),\n         id = fct_reorder(id, tmp))\n\n# Data for mean lines per group\nmean_lines &lt;- plot_dat |&gt;\n  group_by(petal_color) |&gt;\n  summarise(xstart = min(as.numeric(id)),\n            xend   = max(as.numeric(id)),\n            mean_val = mean(admix_proportion),\n            .groups = \"drop\")\n\n# --- (C) Total deviation ---\nc &lt;- ggplot(plot_dat,\n            aes(x = as.numeric(id), y = admix_proportion, color = petal_color)) +\n  geom_point(alpha = 0.7, size = 2) +\n  geom_hline(aes(yintercept = mean_admix)) +\n  labs(title = \"  (C) Total deviation\",\n       color = \"Petal color\",\n       y = \"Admixture proportion\") +\n  geom_segment(aes(xend = as.numeric(id), \n                   yend = mean_admix),\n               color = \"black\", alpha = 0.5, linewidth = 0.5) +\n  theme_light(base_size = 13) +\n  theme(axis.title.x = element_blank(),\n        axis.text.x  = element_blank(),\n        axis.ticks.x = element_blank())\n\n# --- (A) Model deviation ---\na &lt;- ggplot(plot_dat,\n            aes(x = as.numeric(id), y = admix_proportion, color = petal_color)) +\n  geom_point(alpha = 0.7, size = 2) +\n  geom_hline(aes(yintercept = mean_admix)) +\n  # deviation lines\n  geom_segment(aes(xend = as.numeric(id), y = .fitted, yend = mean_admix),\n               color = \"black\", alpha = 0.5, linewidth = 0.5) +\n  # fitted line\n  geom_line(aes(y = .fitted), linewidth = 1.2, show.legend = FALSE) +\n  # horizontal group means\n  geom_segment(data = mean_lines,\n               aes(x = xstart, xend = xend,\n                   y = mean_val, yend = mean_val,\n                   color = petal_color), show.legend = FALSE,\n               linewidth = 1.2, inherit.aes = FALSE) +\n  labs(title = \"(A) Model deviation +\",\n       color = \"Petal color\",\n       y = \"Admixture proportion\")  +\n  theme_light(base_size = 13) +\n  theme(axis.title.x = element_blank(),\n        axis.text.x  = element_blank(),\n        axis.ticks.x = element_blank())\n\n# --- (B) Error deviation ---\nb &lt;- ggplot(plot_dat,\n            aes(x = as.numeric(id), y = admix_proportion, color = petal_color)) +\n  geom_point(alpha = 0.7, size = 2) +\n  geom_hline(aes(yintercept = mean_admix)) +\n  # deviation lines to fitted\n  geom_segment(aes(xend = as.numeric(id), yend = .fitted),\n               color = \"black\", alpha = 0.5, linewidth = 0.5) +\n  # fitted line\n  geom_line(aes(y = .fitted), linewidth = 1.2, show.legend = FALSE) +\n  # horizontal group means\n  geom_segment(data = mean_lines,\n               aes(x = xstart, xend = xend,\n                   y = mean_val, yend = mean_val,\n                   color = petal_color),\n               linewidth = 1.2, inherit.aes = FALSE, show.legend = FALSE) +\n  labs(title = \"(B) Error (residual) deviation =\",\n       color = \"Petal color\",\n       y = \"Admixture proportion\")  +\n  theme_light(base_size = 13) +\n  theme(axis.title.x = element_blank(),\n        axis.text.x  = element_blank(),\n        axis.ticks.x = element_blank())\n\n\n\n# Combine panels with shared y-axis and legend\n(a + b + c) +\n  plot_layout(guides = \"collect\",\n              axis_titles = \"collect\",\n              axes = \"collect_y\") &\n  theme(legend.position = \"right\",\n        legend.text = element_text(size = 20),\n        legend.title = element_text(size = 20),)\n\n\n\n\n\n\n\n\nFigure 1: Partitioning the contribution of petal color and error to admixture proportion of pink and white parviflora plants in the Squirrel Mountain hybrid zone. (A) Model deviation: Lines show the distance between each sample’s predicted value (i.e. group means) and the grand mean (black line). (B) Error (residual) deviation: Lines show the distance between each observation and its predicted value (i.e. group means). (C) Total deviation: Lines show the distance between each observation and the grand mean.\nBefore calculating these sums of squares and mean squares it’s worth visualizing the data and where these deviations come from. Figure 1 shows the:",
    "crumbs": [
      "18. F this!",
      "•  18. F Calculations"
    ]
  },
  {
    "objectID": "book_sections/f/f_calcs.html#what-these-symbols-mean",
    "href": "book_sections/f/f_calcs.html#what-these-symbols-mean",
    "title": "• 18. F Calculations",
    "section": "What these symbols mean",
    "text": "What these symbols mean\n\\(\\Sigma\\) Sum over all individuals.\n\\(Y\\): The response variable.\n\\(\\hat{Y}_i\\): The predicted value of \\(Y\\) in the \\(i^{th}\\) individual.\n\\(\\bar{Y}\\): The average value of \\(Y\\).\n\\(Y_i\\): The actual value of \\(Y\\) in the \\(i^{th}\\) individual.",
    "crumbs": [
      "18. F this!",
      "•  18. F Calculations"
    ]
  },
  {
    "objectID": "book_sections/f/f_calcs.html#calculating-ss-and-ms-from-fig-devs",
    "href": "book_sections/f/f_calcs.html#calculating-ss-and-ms-from-fig-devs",
    "title": "• 18. F Calculations",
    "section": "Calculating SS and MS from Figure 1",
    "text": "Calculating SS and MS from Figure 1\nIn each panel of Figure 1, we can square the length of each line and add them up to find each of the relevant sums of squares! But first we need to find the lengths of these lines. In R we can do this by:\n\nBuilding a linear model (See Chapter 7),\n\nFinding predicted values (with the augment() function in the broom package).\n\nUsing basic dplyr manipulations like mutate() and summarise() to do our calculations.\n\n\nBuilding a linear model\nWe can use the lm(RESPONSE ~ EXPLANATORY, data = DATA) function in R to build model admixture proportion as a function of petal color:\n\n# Data already filtered to be from squirrel mountain\nsm_lm &lt;- lm(admix_proportion ~ petal_color, data = clarkia_hz)\n\n\n\n\nCall:\nlm(formula = admix_proportion ~ petal_color, data = clarkia_hz)\n\nCoefficients:\n     (Intercept)  petal_colorwhite  \n         0.01896          -0.01293  \n\n\n\nAs a refresher on how to read this:\n\n\nSee Chapter 7 for more detail.\n\nThe output (Intercept): The value, 0.019, is the admixture proportion for plants with the “reference level” petal color. Here, the reference level is “pink flowers”. We know this because pink does not appear in the linear model output. \\(\\text{(Intercept)} =\\hat{Y}_\\text{pink}\\)\n\n\n\n\\(\\hat{Y_\\text{pink}} = 0.019\\).\n\nThe output, petal_colorwhite: The value, -0.0129, is the difference in the admixture proportion of white vs pink plants: \\(\\text{petal\\_colorwhite} = \\hat{Y}_{\\text{white}} - \\hat{Y}_{\\text{pink}}\\)\n\n\n\n\\(\\hat{Y}_\\text{pink}\\) = \\(0.019\\) + \\(-0.0129\\) \\(= 0.0061\\).\n\n\nSS Calculations\nNow that we have our model, we can use broom’s augment() function to extract key information about entries in the model. Specifically, we focus on\n\nadmix_proportion: The observed value of the response variable, \\(Y_i\\),\n\n.fitted: The predicted value of the response variable, \\(\\hat{Y}_i\\),\n\n.resid: The residual (i.e. \\(Y_i - \\hat{Y}_i\\)).\n\nI also select the explanatory variable petal_color, just for fun, and use mutate() to add a column showing the grand mean admixture proportion:\n\nlibrary(broom)\nsm_lm_augment &lt;- augment(sm_lm) |&gt;\n  select(admix_proportion, petal_color, .fitted, .resid)|&gt;\n  mutate(grand_mean = mean(admix_proportion))\n\n\n\n\n\n\n\nWe can now apply the definitions of the different types of sums of squares to summarize our data. I also report the number of samples and number of groups because we need that in a bit!\n\nsm_lm_augment |&gt;\n  summarise(SS_model = sum((.fitted - grand_mean)^2),\n            SS_resid  = sum((.fitted - admix_proportion)^2),# OR alternatively\n            other_SS_resid_calc = sum(.resid^2),\n            SS_total = sum((admix_proportion- grand_mean)^2), \n            n = n(),\n            n_groups = n_distinct(petal_color))\n\n# A tibble: 1 × 6\n  SS_model SS_resid other_SS_resid_calc SS_total     n n_groups\n     &lt;dbl&gt;    &lt;dbl&gt;               &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n1  0.00183  0.00105             0.00105  0.00288    46        2\n\n\n\n\n\\(R^2\\) as the effect size\nimportance of how our explanatory variable(s) modulate Remember NHST is only one goal of statistics – we also want to know the extent to which our explanatory variable(s) modulate the value of the response variable. \\(R^2\\) quantifies the overall effect of predictors on the response variable as:\n\\[R^2 = \\frac{\\text{SS}_\\text{model}}{\\text{SS}_\\text{total}}\\]\n\\(R^2\\) summarizes the effect size as the proportion of variance in the response variable explained by our model. For our example, petal color “explains” \\(r^2 = \\frac{0.00183}{0.00288} =0.635\\), or \\(\\approx 63%\\) of the variance in admixture proportion at Squirrel Mountain.\nINTERPRETING \\(R^2\\)\nIt is important to note that, although \\(R^2\\) is a good summary of effect size, it can only be interpreted within the study of interest.\n\n\n\n\n\n\n\n\n\nFigure 2: The impact of relative sampling effort on \\(R^2\\) for a linear model predicting admixture proportion from petal color in parviflora plants from Squirrel Mountain. Each point represents the average \\(R^2\\) across 30 random subsamples of white-flowered individuals (pink-flowered sample size held constant at 18). The red dashed line indicates the \\(R^2\\) value (0.635) from the full dataset with 28 white and 18 pink individuals.\n\n\n\n\n\n\n\n It is incorrect to state \n\n\nPetal color explains 63.4% of the variance in admixture proportion or admixture in parviflora.\n\n\n\n\n\n It is correct to state \n\n\nPetal color explained 63.4% of the variance in admixture proportion in a sample of 18 pink and 28 white parviflora plants.\n\n\n\n\n\nThe value of \\(R^2\\) depends on the unique characteristics of the study sample, such as the relative representation of each host species and the specific conditions they experience.\nTo understand why, consider the effect of downsampling white-petaled flowers from our data set. Changes in sample composition or size will change \\(R^2\\) from roughly \\(64\\%\\) in our full data set to less than \\(40\\%\\) if we only have three white-petaled flowers Figure 2.\n\n\n\n\\(F\\), Degrees of Freedom & Mean Squares\nRemember our mathematical expectations – if our samples represent draws from the same population, the variance among groups will equal the variance within groups (save sampling error). This means that under the null hypothesis Mean Squares Model equals Mean Squares Error (\\(\\sigma_{\\overline{x}}^2 \\times n  = \\sigma_x^2\\)).\nThus to test the null hypothesis that all samples came from the same population, we must calculate two types of mean squares, each with its own degrees of freedom:\n\n\nMean Squares Model\n\\[\\text{Mean Squares Model} = \\frac{\\text{SS model}}{\\text{df}_\\text{model}}\\]\nIn our case, \\(\\text{MS}_\\text{model} = \\frac{0.00183}{2-1}=0.00183\\).\n\n\n\\[\\text{Where    }  \\text{df}_\\text{model}= n_\\text{groups} - 1\\]\n\n\n\nMean Squares Error\n\\[\\text{Mean Squares Error} = \\frac{\\text{SS error}}{\\text{df}_\\text{error}}\\]\n\n\n\\[\\text{Where  }   \\text{ df}_\\text{error}= n - n_\\text{groups}\\]\nIn our case, \\(\\text{MS}_\\text{error} = \\frac{0.001052719}{46-2}=0.000023925\\).\n\n\n\nF and NHST\nUnder the null hypothesis Mean Squares Model equals Mean Squares Error (\\(\\sigma_{\\overline{x}}^2 \\times n  = \\sigma_x^2\\)). Like all null hypotheses, we need a test statistic to evaluate the compatibility of the data with this null model. Here our test statistic is the ratio of Mean Squares Model to Mean Squares Error, \\(F\\):\n\\[F = \\frac{\\text{MS}_\\text{Model}}{\\text{MS}_\\text{Error}}\\]\nIn our case, \\(F = \\frac{0.00183}{0.00002392}=76.5\\).\nTo test the null hypothesis that \\(F\\) has an expected value of one, we find the p-value corresponding to the probability of finding a value this or more extreme from the F distribution. The F requires both \\(\\text{df}_\\text{model}\\), and \\(\\text{df}_\\text{error}\\), in that order. Because any group can have a greater (or smaller) mean according to this framing, we only look at the upper tail of the F-distribution greater than one.\nP = pf(q = 76.5, df1 = 1, df2=44,lower.tail = FALSE) = \\(3.51 \\times 10^{-11}\\).\nBecause the p-value is well smaller than our traditional \\(\\alpha\\) of 0.05. We resoundingly reject the null hypothesis. In fact, if we generated one sample from the F distribution every second, we would need to wait for more than 3,000 years until a random sample was this extreme.",
    "crumbs": [
      "18. F this!",
      "•  18. F Calculations"
    ]
  },
  {
    "objectID": "book_sections/f/FR.html",
    "href": "book_sections/f/FR.html",
    "title": "• 18. F and anova in R",
    "section": "",
    "text": "The aov() |&gt; summary() pipeline\nIn the previous sections we worked through the math and the theory, and even how we could use R to do our calculations for us. This is all useful and I hope it helps you understand what is going on behind the scenes. But now that we know what’s going on, we can have R do our work for us. Here, I show you two ways to have R calculate F and conduct an anova test for us.\nIn so doing we consider how to interpret R’s output, and how to convert it to a “tidy” format with broom’s glance() and tidy() functions.\nThe aov() function is built specifically to conduct ANOVA in R. To do so, it uses the familiar formula syntax: (RESPONSE ~ EXPLANATORY, data = DATA)\nadmix_anova &lt;- aov(admix_proportion ~ petal_color, data= clarkia_hz )\nThe aov() object contains the relevant sums of squares (which match our hand calculations!) and the degrees of freedom. To actually view the ANOVA table, we use summary() on the aov object. This table includes the sums of squares, mean squares, the F statistic, and the associated p-value.\nsummary(admix_anova)\n\n            Df   Sum Sq   Mean Sq F value   Pr(&gt;F)    \npetal_color  1 0.001831 0.0018312   76.54 3.49e-11 ***\nResiduals   44 0.001053 0.0000239                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n27 observations deleted due to missingness\nbroom’s tidy() function provides this output in a “tidy” format, its glance() function shows some relevant summaries of the model (including \\(R^2\\)).\nlibrary(broom)\ntidy(admix_anova)\n\n# A tibble: 2 × 6\n  term           df   sumsq    meansq statistic   p.value\n  &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 petal_color     1 0.00183 0.00183        76.5  3.49e-11\n2 Residuals      44 0.00105 0.0000239      NA   NA       \n\nglance(admix_anova)\n\n# A tibble: 1 × 6\n  logLik   AIC   BIC deviance  nobs r.squared\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;\n1   180. -355. -349.  0.00105    46     0.635",
    "crumbs": [
      "18. F this!",
      "•  18. F and anova in R"
    ]
  },
  {
    "objectID": "book_sections/f/FR.html#the-lm-anova-pipeline",
    "href": "book_sections/f/FR.html#the-lm-anova-pipeline",
    "title": "• 18. F and anova in R",
    "section": "The lm() |> anova() pipeline",
    "text": "The lm() |&gt; anova() pipeline\nAlternatively, we can fit a linear model and then hand it to anova() to create the ANOVA table. This produces the same output as aov() |&gt; summary(), but the intermediate object is an lm object rather than an aov object. Sometimes one format is just easier to work with than the other, but your results will be the same.\n\nlm(admix_proportion ~ petal_color, data= clarkia_hz ) |&gt;\n  anova()\n\nAnalysis of Variance Table\n\nResponse: admix_proportion\n            Df    Sum Sq    Mean Sq F value    Pr(&gt;F)    \npetal_color  1 0.0018312 0.00183122  76.539 3.486e-11 ***\nResiduals   44 0.0010527 0.00002393                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nIf we want more information about our model, we can pass the lm object to glance():\n\nlm(admix_proportion ~ petal_color, data= clarkia_hz ) |&gt;\n  glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared   sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.635         0.627 0.00489      76.5 3.49e-11     1   180. -355. -349.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\nglance() works on model objects like lm or aov, but not on the ANOVA table itself. So lm() |&gt; anova() |&gt; glance() returns an empty tibble.\n\n\nHere is a brief summary of common linear model workflows in R and what they give us.\n\n\n\n\n\n\n\n\nTask\nFunction(s)\nOutput object\n\n\n\n\nRun ANOVA directly\naov() |&gt; summary()\nANOVA table\n\n\nRun via linear model\nlm() |&gt; anova()\nsame ANOVA table, but model object is lm\n\n\nTidy results\nbroom::tidy()\ntidy table of terms\n\n\nModel summary\nbroom::glance()\nincludes R², df, etc.",
    "crumbs": [
      "18. F this!",
      "•  18. F and anova in R"
    ]
  },
  {
    "objectID": "book_sections/f/Finding_connections.html",
    "href": "book_sections/f/Finding_connections.html",
    "title": "• 18. F-inding connecTions",
    "section": "",
    "text": "Motivating Scenario: In the previous chapter we compared two groups as a two sample t-test. In this chapter, we basically did the same thing as an ANOVA. You want to know the relationship between these approaches.\nLearning Goals: By the end of this subchapter, you should be able to:\n\nExplain why a two-sample t-test and a one-way ANOVA with two groups are mathematically equivalent.\nDescribe how t and F statistics are related and what each measures.\nConnect \\(r\\) (correlation) and \\(R^2\\) (proportion of variance explained).\n\nRecognize these equivalences as special cases of the general linear model framework.\n\n\n\n\nShould I conduct an ANOVA or t-test?\n\nThis question is commonly asked by students with data in hand, and even by professors on an exam to evaluate students’ understanding. The answer is – for a case with a binary explanatory variable it makes absolutely no difference.\nReassuringly whether conducting and ANOVA or a t-test on a continuous response and a binary explanatory variable we get the exact same p-value. This is reassuring, as we would like our answers to be robust to such an arbitrary choice. It’s up to you whether you prefer to present results in terms of the number of standard errors separating the groups, or as a ratio of mean squares.\nHere we go through some connections between these modeling approaches.\n\n\n\n\n\n\nNote\n\n\n\nThis section is fully optional I just think it’s cool.\n\n\n\nComparing F and t\nLet’s consider our example from this chapter – comparing the admixture proportion of pink and white flowers at Squirrel Mountain.\nA t-test provides the following results:\n\nt.test(admix_proportion ~ petal_color, data = clarkia_hz, var.equal=TRUE)\n\n\n    Two Sample t-test\n\ndata:  admix_proportion by petal_color\nt = 8.7486, df = 44, p-value = 3.486e-11\nalternative hypothesis: true difference in means between group pink and group white is not equal to 0\n95 percent confidence interval:\n 0.009949917 0.015906235\nsample estimates:\n mean in group pink mean in group white \n        0.018958210         0.006030134 \n\n\n\n\nlm(admix_proportion ~ petal_color, data = clarkia_hz) |&gt;\n  anova()\n\nAnalysis of Variance Table\n\nResponse: admix_proportion\n            Df    Sum Sq    Mean Sq F value    Pr(&gt;F)    \npetal_color  1 0.0018312 0.00183122  76.539 3.486e-11 ***\nResiduals   44 0.0010527 0.00002393                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe most obvious similarities here are:\n\nThe p-values are identical, and\n\\(\\text{df}_\\text{error}\\) for the ANOVA equals \\(\\text{df}\\) for the t-test. .\n\nBut wait, there’s more:\n\nAdditionally the F value (76.539), is simply the t-value (8.7486) squared.\n\nFinally, although R did not show us this, the Mean Squares error of 0.0000239 equals the pooled variance:\n\n\n# finding the pooled variance\nclarkia_hz |&gt;\n    filter(!is.na(petal_color))|&gt;\n    group_by(petal_color)|&gt;\n    mutate(mean_admix = mean(admix_proportion ))|&gt;\n    ungroup()|&gt;\n    mutate(deviation = admix_proportion - mean_admix) |&gt;\n    summarise(pooled_var = sum(deviation^2) / (n() - 2))\n\n# A tibble: 1 × 1\n  pooled_var\n       &lt;dbl&gt;\n1  0.0000239\n\n\n\nThis is all to say that the two sample t-test and an ANOVA with two groups are essentially identical. I hope this makes us feel good about moving on to more complex ANOVAs, and about linear models more broadly.\n\nWhy does \\(F = t^2\\)?\n\nt measures the difference between group means in standard error units (i.e., how many standard deviations of the sampling distribution separate the groups).\nF compares the variance between groups to the variance within groups, so it’s measured on the variance scale.\n\nBecause variance is just the square of a standard deviation, F is simply the square of the corresponding t when there are two groups.\n\n\n\n\\(R^2\\) is the square of \\(r\\)\nRemember that \\(R^2\\) is the “proportion of variance explained” (i.e. the proportion of variance in our response variable that we can pin on our explanatory variable).\n\n\n\\[R^2 = \\frac{\\text{SS}_\\text{model}}{\\text{SS}_\\text{total}}\\].\nWe can find \\(R^2\\) from the output of our linear model:\n\nlibrary(broom)\naov(admix_proportion ~ petal_color, data = clarkia_hz) |&gt;\n    glance()|&gt; \n    kable()\n\n\n\n\nlogLik\nAIC\nBIC\ndeviance\nnobs\nr.squared\n\n\n\n\n180.4843\n-354.9686\n-349.4827\n0.0010527\n46\n0.6349717\n\n\n\n\n\n\nYou also may remember that \\(r\\) is the correlation between variables. If we assign a numeric value of zero to the explanatory variable for one category (e.g., white flowers), and one for the other category (e.g., pink flowers), we can calculate a correlation (\\(r\\)). If we square this correlation \\(r\\) squared equals \\(R^2\\)!\n\n\n\\[r = \\frac{ \\text{cov}_{x,y} }{s_x \\times s_y}\\]\n\n\n\n\n\n\nThe spiderman meme for ANOVA / regression\n\n\n\n\nclarkia_hz |&gt; \n  filter(!is.na(petal_color))|&gt;\n  mutate(petal_color_num = ifelse(petal_color == \"white\", 0 ,1))|&gt;\n  summarise(r = cor(petal_color_num , admix_proportion),\n            r.squared = r^2)\n\n# A tibble: 1 × 2\n      r r.squared\n  &lt;dbl&gt;     &lt;dbl&gt;\n1 0.797     0.635",
    "crumbs": [
      "18. F this!",
      "•  18. F-inding connecTions"
    ]
  },
  {
    "objectID": "book_sections/f/f_summary.html",
    "href": "book_sections/f/f_summary.html",
    "title": "• 18. F (ANOVA) summary",
    "section": "",
    "text": "Chapter summary\nLinks to: Summary. Chatbot tutor. Questions. Glossary. R packages. R functions. More resources.\nEnjoy this “insomnia meme” about the ANOVA. From this video\nThe analysis of variance (ANOVA) provides a way to understand variation by breaking the variability attributable to our model, and variability uncounted for by our model. The F statistic compares these two sources of variation. Under the null hypothesis that the groups come from the same (statistical) population, the ratio of between to within group variation has an expected value of one. In the special case of two groups, ANOVA and the two-sample t-test are mathematically equivalent—both quantify how much separation exists between group means relative to expected sampling variation.",
    "crumbs": [
      "18. F this!",
      "• 18. F (ANOVA) summary"
    ]
  },
  {
    "objectID": "book_sections/f/f_summary.html#f_summary_chapter-summary",
    "href": "book_sections/f/f_summary.html#f_summary_chapter-summary",
    "title": "• 18. F (ANOVA) summary",
    "section": "",
    "text": "Chatbot tutor\n\nPlease interact with this custom chatbot (link here). I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you got what you needed from it.",
    "crumbs": [
      "18. F this!",
      "• 18. F (ANOVA) summary"
    ]
  },
  {
    "objectID": "book_sections/f/f_summary.html#t_summary_practice-questions",
    "href": "book_sections/f/f_summary.html#t_summary_practice-questions",
    "title": "• 18. F (ANOVA) summary",
    "section": "Practice Questions",
    "text": "Practice Questions\nTry these questions! By using the R environment you can work without leaving this “book”. I even pre-loaded all the packages you need!\n\nPart 1: Reflecting on this chapter\nIn this chapter we used the ANOVA to test the null hypothesis that pink and white parviflora flowers have the same admixture proportion at Sawmill Road.\n\nQ1) Under the null hypothesis that both groups come from the same population, the expected value of \\(F\\) is 01It depends on the degrees of freedom.\nQ2) If \\(F\\) is greater than the expected value under the null, (select all correct)\n\n The null is false The null is true We reject the null We fail to reject the null We may or may not reject the null depending on the degrees of freedom.\n\n.\nQ3) If \\(F\\) is less than the expected value under the null, (select all correct)\n\n The null is false The null is true We reject the null We fail to reject the null We may or may not reject the null depending on the degrees of freedom.\n\n.\nQ4) Our p-value was very small (much less than the traditional (= 0.05) threshold). This means… (select all correct):\n\n It would be incredibly unlikely to observe such an extreme F statistic if the null hypothesis were true Pollinators avoid white flowers The pink petal color has introgressed from xantiana\n\n.\n\n\n\nPart 2: Recognizing types of sums of squares (from code)\nConsider the code below for the next three questions:\n\nSS &lt;- data |&gt;\n  mutate(grand_mean = mean(response))|&gt;\n  group_by(explanatory)|&gt;\n  mutate(yhat = mean(response))|&gt;\n  ungroup()|&gt;\n  summarise(SS_A = sum((yhat - grand_mean)^2),\n            SS_B = sum((response - yhat)^2),\n            SS_C = sum((response - grand_mean)^2) )\n\n\nQ5) Consider the code above. Which calculates \\(\\text{SS}_\\text{error}\\)? SS_ASS_BSS_C\nQ6) Consider the code above. Which calculates \\(\\text{SS}_\\text{model}\\)? SS_ASS_BSS_C\nQ7) Consider the code above. Which calculates \\(\\text{SS}_\\text{total}\\)? SS_ASS_BSS_C\n\n\n\nPart 3: New analysis\nNow let’s look at the same question in a different location, Site 22. First let me process the data for you:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nProcessing data for S22 hybrid zone\nhz_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/zones_df_admix_weight_cutoff0.9_gps_dist2het_phenos_excl_GC.csv\"\n\n\nclarkia_s22_hz &lt;- read_csv(hz_link)|&gt; \n  rename(admix_proportion = `cutoff0.9`) |&gt;\n  filter(!is.na(admix_proportion), !is.na(petal_color), subsp==\"P\", site == \"S22\")|&gt;\n  clean_names() |&gt;\n  mutate(tmp = as.numeric(factor(petal_color)) + admix_proportion,\n         id = as.factor(id),\n         id = fct_reorder(id, tmp))|&gt;\n  select(admix_proportion, petal_color,id)\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nQ8) Use the area above to find the difference in the admixture proportion between pink- and white- flowered parviflora plants at Site 22. Please return a positive number (pink minus white) .\n\n\nCode\n\n\n#  Group the dataset by petal color, then calculate the mean admixture proportion for each group\nmean_admix_by_petal_color &lt;- clarkia_s22_hz |&gt; \n  group_by(petal_color) |&gt;                     # group data into pink vs white flowers\n  summarise(mean_admix = mean(admix_proportion))  # calculate mean admixture proportion per group\n\n\nmean_admix_by_petal_color #  View the resulting summary table\n\n# A tibble: 2 × 2\n  petal_color mean_admix\n  &lt;chr&gt;            &lt;dbl&gt;\n1 pink            0.0330\n2 white           0.0132\n\n#  Calculate the difference in mean admixture proportion between the two petal colors\n# `diff()` takes the second value minus the first value of `mean_admix`\n# (Order depends on how petal_color is sorted in the data!)\nmean_admix_by_petal_color |&gt; \n  summarise(diff_admix = diff(mean_admix))\n\n# A tibble: 1 × 1\n  diff_admix\n       &lt;dbl&gt;\n1    -0.0199\n\n\nThis is white minus pink, so be sure to enter the absolute value.\n\n\n\n\n\n\n\n\n\n\nFigure 1: Partitioning the contribution of petal color and error to admixture proportion of pink and white parviflora plants in the S22 hybrid zone.\n\n\n\n\n\n\nQ9) Consider the plots above. Which has lines showing the “error” deviation? ABC\nQ10) Consider the plots above. Which has lines showing the “model” deviation? ABC\nQ11) Consider the plots above. Which has lines showing the “total” deviation? ABC\n\n\nNow let’s run the model!\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ12) Given the p-value, what do you do to the null hypothesis?\n\n Reject it Fail to reject it Accept it Fail to accept it\n\nQ13) This means the null\n\n Is true Is false Has a 0.00805 chance of being true We can’t say for sure.\n\nQ14) From this output, the \\(R^2\\) value is: .\n\n\nHelp\n\nRecall:\n\n\\(R^2 = \\frac{\\text{SS}_\\text{model}}{\\text{SS}_\\text{total}}\\)\n\n\\(\\text{SS}_\\text{total} = \\text{SS}_\\text{model} +\\text{SS}_\\text{error}\\)\n\n\nQ15) From this output and your answer to question eight (“What’s the difference in mean admixture proportion by petal color”), Cohen’s D is\n\n Not worth reporting (&lt; 0.01) Tiny (0.01 – 0.20) Small (0.20 – 0.50) Medium (0.50 – 0.80) Large (0.80 – 1.20) Very large (1.20 – 2.00) Huge (&gt; 2.00)\n\n.\n\n\nEquation for Cohen’s D\n\n\\[\\text{Cohen's D} = \\frac{\\text{Difference in group means}}{\\text{pooled standard deviation}}\\]\n\n\n\nFinding the pooled standard deviation\n\nWith just two samples the mean squared error equals the pooled variance. So the square root of this is the pooled standard deviation.\n\n\n\nMath\n\n\\[D = \\frac{0.0199}{\\sqrt{0.00031557}}\\]\n\\[D = \\frac{0.0199}{0.01776}\\]\n\\[D = 1.12\\]\n\n\nhttps://bookdown.org/steve_midway/DAR/understanding-anova-in-r.html\nhttps://www.crumplab.com/rstatsforpsych/semester-project-2021.html",
    "crumbs": [
      "18. F this!",
      "• 18. F (ANOVA) summary"
    ]
  },
  {
    "objectID": "book_sections/f/f_summary.html#f_glossary",
    "href": "book_sections/f/f_summary.html#f_glossary",
    "title": "• 18. F (ANOVA) summary",
    "section": "📊 Glossary of Terms",
    "text": "📊 Glossary of Terms\n\n\n\n📚 1. Core Concepts\n\nAnalysis of Variance (ANOVA): A statistical framework that partitions total variability in a dataset into variability explained by a model (between groups) and variability left unexplained (within groups).\nPartitioning Variance: Decomposing total variability into distinct components (e.g., model vs. error) to understand sources of variation.\nNull Hypothesis (\\(H_0\\)): The assumption that all groups are samples from the same statistical population; under H₀, the expected value of the F statistic is 1.\nAlternative Hypothesis (\\(H_1\\)): The assumption that at least one group comes from a different population, resulting in more between-group variation than expected by chance.\n\n\n\n\n🔢 2. Variance Components\n\nBetween-Group Variability: Variation among group means, attributed to the model (e.g., differences between pink vs. white flowers in admixture proportion).\nWithin-Group Variability: Variation among individuals within groups, reflecting background noise and sampling error.\nTotal Variability: The sum of between-group and within-group variability.\n\n\n\n\n🧮 3. Sums of Squares, Mean Squares, and their ratio (\\(F\\))\n\nSum of Squares (SS): A measure of variability; calculated by summing squared deviations.\n\n\\(\\text{SS}_\\text{model}\\) (aka \\(\\text{SS}_\\text{groups}\\))): Variability explained by our explanatory variable(s).\n\n\\(\\text{SS}_\\text{error}\\) (aka \\(\\text{SS}_\\text{residual}\\))): Variability within groups, not explained by the model.\n\\(\\text{SS}_\\text{total}\\): Total variability in the response variable.\n\nDegrees of Freedom (df): The number of independent pieces of information used to estimate variability.\n\n$_ = \\(n_\\text{groups}-1\\).\n\n$_ = \\(n - n_\\text{groups}\\).\n\n\nMean Square (MS): An estimate of variance obtained by dividing SS by its degrees of freedom.\n\n\\(\\text{MS}_\\text{model}\\) =\\(\\frac{\\text{MS}_\\text{model}}{\\text{df}_\\text{model}}\\)\n\n\\(\\text{MS}_\\text{error}\\) =\\(\\frac{\\text{SS}_\\text{error}}{\\text{df}_\\text{error}}\\)\n\nF Statistic: A ratio of variances that compares between-group variability (\\(\\text{MS}_\\text{model}\\)) to within-group variability (\\(\\text{MS}_\\text{error}\\)). \\[F = \\frac{\\text{MS}_\\text{model}}{\\text{MS}_\\text{error}}\\]",
    "crumbs": [
      "18. F this!",
      "• 18. F (ANOVA) summary"
    ]
  },
  {
    "objectID": "book_sections/f/f_summary.html#f_summaryR_packages-introduced",
    "href": "book_sections/f/f_summary.html#f_summaryR_packages-introduced",
    "title": "• 18. F (ANOVA) summary",
    "section": "R Packages Introduced",
    "text": "R Packages Introduced\n\nThis chapter relies on packages you’ve already encountered, including:\n\nbroom: Tidies model outputs (like fitted values and residuals) into neat data frames.\nggplot2: Used for visualizing data and model fits.",
    "crumbs": [
      "18. F this!",
      "• 18. F (ANOVA) summary"
    ]
  },
  {
    "objectID": "book_sections/f/f_summary.html#f_summary_key-r-functions",
    "href": "book_sections/f/f_summary.html#f_summary_key-r-functions",
    "title": "• 18. F (ANOVA) summary",
    "section": "🛠️ Key R Functions",
    "text": "🛠️ Key R Functions\n\n\naov(): Fits an analysis of variance model directly.\nsummary(): Use summary() on the resulting object to view the ANOVA table, including sums of squares, mean squares, F statistic, and p-value.\n\n\naov(response ~ group, data = dataset)|&gt;\n  summary()\n\n\n\nanova(): Produces an ANOVA table from a fitted linear model.\n\n\nlm(response ~ group, data = dataset) |&gt; \n    anova()\n\nThis approach is mathematically identical to aov() for one-way ANOVA, but uses a linear model object as input.\n\n\nlm(): Fits a linear model.\n\n\nlm(response ~ group, data = dataset)\n\nOne-way ANOVA is just a special case of a linear model with a categorical predictor. This makes it easy to extend to more complex designs later.\n\n\ntidy() and glance() (from the broom package):\n\ntidy() formats the ANOVA table into a tidy data frame.\nglance() returns model-level summaries, including ( R^2 ).\n\n\n\naov(response ~ group, data = dataset) |&gt; \n  broom::glance()",
    "crumbs": [
      "18. F this!",
      "• 18. F (ANOVA) summary"
    ]
  },
  {
    "objectID": "book_sections/f/f_summary.html#t_summary_additional-resources",
    "href": "book_sections/f/f_summary.html#t_summary_additional-resources",
    "title": "• 18. F (ANOVA) summary",
    "section": "Additional resources",
    "text": "Additional resources\n\nBook chapters:\nChapter 7: Understanding the ANOVA in R from “Data Analysis in R”\nVideos:\nKahn Academy ANOVA\nPartitioning Variance For ANOVA and linear modelling",
    "crumbs": [
      "18. F this!",
      "• 18. F (ANOVA) summary"
    ]
  },
  {
    "objectID": "book_sections/anova.html",
    "href": "book_sections/anova.html",
    "title": "19. >2 Categories",
    "section": "",
    "text": "Example data\nWe have considered both the two-sample t-test, and the variance partitioning approach, to test the null hypothesis that group means do not differ. We saw that these approaches give the same answers when analyzing a continuous response variable with a binary explanatory variable. Here, we consider what to do when our explanatory variable has more than two “levels”.\nWe will see that testing for a difference between each group mean in a pairwise manner introduces a “multiple testing problem.” We show that the ANOVA approach allows us to overcome this multiple testing problem by testing null hypotheses that the true group means are identical (i.e. there is no variance among groups in our population). We then show how you can use the “post-hoc” testing framework to adjust your t-values to incorporate the fact that you are testing multiple hypotheses after you found that not all groups means are equal.\nHere we revisit our data from the last section – parviflora plants collected from natural hybrid zone. Here we will look at the variability in petal area among parviflora populations. The is potentially interesting because ecological features and the admixture proportion differ across sites, and it’s worth noting if this is associated with any differences in key phenotypes in the field! The data are plotted below:\nCode for plotting\nlibrary(ggforce)\nggplot(clarkia_hz , aes(x = site, y =mean_petal_area_sq_cm, color = site))+\n    geom_violin()+\n    geom_sina(alpha = .4, size = 2)+\n    stat_summary(fun.data = \"mean_cl_normal\",geom = \"errorbar\", color = \"black\",width = .15)+\n    labs(y = expression(Mean~petal~area~(cm^2)))+\n    theme(legend.position = \"none\", \n          axis.text = element_text(size = 12), \n          axis.title = element_text(size = 12))",
    "crumbs": [
      "19. >2 Categories"
    ]
  },
  {
    "objectID": "book_sections/anova.html#lets-goooooo",
    "href": "book_sections/anova.html#lets-goooooo",
    "title": "19. >2 Categories",
    "section": "Let’s goooooo",
    "text": "Let’s goooooo\nHow to anlyze these data. As we will see soon, doing a bunch of t-tests is not the answer, but an ANOVA, foloowed by post-hooc tests is! Let’s see why this is, and how to do it.",
    "crumbs": [
      "19. >2 Categories"
    ]
  },
  {
    "objectID": "book_sections/anova/multiple_testing.html",
    "href": "book_sections/anova/multiple_testing.html",
    "title": "• 19. Multiple testing problem",
    "section": "",
    "text": "Multiple tests make a liar of your p-value\nThere are \\({4 \\choose 2} = 6\\) possible pairwise comparisons of mean petal areas among the four Clarkia xantiana parviflora hybrid zone populations we studied (Figure 1). Even when all six nulls are true, there’s roughly a one in four chance that at least one test will falsely appear ‘significant.’ That’s because the probability all six avoid a false positive is \\(0.95^6 =  0.735\\).\nThus for this study our overall \\(\\alpha\\) would be 1- 0.735 = 0.265, a value much larger than the \\(\\alpha = 0.05\\) that was advertised. This problem gets pretty bad pretty quick (Figure 2). As such, conducting many t-tests on the same data makes your p-values misleading—they no longer represent the 5% false-positive rate we usually assume. When you run multiple tests, the chance of seeing at least one ‘significant’ result just by luck is much higher, so the reported p-values give a false sense of confidence\ncomparisons &lt;- tibble(n_groups= 2:15)|&gt;\n    mutate(n_comparisons = choose(n_groups,2),\n           experiment_alpha = 1-.95^n_comparisons)\n\nggplot(comparisons, aes(x = n_groups,  y =experiment_alpha))+\n    geom_point(size= 4)+\n    geom_line(linetype = 3, linewidth = 1.4)+\n    labs(x = \"# groups\",\n         y = \"P(≥ 1 false positive)\", \n         title =\"The multiple testing problem\")+\n    theme(axis.text = element_text(size = 23),\n          title = element_text(size = 23),\n          axis.title = element_text(size = 23))+\n    scale_x_continuous(breaks = seq(2,14,2))\n\n\n\n\n\n\n\n\nFigure 2: The probability of rejecting at least one true null hypothesis at the nominal α = 0.05 level when conducting all pairwise comparisons. With ten groups we have 45 pairwise comparisons and true experiment-wide α = 0.90.",
    "crumbs": [
      "19. >2 Categories",
      "•  19. Multiple testing problem"
    ]
  },
  {
    "objectID": "book_sections/anova/multiple_testing.html#multiple-tests-make-a-liar-of-your-p-value",
    "href": "book_sections/anova/multiple_testing.html#multiple-tests-make-a-liar-of-your-p-value",
    "title": "• 19. Multiple testing problem",
    "section": "",
    "text": "More broadly, the number of pairwise comparisons between n groups equals\n\\(n_\\text{pairs} = \\binom{n}{2} = \\frac{n (n-1)}{2}\\), and the experiment-wise false positive rate equals, \\(1-(1-\\alpha)^{n_\\text{pairs}}\\).",
    "crumbs": [
      "19. >2 Categories",
      "•  19. Multiple testing problem"
    ]
  },
  {
    "objectID": "book_sections/anova/multiple_testing.html#anova-can-solve-the-multiple-testing-problem",
    "href": "book_sections/anova/multiple_testing.html#anova-can-solve-the-multiple-testing-problem",
    "title": "• 19. Multiple testing problem",
    "section": "ANOVA can solve the multiple testing problem",
    "text": "ANOVA can solve the multiple testing problem\nFor p-values to be worth anything, they should correspond to the problem we set up. There are numerous ways to address the multiple testing problem (see below, and Wikipedia).\nInstead of testing each combination of groups separately, ANOVA poses and tests a single null hypothesis — that all samples come from the same statistical population. This results in a well-calibrated null model (i.e. we will reject a true null with proability \\(\\alpha\\)).\nANOVA hypotheses\n\n\\(H_0\\): All samples come from the same (statistical) population. Practically, this says that all groups have the same mean.\n\n\\(H_A\\): Not all samples come from the same (statistical) population. Practically this says that not all groups have the same mean.\n\n\nBut how do we see which groups differ? Our scientific hypotheses and interpretations depend not just on the single null hypothesis – “all groups are equal”– but on knowing which groups differ from one another. Later in this chapter we will introduce “post-hoc tests”, which - in combination with an ANOVA test which pairs of groups differ from one another.\nThat is, ANOVA shows whether groups differ; post-hoc tests show which groups differ.\n\n\n\n\n\n\n\nxkcd’s classic description of the multiple testing problem (and the related communication and hype cycle). The original rollover text said: ‘So, uh, we did the green study again and got no link. It was probably a–’ ‘RESEARCH CONFLICTED ON GREEN JELLY BEAN/ACNE LINK; MORE STUDY RECOMMENDED!’ For more discussion see the associated explain xkcd.",
    "crumbs": [
      "19. >2 Categories",
      "•  19. Multiple testing problem"
    ]
  },
  {
    "objectID": "book_sections/anova/multiple_testing.html#other-ways-to-handle-multiple-comparisons",
    "href": "book_sections/anova/multiple_testing.html#other-ways-to-handle-multiple-comparisons",
    "title": "• 19. Multiple testing problem",
    "section": "Other ways to handle multiple comparisons",
    "text": "Other ways to handle multiple comparisons\nANOVA solves the multiple-testing problem by asking one big question instead of many small ones. But sometimes we really do need to test many hypotheses e.g., when comparing every pair of groups, analyzing many traits, or in genome wide association studies etc. So there are other approaches to deal with this issue.\nThe Bonferroni correction is the simplest correction for multiple tests. A Bonferroni correction creates a new \\(\\alpha\\) threshold by dividing your stated \\(\\alpha\\) by the number of tests. So, if you test five different nulls at an \\(\\alpha = 0.05\\), you apply the the Bonferroni by only rejecting the null when \\(p&lt;0.01\\).\nAs the number of comparisons increases this correction becomes overly conservative, so people turn to other methods.\nThe False Discovery Rate (FDR) Rather than considering any false positive, FDR based methods consider the expected proportion of false positives among the results we call significant. So FDR based corrections ensures that on average, about 5% of the results we call ‘significant’ are actually false positives.",
    "crumbs": [
      "19. >2 Categories",
      "•  19. Multiple testing problem"
    ]
  },
  {
    "objectID": "book_sections/anova/anova_lm.html",
    "href": "book_sections/anova/anova_lm.html",
    "title": "• 19. ANOVA: a linear model",
    "section": "",
    "text": "Residuals\nRecall that “linear models” are “linear” because we find an individual’s predicted value \\(\\hat{Y_i}\\) by adding up predictions from each component of the model. So, for example, \\(\\hat{Y_i}\\) equals the parameter estimate for the “intercept”, \\(a\\), plus its value for the first explanatory variable, \\(y_{1,i}\\), times the effect of this variable, \\(b_1\\), plus its value for the second explanatory variable, \\(y_{2,i}\\) times its effect, \\(b_2\\), etc.\n\\[\\hat{Y}_i = a + b_1 y_{1,i} + b_2 y_{2,i} + \\dots{}\\]\nJust like a one or two sample t-test, the ANOVA is a linear model. As such, it models group means as a function of group membership. This works by picking one group as the “reference level” and having that group’s mean represented as the intercept. Then each “slope” \\(b\\) is the difference between that group’s mean and the reference group’s mean, and the corresponding \\(y_i\\)s are one if the individual is in that group and zero otherwise.\nWe cast this as a linear model as follows: \\(\\widehat{\\text{admix prop}}_i = 0.0071 + 0.0183 \\times \\text{S22}_{i}  + 0.0025 \\times \\text{SM}_{i}  +  0.0221  \\times \\text{S26}_{i}\\).\nHere SR is the “reference level”, and as such, SR’s mean admixture proportion equals the intercept, 0.0071. Similarly, we estimate an admixture proportion of 0.0183 + 0.0071 =  0.0254 at site S22.\nOf course, individual observations deviate from their predicted value! As you know by now the difference between an observation, \\(Y_i\\) and its predicted value, \\(\\hat{Y_i}\\) is called the residual, \\(\\epsilon_i\\). Figure 1 shows the residuals as the length of the lines connecting individual observations to group means.\nFigure 1\nRecall that the augment() function in the broom package shows \\(\\hat{Y_i}\\) as the .fitted column, and the residuals as the .resid column:\nlibrary(broom)\naugment(hybrid_anova)\nadmix_proportion\nsite\n.fitted\n.resid\n\n\n\n\n0.0265\nS22\n0.0254\n0.0011\n\n\n0.0329\nS22\n0.0254\n0.0075\n\n\n0.0895\nS22\n0.0254\n0.0641\n\n\n0.0042\nS22\n0.0254\n-0.0212\n\n\n0.0041\nS22\n0.0254\n-0.0212\n\n\n0.0033\nS22\n0.0254\n-0.0221",
    "crumbs": [
      "19. >2 Categories",
      "•  19. ANOVA: a  linear model"
    ]
  },
  {
    "objectID": "book_sections/anova/anova_lm.html#residuals",
    "href": "book_sections/anova/anova_lm.html#residuals",
    "title": "• 19. ANOVA: a linear model",
    "section": "",
    "text": "Mathematical descriptions of residuals \\[\\hat{Y_i}=Y_i+\\epsilon_i\\] \\[Y_i=\\hat{Y_i}-\\epsilon_i\\] \\[\\epsilon_i= Y_i-\\hat{Y_i}\\]\n\n\n\n\n\n\nChanging reference levels Alphabetical order is not always the “right” order. For example, in this data set of Daphnia resistance to a poisonous cyanobacteria across years with high, medium and low concentrations of cyanobacteria.\n\ndaphnia_data &lt;- read_csv(\"https://whitlockschluter3e.zoology.ubc.ca/Data/chapter15/chap15q17DaphniaResistance.csv\")\n\nlm(resistance ~ cyandensity, data = daphnia_data)\n\n\nCall:\nlm(formula = resistance ~ cyandensity, data = daphnia_data)\n\nCoefficients:\n   (Intercept)  cyandensitylow  cyandensitymed  \n        0.8000         -0.1167         -0.0170  \n\n\nThe code below uses the fct_relevel() function in the forcats package to change the order of our factors, so the reference level makes more sense.\n\nlibrary(forcats)\ndaphnia_data &lt;- daphnia_data |&gt;\n  mutate(cyandensity =fct_relevel(cyandensity, \"low\",\"med\",\"high\"))\n\nlm(resistance ~ cyandensity, data = daphnia_data)\n\n\nCall:\nlm(formula = resistance ~ cyandensity, data = daphnia_data)\n\nCoefficients:\n    (Intercept)   cyandensitymed  cyandensityhigh  \n        0.68333          0.09967          0.11667",
    "crumbs": [
      "19. >2 Categories",
      "•  19. ANOVA: a  linear model"
    ]
  },
  {
    "objectID": "book_sections/anova/anova_assumptions.html",
    "href": "book_sections/anova/anova_assumptions.html",
    "title": "• 19. ANOVA assumptions",
    "section": "",
    "text": "ANOVA Assumes…\nThe ANOVA shares a core set of assumptions with most standard linear models. The assumptions are:\nLike the two-sample t-test, ANOVA assumes:\nHere we use the parviflora admixture data to look into these assumptions",
    "crumbs": [
      "19. >2 Categories",
      "•  19. ANOVA assumptions"
    ]
  },
  {
    "objectID": "book_sections/anova/anova_assumptions.html#what-to-do-when-you-violate-assumptions",
    "href": "book_sections/anova/anova_assumptions.html#what-to-do-when-you-violate-assumptions",
    "title": "• 19. ANOVA assumptions",
    "section": "What to do when you violate assumptions",
    "text": "What to do when you violate assumptions\nWe’ll continue using these data to learn how ANOVA works. But when the assumptions are clearly broken, as in this example, we need to think about what to do. I wan to emphasize that there’s usually not a single right answer, and that your choice will be a compromise between trade-offs of each potential approach. Below are some options, along with my (strong) opinions. I also note that you don’t need to pick a single analysis, sometimes it makes sense to present results from different types of analyses to show that your take-home message is robust to any specific assumptions.\nMost importantly, think about what you’re doing and why. While you should not run a statistical test that gives meaningless results, there’s usually a trade-off between statistical purity and clear communication of your results. So don’t rush to the most statistical pure approach if it means your results are difficult to interpret.\nIf you’re interested in NHST, breaking assumptions means that you p-values aren’t properly calibrated, so you cannot literally interpret your p-value. But how often do you care about the exact p-value? If you’re worried about misleading confidence intervals, consider the trade-offs between having these perfectly calibrated, and loss of information that may occur when presenting results on a scale that is difficult to interpret. Let clarity about your goals and the meaning of your results take precedence over strict rule following.\n\nMinor violations: If the test is known to be robust to small deviations—like mild non-normality or modest differences in variance—you can usually proceed without concern.\nMajor non-normality: When data are far from normal within groups, consider transforming your data (e.g., log, square root) or using a model that explicitly captures the shape of the distribution that generated your data. We’ll return to this idea later when we discuss generalized linear models.\nRobust approaches: Methods like Welch’s ANOVA (which allows group variances to differ, below), or approaches that “trim” extreme outliers, are generally solid and defensible choices.\nPermutation tests: While tempting, permutation-based ANOVAs often introduce new violations - particularly non-independence and can make variance unequal between groups - so interpreting these results is difficult.\nRank-based tests: Alternatives like the Kruskal–Wallis test assess differences in medians rather than means and are less sensitive to assumption violations. However, their effect sizes are harder to interpret in practical or biological terms.",
    "crumbs": [
      "19. >2 Categories",
      "•  19. ANOVA assumptions"
    ]
  },
  {
    "objectID": "book_sections/anova/anova_assumptions.html#welchs-anova-when-variance-differs",
    "href": "book_sections/anova/anova_assumptions.html#welchs-anova-when-variance-differs",
    "title": "• 19. ANOVA assumptions",
    "section": "Welch’s ANOVA when variance differs",
    "text": "Welch’s ANOVA when variance differs\nYou can use a Welch’s ANOVA when variance differs substantially among groups. Welch’s ANOVA has the same underlying logic as a standard ANOVA, but gives more weight to groups whose means from samples with smaller variance. This means that F and the degrees of freedom etc can be interpreted similarly as in a standard ANOVA, but are calculated differently.\nThe oneway.test() function in R uses a Welch’s ANOVA by default:\n\noneway.test(admix_proportion ~ site, data = clarkia_hz)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  admix_proportion and site\nF = 32.224, num df = 3.00, denom df = 52.34, p-value = 6.117e-12\n\n\nHere, we resoundingly reject the null hypothesis that between-group variability equals within-group variability.",
    "crumbs": [
      "19. >2 Categories",
      "•  19. ANOVA assumptions"
    ]
  },
  {
    "objectID": "book_sections/anova/anova_example.html",
    "href": "book_sections/anova/anova_example.html",
    "title": "• 19. ANOVA Example",
    "section": "",
    "text": "ANOVA in R\nWe previously saw how to partition variance components to calculate the relevant sums of squares needed to conduct an ANOVA and estimate the proportion of variance explained by our model. Recall that we found:\nTogether these sum to equal\nWe can use the logic illustrated in Figure 1 to calculate the relevant sums of squares, and the proportion variance explained by the model, \\(r^2\\):\nKnowing that \\(\\text{DF}_\\text{model} = n_\\text{groups}-1\\), and \\(\\text{DF}_\\text{error} = n- n_\\text{groups}\\), we can find mean squares, \\(F\\) and our p-value:\nOf course R can do these calculations for us, with either the aov() |&gt; summary() pipeline, or the lm() |&gt; anova() pipeline. both approaches yield the same result as our manual calculation above.",
    "crumbs": [
      "19. >2 Categories",
      "•  19. ANOVA Example"
    ]
  },
  {
    "objectID": "book_sections/anova/anova_example.html#anova-in-r",
    "href": "book_sections/anova/anova_example.html#anova-in-r",
    "title": "• 19. ANOVA Example",
    "section": "",
    "text": "aov() |&gt; summary()\n\naov(admix_proportion ~ site, data = clarkia_hz)\n\nCall:\n   aov(formula = admix_proportion ~ site, data = clarkia_hz)\n\nTerms:\n                      site  Residuals\nSum of Squares  0.01421379 0.01885515\nDeg. of Freedom          3        208\n\nResidual standard error: 0.009521017\nEstimated effects may be unbalanced\n\n\n\naov(admix_proportion ~ site, data = clarkia_hz)|&gt;\n  summary()\n\n             Df  Sum Sq  Mean Sq F value Pr(&gt;F)    \nsite          3 0.01421 0.004738   52.27 &lt;2e-16 ***\nResiduals   208 0.01886 0.000091                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nlm() |&gt; anova()\n\nlm(admix_proportion ~ site, data = clarkia_hz)\n\n\nCall:\nlm(formula = admix_proportion ~ site, data = clarkia_hz)\n\nCoefficients:\n(Intercept)      siteS22       siteSM       siteS6  \n   0.007094     0.018282     0.002536     0.022087  \n\n\n\nlm(admix_proportion ~ site, data = clarkia_hz)|&gt;\n  anova()\n\nAnalysis of Variance Table\n\nResponse: admix_proportion\n           Df   Sum Sq   Mean Sq F value    Pr(&gt;F)    \nsite        3 0.014214 0.0047379  52.266 &lt; 2.2e-16 ***\nResiduals 208 0.018855 0.0000906                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "19. >2 Categories",
      "•  19. ANOVA Example"
    ]
  },
  {
    "objectID": "book_sections/anova/post_hoc.html",
    "href": "book_sections/anova/post_hoc.html",
    "title": "• 19. Post hoc tests",
    "section": "",
    "text": "Motivation\nANOVA tells us that at least one group differs, but not which one. So, if you failed to reject the null, you’re done! But, if you did reject the null, this rejection might feel a bit hollow. You know that not all groups are the same, but you don’t know which groups differ from one another, so what’s the point even?\nHere, we introduce post-hoc tests. Post hoc is Latin for “after this”. So, after you reject the null hypothesis that all groups come from the same population, you can run a post hoc test to see which groups differ from one another. Post hoc tests are much like pairwise t-tests, with slight modifications to control for false positives from multiple comparisons and to account for the fact that we’ve already rejected the overall null hypothesis that all groups are the same.",
    "crumbs": [
      "19. >2 Categories",
      "•  19. Post hoc tests"
    ]
  },
  {
    "objectID": "book_sections/anova/post_hoc.html#motivation",
    "href": "book_sections/anova/post_hoc.html#motivation",
    "title": "• 19. Post hoc tests",
    "section": "",
    "text": "Do not run a post-hoc test if you fail to reject the null!",
    "crumbs": [
      "19. >2 Categories",
      "•  19. Post hoc tests"
    ]
  },
  {
    "objectID": "book_sections/anova/post_hoc.html#p-values-from-lm-summary-are-wrong",
    "href": "book_sections/anova/post_hoc.html#p-values-from-lm-summary-are-wrong",
    "title": "• 19. Post hoc tests",
    "section": "P-values from lm() |> summary() are wrong",
    "text": "P-values from lm() |&gt; summary() are wrong\nYou might think “Hey, I know how to find p-values associated with specific categories, I can just pipe the lm() output into summary().” But you would be wrong.\n\nlm(admix_proportion ~ site, data = clarkia_hz)|&gt;\n    summary()\n\n\n\n\n\n\n\n\n\n\n\nThe outputs of this pipeline below is not fully meaningless, but are often misleading and should be ignored for multi-group comparisons. Hopefully the rows are familiar by now:\n\nEstimate: The estimated coefficient for that term in the model. For (Intercept), it’s the mean of the reference group. For the other groups its the deviation from this mean.\n\nStd. Error: Is the uncertainty in this estimated value.\n\nt-value: Is the distance between our estimated value and 0 in units of standard errors (i.e. Estimate / Std. Error).\n\nPr(&gt;|t|): Is the p-value: 2 * pt(abs(t), df, lower.tail = FALSE).\n\nThe estimate and standard error are usually interesting and meaningful. However, the t-value and p-value from the lm() |&gt; summary() pipeline are usually useless and confusing.\n\nFor the (Intercept) row, these relate to the null hypothesis that this value equals zero. In our case this is clearly stupid – the admixture proportion cannot be negative, so any nonzero value means we know the mean is not zero.\nthe t- and p-values test whether each category differs from the reference level. While these hypotheses may be interesting, these summaries should still be ignored because:\n\nThe t-value is wrong because it uses the total error degrees of freedom. This is wrong because in ANOVA, we partition variance among groups, so the residual df used here aren’t appropriate for multiple comparisons.\nThe p-value is wrong because it comes from a wrong t-value, and because it does not acknowledge the multiple comparisons or the fact that we already reject the null from the ANOVA.\n\n\nAdditionally, this output does not allow us to test differences between two non-reference categories.",
    "crumbs": [
      "19. >2 Categories",
      "•  19. Post hoc tests"
    ]
  },
  {
    "objectID": "book_sections/anova/post_hoc.html#post-hoc-tests",
    "href": "book_sections/anova/post_hoc.html#post-hoc-tests",
    "title": "• 19. Post hoc tests",
    "section": "Post-hoc tests",
    "text": "Post-hoc tests\nThere are two flavors of post-hoc tests.\nIn “planned comparisons” we are only interested in a small subset of the potential pairwise comparisons. In these cases, we limit our analysis to specific differences that are interesting for biological reasons, and ignore most of the potential comparisons. Planned comparisons provide more power, because we limit our attention to a small number of tests.\n\n\nI will not focus planned comparisons. See this resource for more information.\nIn “unplanned comparisons” we are interested in all potential pairwise differences. Post-hoc test overcome the multiple comparisons problem by building a sampling distribution that conditions on there being at least one difference and numerous pairwise tests. Below we introduce numerous ways how to conduct unplanned post-hoc tests in R.\n\n\nThe Tukey-Kramer method’s is one common post hoc test. Its test statistic. \\(q\\) measures the distance bewtween group means in standard error units.\n\\(q= \\frac{Y_i-Y_j}{SE}\\), where \\(\\text{SE} = \\sqrt{\\text{MS}_\\text{error}(\\frac{1}{n_i}+ \\frac{1}{n_j})}\\).\n\nFrom aov()\nIf we built our ANOVA with the aov() function, piping this output to the TukeyHSD() conducts this post-hoc test for you. You can see from the example below that we resoundingly reject the null that means are equal for all pairwise comparisons except SR vs SM, and S6 vs S22.\n\naov(admix_proportion ~ site, data = clarkia_hz)|&gt; \n  TukeyHSD()\n\n\n\n\n\n\ncomparison\ndiff\nlwr\nupr\np.adj\n\n\n\n\nS22-SR\n0.0183\n0.0131\n0.0235\n0.0000\n\n\nSM-SR\n0.0025\n-0.0014\n0.0064\n0.3378\n\n\nS6-SR\n0.0221\n0.0162\n0.0280\n0.0000\n\n\nSM-S22\n-0.0157\n-0.0211\n-0.0104\n0.0000\n\n\nS6-S22\n0.0038\n-0.0031\n0.0107\n0.4859\n\n\nS6-SM\n0.0196\n0.0136\n0.0255\n0.0000\n\n\n\n\n\n\n\n\nFrom lm()\nIf we built our anova as a linear model we can conduct a similar post-hoc test the glht() function in the multicomp package. This implementation of the Tukey test is slightly different from how it’s done in TukeyHSD(), so our p-values are slightly different.\n\n\nWe will see later that this approach can be generalized to more complex linear models.\n\nlibrary(multcomp)\nlm(admix_proportion ~ site, data = clarkia_hz)|&gt;\n    glht(linfct = mcp(site = \"Tukey\"))|&gt;\n    summary()\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lm(formula = admix_proportion ~ site, data = clarkia_hz)\n\nLinear Hypotheses:\n               Estimate Std. Error t value Pr(&gt;|t|)    \nS22 - SR == 0  0.018282   0.002016   9.069   &lt;0.001 ***\nSM - SR == 0   0.002536   0.001511   1.678    0.329    \nS6 - SR == 0   0.022087   0.002272   9.721   &lt;0.001 ***\nSM - S22 == 0 -0.015746   0.002065  -7.626   &lt;0.001 ***\nS6 - S22 == 0  0.003805   0.002672   1.424    0.477    \nS6 - SM == 0   0.019551   0.002316   8.443   &lt;0.001 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)",
    "crumbs": [
      "19. >2 Categories",
      "•  19. Post hoc tests"
    ]
  },
  {
    "objectID": "book_sections/anova/post_hoc.html#from-welchs-anova",
    "href": "book_sections/anova/post_hoc.html#from-welchs-anova",
    "title": "• 19. Post hoc tests",
    "section": "From Welch’s ANOVA",
    "text": "From Welch’s ANOVA\nThe Games Howell test, available in the rstatix package (as games_howell_test()), is a post-hoc test which does not assume equal variance among groups. You can see that our main conclusions hold, but that we have slightly more power and can now reject the null that SR and SM have the same admixture proportion.\nHowever I worry that this power is unearned, as a I fear the low variance in sites SR and SM may reflect non-independence of data at each site.\n\nlibrary(rstatix)\ngames_howell_test(admix_proportion ~ site, data = clarkia_hz)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.y.\ngroup1\ngroup2\nestimate\nconf.low\nconf.high\np.adj\np.adj.signif\n\n\n\n\nadmix_proportion\nSR\nS22\n0.0183\n0.0083\n0.0283\n0.0001\n***\n\n\nadmix_proportion\nSR\nSM\n0.0025\n0.0003\n0.0048\n0.0180\n*\n\n\nadmix_proportion\nSR\nS6\n0.0221\n0.0145\n0.0297\n0.0000\n****\n\n\nadmix_proportion\nS22\nSM\n-0.0157\n-0.0259\n-0.0056\n0.0010\n***\n\n\nadmix_proportion\nS22\nS6\n0.0038\n-0.0083\n0.0159\n0.8380\nns\n\n\nadmix_proportion\nSM\nS6\n0.0196\n0.0117\n0.0274\n0.0000\n****",
    "crumbs": [
      "19. >2 Categories",
      "•  19. Post hoc tests"
    ]
  },
  {
    "objectID": "book_sections/anova/sig_groups.html",
    "href": "book_sections/anova/sig_groups.html",
    "title": "• 19. Significance groups",
    "section": "",
    "text": "Tricky cases\nSo we now know which pairs differ from one another! But, there is a real mental burden in trying to make sense of results from many pairwise tests. Defining “significance groups” helps us present results of a post-hoc test. Here, we assign the same letter to groups that do not significantly differ, and different letters to groups that do. In our case we see that:\nThese letter-based summaries are handy for communicating complex post-hoc results. But remember, they are shorthand for results of your post-hoc tests, not new analyses. We can communicate these groups by adding this information to a plot:\ncomparison\nsignificant\n\n\n\n\nX-Y\nTRUE\n\n\nX-Z\nFALSE\n\n\nY-Z\nFALSE\nConsider the hypothetical (and not uncommon) outcome of a post-hoc test, displayed in the right margin. Here:\nWhen you see a category with two letters (like ‘a,b’), it means the sample is not significantly different from categories in group a and in group b, but all categories uniquely assigned to a are significantly different from all categories uniquely assigned to group group b.",
    "crumbs": [
      "19. >2 Categories",
      "•  19. Significance groups"
    ]
  },
  {
    "objectID": "book_sections/anova/sig_groups.html#tricky-cases",
    "href": "book_sections/anova/sig_groups.html#tricky-cases",
    "title": "• 19. Significance groups",
    "section": "",
    "text": "Groups X and Y significantly differ. So we put them in different significance groups.\n\nWe’ll assign X to group “a”.\nWe’ll assign Y to group “b”.\n\nBut Z differs from neither group. So,\n\nWe’ll assign Z to groups “a” and “b” because it does not significantly differ from either group. In a plot we would show this as “a,b”.",
    "crumbs": [
      "19. >2 Categories",
      "•  19. Significance groups"
    ]
  },
  {
    "objectID": "book_sections/anova/sig_groups.html#r-automation",
    "href": "book_sections/anova/sig_groups.html#r-automation",
    "title": "• 19. Significance groups",
    "section": "R automation",
    "text": "R automation\nAs you might imagine, logic-ing this all out can get messy. Luckily, you can pipe your glht() output into the cld() function which will assign samples to “significance groups”:\n\nlibrary(multcomp)\nlm(admix_proportion ~ site, data = clarkia_hz)|&gt;\n    glht(linfct = mcp(site = \"Tukey\"))|&gt;\n    cld()\n\n SR S22  SM  S6 \n\"a\" \"b\" \"a\" \"b\"",
    "crumbs": [
      "19. >2 Categories",
      "•  19. Significance groups"
    ]
  },
  {
    "objectID": "book_sections/anova/Ranova_pipeline.html",
    "href": "book_sections/anova/Ranova_pipeline.html",
    "title": "• 19. R ANOVA pipeline",
    "section": "",
    "text": "ANOVA calculations by hand\nHere’s a brief review of how to complete all the bits of an ANOVA in R. To do so, we will look into Sepal.Width by Species in the iris dataset.\nWe can decompose variance components ourselves as follows:\nFirst as the sums of squares\nSS_iris &lt;- iris |&gt;\n  mutate(grand_mean = mean(Sepal.Width))|&gt;\n  group_by(Species)|&gt;\n  mutate(group_mean = mean(Sepal.Width))|&gt;\n  ungroup()|&gt;\n  summarise(SS_model = sum((group_mean   - grand_mean)^2) , \n            SS_error = sum((Sepal.Width - group_mean)^2) , \n            SS_total = sum((Sepal.Width - grand_mean)^2),\n            n_total  = n(),\n            n_groups = n_distinct(Species))\nSS_model\nSS_error\nSS_total\nn_total\nn_groups\n\n\n\n\n11.345\n16.962\n28.307\n150\n3\nThen as full blown ANOVA results\nanova_results &lt;- SS_iris |&gt;\n  mutate(df_model = n_groups - 1,\n         df_error  = n_total  - n_groups,\n         MS_model  = SS_model / df_model,\n         MS_error  = SS_error / df_error,\n         F_stat    = MS_model / MS_error,\n         r2        = SS_model / SS_total,\n         p_value   = pf(F_stat , df1 = df_model, df2 = df_error, \n                        lower.tail = FALSE))\nSS_model\nSS_error\nSS_total\ndf_model\ndf_error\nMS_model\nMS_error\nF_stat\nr2\np_value\n\n\n\n\n11.34\n16.96\n28.31\n2\n147\n5.67\n0.12\n49.16\n0.4\n4.49e-17",
    "crumbs": [
      "19. >2 Categories",
      "•  19. R ANOVA pipeline"
    ]
  },
  {
    "objectID": "book_sections/anova/Ranova_pipeline.html#or-r-can-do-this-for-you",
    "href": "book_sections/anova/Ranova_pipeline.html#or-r-can-do-this-for-you",
    "title": "• 19. R ANOVA pipeline",
    "section": "Or R can do this for you",
    "text": "Or R can do this for you\nWe saw we can do this with the aov() |&gt; summary() pipeline, or the lm() |&gt; anova() pipeline. Here I do the latter. Either way our answers matvh those that we derived above:\n\nlm(Sepal.Width ~ Species, data = iris) |&gt; \n  anova()\n\nAnalysis of Variance Table\n\nResponse: Sepal.Width\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies     2 11.345  5.6725   49.16 &lt; 2.2e-16 ***\nResiduals 147 16.962  0.1154                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe tidy() function in the broom package makes this output tidy and shows our p-values are identical!\n\nlibrary(broom)\nlm(Sepal.Width ~ Species, data = iris) |&gt; \n  anova()|&gt;\n  tidy()\n\n# A tibble: 2 × 6\n  term         df sumsq meansq statistic   p.value\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Species       2  11.3  5.67       49.2  4.49e-17\n2 Residuals   147  17.0  0.115      NA   NA       \n\n\n\nPosthoc tests and significance groups\nAs above there are a few ways to do this, and, as above we stuck the the lm() framework.\n\nlibrary(multcomp)\nlm(Sepal.Width ~ Species, data = iris)|&gt;\n   glht(linfct = mcp(Species = \"Tukey\"))|&gt; \n   summary()\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lm(formula = Sepal.Width ~ Species, data = iris)\n\nLinear Hypotheses:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \nversicolor - setosa == 0    -0.65800    0.06794  -9.685  &lt; 0.001 ***\nvirginica - setosa == 0     -0.45400    0.06794  -6.683  &lt; 0.001 ***\nvirginica - versicolor == 0  0.20400    0.06794   3.003  0.00881 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\nWe see that all groups differ significantly from one another. So call one significance group “a”, another significance group “b”, and the third significance group “c”. It doesn’t matter which one’s which—just that they’re all different. These are arbitrary labels. For more complex cases, R can find significance groups for us:\n\nlm(Sepal.Width ~ Species, data = iris)|&gt;\n   glht(linfct = mcp(Species = \"Tukey\"))|&gt; \n   cld()\n\n    setosa versicolor  virginica \n       \"a\"        \"b\"        \"c\"",
    "crumbs": [
      "19. >2 Categories",
      "•  19. R ANOVA pipeline"
    ]
  },
  {
    "objectID": "book_sections/anova/anova_summary.html",
    "href": "book_sections/anova/anova_summary.html",
    "title": "• 19. ANOVA summary",
    "section": "",
    "text": "Chapter summary\nLinks to: Summary. Chatbot tutor. Questions. Glossary. R packages. R functions. More resources.\nEnjoy this “insomnia meme” about the ANOVA. From this video\nWe previously introduced F and the ANOVA approach as an alternative to the two sample t-test. Here we show it’s true utility - by testing the single null hypothesis that all sample come from the same statistical population, we avoid the “multiple testing problem.” As such, compared to the naive testing of all pairwise differences, the ANOVA approach allows us to farily test for differences in means among numerous groups. If we reject the null that all groups are equal, we conduct a “post-hoc” test to see which groups differ.",
    "crumbs": [
      "19. >2 Categories",
      "• 19. ANOVA summary"
    ]
  },
  {
    "objectID": "book_sections/anova/anova_summary.html#anova_summary_chapter-summary",
    "href": "book_sections/anova/anova_summary.html#anova_summary_chapter-summary",
    "title": "• 19. ANOVA summary",
    "section": "",
    "text": "Chatbot tutor\n\nPlease interact with this custom chatbot (ChatGPT link here, Now Gemini alllows us to share gems! Gemini link here). I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you got what you needed from it.",
    "crumbs": [
      "19. >2 Categories",
      "• 19. ANOVA summary"
    ]
  },
  {
    "objectID": "book_sections/anova/anova_summary.html#t_summary_practice-questions",
    "href": "book_sections/anova/anova_summary.html#t_summary_practice-questions",
    "title": "• 19. ANOVA summary",
    "section": "Practice Questions",
    "text": "Practice Questions\nTry these questions! By using the R environment you can work without leaving this “book”. I even pre-loaded all the packages you need!\n\nSetup\nFiddler males have a greatly enlarged “major” claw, which is used to attract females and to defend a burrow. Darnell and Munguia (2011) suggested that this appendage might also acts as a heat sink, keeping males cooler while out of the burrow on hot days. To test this, they placed four groups of crabs into separate plastic cups and supplied a source of radiant heat (60-watt light bulb) from above. The four groups were intact male crabs; male crabs with the major claw removed; male crabs with the other (minor) claw removed (control), and intact female ﬁddler crabs. They measured body temperature of crabs every 10 minutes for 1.5 hours. These measurements were used to calculate a rate of heat gain for every individual crab in degrees C/log minute.\nRates of heat gain for all crabs are loaded here as crabs but can be downloaded from this link: https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/crabs.csv\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ1.a) There are four groups. How many potential pairwise comparisons are there? .\n\n\nExplanation\n\nFor 4 groups, the number of unique pairs is: \\[\\frac{k(k - 1)}{2} = \\frac{4(3)}{2} = 6\\]\n\nQ1.b) What is the probability of incorrectly rejecting one true null (at the \\(\\alpha\\) threshold of 0.05) if all of these nulls were true? \n\n\nExplanation\n\nThe probability of not incorrectly rejecting a true null is 0.95. To not falsely reejct any try nulls we need six such successes. \\[0.95^6 \\approx 0.735\\] So the probability of incorrectly rejecting at least one true null is:\n\\[1 - 0.735 = 0.265 \\]\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ2) Consider the plot above. Before running an ANOVA or posthoc test, which comparisons do you think is most lilely to be significantly different?\n\n female x male major removed female x male minor removed intact x male major removed intact x male minor removed male major removed x male minor removed\n\n\n\nCalculate the sample size, mean, and variance for each treatment\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ3) Compare the variance with in each treatment. What is the greatest fold difference in variance between treatments?\n\n It's exactly the same Less than two-fold Between two and five-fold Between five and ten-fold More than ten-fold\n\nQ4) With this difference in variance among groups, the assumption of homoscedasticity is\n\n Probably not a major concern Somewhat worrisome, but we can plow ahead anyways A big concern, we must transform or try a Welch's ANOVA\n\n\n\n\n\n\n\n\n\n\n\n\n\nQ5.a) Which of the plots above shows total deviations? abc.\nQ5.b) Which of the plots above shows model deviations? abc.\nQ5.c) Which of the plots above shows error deviations? abc.\n\n\nAssume we where using the code below to build an ANOVA for the crabs data\n\ncrabs                                          |&gt;\n    mutate(grand_mean = mean(bodyTemperature)) |&gt;\n    group_by(crabType)                         |&gt;\n    mutate(group_mean = mean(bodyTemperature)) |&gt;\n    ungroup()                                  |&gt;\n    summarise(this_partition = sum((group_mean - grand_mean)^2))\n\n\nQ6) What would be the best name for this_partition? MS_totalMS_modelMS_errorSS_totalSS_modelSS_error\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nQ7.a) Why didn’t the code above work?\n\n We did not load the broom pacakge We never generated augmented_crab you must use the original tibble, not augment() output to make a plot geom_qq expects the aesthetic to be called sample, not x\n\nQ7.b) Fixing this error, which assumption are these plots meant evaluate?\n\n Equal variance among groups Normal distirbutions of residuals Independence Unbiased data\n\nQ7.c) Which wo of the plots are reasonable ways to evaluate this assumption. Fixing this error, which assumption are these plots meant evaluate?\n\n Plot 1 and Plot 2 Plot 1 and Plot 3 Plot 1 and Plot 4 Plot 2 and Plot 3 Plot 2 and Plot 4 Plot 3 and Plot 4\n\n\nQ8) In the context of an ANOVA, which R command gives you:\nQ8.a) Estimates of model coefficients ONLY?\n\n lm(y ~ x) |&gt; anova() OR aov(y ~ x) |&gt; summary() aov(y ~ x) |&gt; TukeyHSD() lm(y ~ x)|&gt; summary()  lm(y ~ x)\n\nQ8.b) An ANOVA table\n\n lm(y ~ x) |&gt; anova() OR aov(y ~ x) |&gt; summary() aov(y ~ x) |&gt; TukeyHSD() lm(y ~ x)|&gt; summary() lm(y ~ x)\n\nQ8.c) Results of post-hoc tests\n\n lm(y ~ x) |&gt; anova() OR aov(y ~ x) |&gt; summary() aov(y ~ x) |&gt; TukeyHSD() lm(y ~ x)|&gt; summary()  lm(y ~ x)\n\nQ8.d) Model coefficients and standard errors, plus numerous potentially misleading p-values and t-values that should largely be ignored\n\n lm(y ~ x) |&gt; anova() OR aov(y ~ x) |&gt; summary() aov(y ~ x) |&gt; TukeyHSD() lm(y ~ x)|&gt; summary()  lm(y ~ x)\n\n\n\nRun the relevant statistical tests below\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nQ9) Run an anova on the data. What do we do to the null hypothesis? Reject itFail to reject itAccept itFail to accept it\nQ10) According to. the post-hoc test, if female intact is in significance group, “a”, which group(s) is intact male male major in? a onlyb onlyc onlya,ba,cb,ca,b,bc",
    "crumbs": [
      "19. >2 Categories",
      "• 19. ANOVA summary"
    ]
  },
  {
    "objectID": "book_sections/linear_regression.html",
    "href": "book_sections/linear_regression.html",
    "title": "20. Linear Regression",
    "section": "",
    "text": "Review\nMotivating scenarios:   We are interested in predicting one continuous variable based on the value of another, and to estimate the uncertainty and significance of this prediction.\nLearning goals: By the end of this chapter you should be able to\nEarlier we explored how to describe associations between continuous variables. Here we review that material and the integrate this into our linear model framework.",
    "crumbs": [
      "20. Linear Regression"
    ]
  },
  {
    "objectID": "book_sections/linear_regression.html#review-describing-associations",
    "href": "book_sections/linear_regression.html#review-describing-associations",
    "title": "20. Linear Regression",
    "section": "Review: Describing Associations",
    "text": "Review: Describing Associations\nThere are three common and closely related ways to describe the association between two continuous variables, ( x ) and ( y ): covariance, correlation, and slope.\n\nCovariance describes how ( X ) and ( Y ) vary together across individuals: [ = ]\nCorrelation (( r )) describes the strength and reliability of the joint variation between ( X ) and ( Y ). It is the covariance divided by the product of the standard deviations of ( x ) and ( y ), ( s_x ) and ( s_y ), respectively: [ r = ]\nSlope (( b )) indicates the amount by which ( Y ) changes for each unit increase in ( X ). It is the covariance divided by the variance of ( x ), ( s_x^2 ): [ b = ]",
    "crumbs": [
      "20. Linear Regression"
    ]
  },
  {
    "objectID": "book_sections/linear_regression.html#review-linear-models",
    "href": "book_sections/linear_regression.html#review-linear-models",
    "title": "20. Linear Regression",
    "section": "Review: Linear Models",
    "text": "Review: Linear Models\nAs introduced previously, linear models predict one response variable from one or more explanatory variables.\n[ = f(_i) ]\nNaturally, observations do not perfectly match predictions. The residual, ( _i ), describes the difference between the observed value of the ( i^{} ) observation, ( Y_i ), and its predicted value, ( ).\n[ _i = Y_i - ]\nLinear models typically include an intercept (( a )) and an effect (( b )) of each explanatory variable (( x )) on the response. For t-tests and ANOVAs, the intercept represents the mean of the “reference category,” and the effect describes the difference between the mean of individuals in category ( j ) and the mean of the reference category. In a simple linear regression, ( b ) is the slope (as defined above), and ( x_i ) is the value of ( x ) for individual ( i ).\n[ Y_i = a + b x_i + _i ]\n\nReview: The regression as a linear model\nFor a simple linear regression with one explanatory and one response variable,\n\\[\\begin{equation}\n\\begin{split}\nY_i &=  a + b  x_{i}  \n\\end{split}\n(\\#eq:linreg)\n\\end{equation}\\]\nWhere\n\n\\(a\\) is the intercept,\n\n\\(b\\) is the slope, and\n\n\\(X_i\\) is the \\(i^{th}\\) individual’s observed value for the explanatory variable, \\(X\\)\n\nIn simple regression, the intercept can be found as\n\\[a = \\bar{Y} - b \\times \\bar{X}\\]\n\n\n\n\n\nA Visualizing different slopes in a regression. B Making predictions from a linear model.",
    "crumbs": [
      "20. Linear Regression"
    ]
  },
  {
    "objectID": "book_sections/linear_regression.html#review-statistical-evaluation-of-linear-models.",
    "href": "book_sections/linear_regression.html#review-statistical-evaluation-of-linear-models.",
    "title": "20. Linear Regression",
    "section": "Review: Statistical evaluation of linear models.",
    "text": "Review: Statistical evaluation of linear models.\nStatistical analyses of linear models allow us to describe uncertainty in our estimates and conduct null hypothesis significance testing. Of course, we can also do this with bootstrapping and permuting as explored previously, however compared to permuting and bootstrapping, a linear model is nice because it is fast, you always get the same answers, and is based on an actual model.\n\nUncertainty in Linear Models\nWhen working with real-world data, it almost always represents an estimate from a sample rather than a true population parameter. This means we can think of an estimate as a single sample drawn from the (unknown) population sampling distribution—the distribution of estimates we would obtain from repeated sampling of a population. However, since we only have one sample, we cannot know the true sampling distribution, but we can approximate it.\n\nBootstrapping (i.e., resampling observations with replacement): uses our data to approximate the sampling distribution.\n\nIn bootstrapping, we describe uncertainty with the bootstrap standard error (the standard deviation of the bootstrap distribution) and the 95% bootstrap confidence interval—the values between the upper and lower 2.5% tails of our bootstrapped distribution.\n\nThe z and t distributions: provide mathematical approximations of a sampling distribution for normally distributed random variables. Both distributions consider our estimate in units of standard errors away from the center o its distribution. The t distribution is more applicable to real-world data because it incorporates uncertainty in the estimate of the standard deviation, in contrast to the z distribution, which assumes we know the true standard deviation. As sample sizes increase, these distributions become more similar because our estimate of the standard error improves.\n\nWith the t and z distributions, we estimate the standard error through a mathematical formula and describe the 95% confidence interval as the mean ± the standard error multiplied by the critical value, which captures the upper and lower 2.5% of these distributions.\n\n\nThese are two approaches to estimating the same thing, so they usualy provide similar results.\n\n\nNHST for a Linear Model\nIn linear models, we test the null hypothesis that ( y_i ) is independent of our predictors.\nWe can test the null by permutation: This involves randomly reassigning explanatory variables to response values and estimating our parameter under this random shuffling. Repeating this process multiple times approximates the sampling distribution under the null hypothesis of no association. We test the null by assessing the proportion of this null sampling distribution that is as extreme or more extreme than our observed estimate. If this proportion is low (typically less than 0.05), we reject the null hypothesis.\nWe can also test the null by using standard mathematical distributions like the ( Z ), ( t ), and ( F ) distributions: If our test statistic falls in the extreme tail (i.e., only 5% of the distribution is as or more extreme), we reject the null. Recall: - t and z distributions describe estimates of individual parameters in units of standard errors away from the null value of zero, and we consider both tails of these distributions. - F tests compare the variability explained by our model to the residual variation. Under the null, F equals one, and we examine only the upper tail when testing the null hypothesis.\nPermutation tests and mathematical tests aim to achieve the same goal and, therefore, often yield similar results.\n\n\nAssumptions of Linear Models\nThese methods offer benefits but come with assumptions—linear models rely on particular assumptions about the distribution of residuals. In addition to standard assumptions of independence and unbiased sampling, single-factor linear models assume:\n\nResiduals are normally distributed.\nResiduals are independent of the predicted values.\nVariance in residuals is independent of the predicted values.\n\nWhen these assumptions are not met, other approaches (e.g., permutation and bootstrapping methods, models that better account for residual patterns, nonparametric models, etc.) are more appropriate. However, the magic of the central limit theorem means that most linear models are fairly robust to modest violations of these assumptions.",
    "crumbs": [
      "20. Linear Regression"
    ]
  },
  {
    "objectID": "book_sections/linear_regression.html#the-dataset",
    "href": "book_sections/linear_regression.html#the-dataset",
    "title": "20. Linear Regression",
    "section": "The dataset",
    "text": "The dataset\nHow do snakes heat up when they eat? To find out, Tattersall et al. (2004) noted the temperature of snakes as a function of their prey size (Fig. @ref(fig:snakeregression)).\nThe prey size of the \\(X\\) axis of Figure @ref(fig:snakeregression)C is in units of percent of a snake’s body weight, and the temperature on the Y axis of Figure @ref(fig:snakeregression)C, is the degrees Celcius increase in a snake’s body temperature two days after eating.\n\n\n\n\n\nInfrared thermal image of a rattlesnake (A) before feeding and (B) two days after eating 32% of its body mass, C shows the association between meal size and change in temperature. images A, and B, and data from (Tattersall et al., 2004). data here. code here\n\n\n\n\n\nA quick summary of the associtations.\nA quick data summary shows a very tight correlation – temp change is reliably associated with meal size (r = 0.899). We interpret our slop to mean that in this study, a snakes temperature increases, on average, by 0.0274 degrees Celsius for every increase in meal size (measured as percent of a snake’s mass) in addition to the baseline increase in temperature of 0.32 degrees Celsius, on average, that all snakes experienced, regardless of their meal size.\n\nsnake_data &lt;-  read_csv(\"https://whitlockschluter3e.zoology.ubc.ca/Data/chapter17/chap17q11RattlesnakeDigestion.csv\")%&gt;% \n  rename(meal_size = mealSize, body_temp = tempChange)\nsnake_data  %&gt;% \n  summarise(n               = n(),\n            snake_cov       = cov(meal_size, body_temp),\n            snake_cor       = cor(meal_size, body_temp),\n            snake_slope     = snake_cov /var(meal_size),\n            snake_intercept = mean(body_temp) - snake_slope * mean(meal_size))     \n\n# A tibble: 1 × 5\n      n snake_cov snake_cor snake_slope snake_intercept\n  &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;           &lt;dbl&gt;\n1    17      5.28     0.899      0.0274           0.320\n\n\n\nA linear regression in R.\nWe can easily build a linear model in R with the lm() function.\n\nsnake_regression &lt;- lm(body_temp ~ meal_size, data = snake_data)\ncoef(snake_regression)\n\n(Intercept)   meal_size \n 0.31955925  0.02738402 \n\n\n\n\n\nUncertainty in parameter estimates\n\nBootstrap based uncertainty\nAs described above, we can approximate the sampling distirbuiont by bootsrapping. Here we can visualize the uncertainty:\n\nsnake_boot &lt;- replicate(1000, simplify = FALSE,\n                        slice_sample(snake_data, prop=1, replace = TRUE) %&gt;%\n                          lm(body_temp ~ meal_size, data = .)%&gt;%\n                          coef()) %&gt;%\n  bind_rows()\n\n\n\n\n\n\nVisualizing bootstrap based uncertainty in the linear model (A), and the slope and intercept (B). Red denotes the estimate. code here\n\n\n\n\nWe can also generate traditional summaries of uncertainty\n\nsnake_boot%&gt;% \n    summarise(SE_slope = sd(meal_size), \n              lower_CI_slope = quantile(meal_size, prob = 0.025),\n              upper_CI_slope = quantile(meal_size, prob = 0.975),\n              SE_intercept = sd(`(Intercept)`), \n              lower_CI_intercept = quantile(`(Intercept)`, prob = 0.025),\n              upper_CI_intercept = quantile(`(Intercept)`, prob = 0.975)) \n\n\n\n\n\n\nterm\nSE\nlower_CI\nupper_CI\n\n\n\n\nintercept\n0.1061\n0.1744\n0.5714\n\n\nslope\n0.0047\n0.0183\n0.0360\n\n\n\n\n\n\n\nMath-based uncertainty and NHST.\n\nMath-based uncertainty and NHST for the slope\nAlternatively we can estimate the standard error in the slope mathematically, which may be preferable for this modest sample size, following equation @ref(eq:seb):\n\\[\\begin{equation}\n\\begin{split}\nSE_b  &= \\sqrt{\\frac{\\text{MS}_\\text{resid}}{\\sum(X_i-\\overline{X})^2}}  =  \\sqrt{\\frac{\\Sigma(\\epsilon_i^2) / \\text{df}_{b}}{\\sum(X_i-\\overline{X})^2}}    =  \\sqrt{\\frac{\\sum(Y_i-\\widehat{Y_i})^2 /  (n-2) }{\\sum(X_i-\\overline{X})^2}}   \\\\\n\\end{split}\n(\\#eq:seb)\n\\end{equation}\\]\nWhere \\(\\text{df}_b\\) is the degrees of freedom for the slope, which equals the sample size minus two (one for the slope, and one for the intercept).\nEquation @ref(eq:seb) might look, at first blush, a bit different than most equations for the standard error we’ve seen before, but it actually is pretty familiar. The numerator i the difference between an observations and its mean value (conditional, on X, which is a good way to think of \\(\\widehat{Y_i}\\)) divided by the degrees of freedom, which is basically the sample variance in the residuals. We estimate the standard error of the slope by taking the square root of this variance, after dividing through by the sum of square in our predictor X.\nWe can calculate this in R using the augment() function from the broom package. While we’re at it,\n\nWe can include 95% confidence intervals in the standard way—by adding and subtracting the product of the standard error and our critical t-value from the estimate.\n\nWe can also calculate a p-value – testing the null of a slope of zero, by calculating t and looking this up from the t-distribution.\n\n\nsnake_math_uncertainty_slope &lt;- augment(snake_regression) %&gt;%\n  summarise(df_slope = n() -2, \n            numer =  sum(.resid^2)/(df_slope),\n            denom = sum((meal_size-mean(meal_size))^2 ),\n            estimate = coef(snake_regression)[[\"meal_size\"]],\n            se_slope = sqrt(numer/denom),\n            t_slope =  estimate / se_slope, \n            t_crit = qt(0.025,df= df_slope, lower.tail = FALSE),\n            lower_CI = estimate - se_slope *  t_crit,\n            upper_CI = estimate + se_slope *  t_crit,\n            p_slope = 2 * pt(q = t_slope , df = df_slope, lower.tail = FALSE))  %&gt;% \n  select(df_slope, se_slope, estimate, lower_CI, upper_CI, p_slope)\n\n\n\n\n\n\ndf_slope\nse_slope\nestimate\nlower_CI\nupper_CI\np_value\n\n\n\n\n15\n0.0034\n0.0274\n0.02\n0.0347\n9.381e-07\n\n\n\n\n\nFrom these results, we observe:\n\nUncertainty estimates comparable to those achieved through bootstrapping.\nAn extremely low p-value, indicating a very strong association that would be highly unlikely if there were no actual association. Given how low this p-value is, and because I’ve dithered enough, I won’t generate a permutation-based p-value (which would also likely be close to zero).\n\n\n\nMath-based uncertainty and NHST for the intercept\nWe can similarly find the standard error for the intercept. Calculating the intercept’s uncertainty requires accounting for the overall spread of our predictor values, which may be especially useful when (X) values are far from zero.\n[\n\\[\\begin{split}\nSE_{\\text{intercept}} &= \\sqrt{\\frac{\\text{MS}_\\text{resid}}{n} + \\frac{\\overline{X}^2 \\, \\text{MS}_\\text{resid}}{\\sum (X_i - \\overline{X})^2}} \\\\\n\\end{split}\\]\n(#eq:sei) ]\nHere, () (residual mean square) comes from dividing the sum of squared residuals by the degrees of freedom for error, which is (n-2). This equation for (SE{}) might look a bit different, but it’s quite intuitive. The term () represents baseline variance around zero, while () adjusts for any offset in (), accounting for how spread in (X) increases uncertainty in the intercept.\nWe can calculate this in R using augment() from the broom package. While we’re at it, lets Include a 95% confidence interval by adding and subtracting the product of the standard error and critical t-value. But we wont calculate a p-value for the intercept. The null hypothesis of an intercept of zero is not particularly interesting in this case.\n\nsnake_math_uncertainty_intercept &lt;- augment(snake_regression) %&gt;%\n  summarise(df_intercept = n() - 2,\n            mean_X = mean(meal_size),\n            MS_resid = sum(.resid^2) / df_intercept,\n            estimate_intercept = coef(snake_regression)[[\"(Intercept)\"]],\n            se_intercept = sqrt((MS_resid / n()) + \n                                  (mean_X^2 * MS_resid / sum((meal_size - mean_X)^2))),\n            t_intercept = estimate_intercept / se_intercept,\n            t_crit = qt(0.025, df = df_intercept, lower.tail = FALSE),\n            lower_CI_intercept = estimate_intercept - se_intercept * t_crit,\n            upper_CI_intercept = estimate_intercept + se_intercept * t_crit) %&gt;%\n  select(df_intercept, se_intercept, estimate_intercept, lower_CI_intercept, upper_CI_intercept)\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf_intercept\nse_intercept\nestimate_intercept\nlower_CI_intercept\nupper_CI_intercept\n\n\n\n\n15\n0.091\n0.3196\n0.1256\n0.5135\n\n\n\n\n\nAgain, results resmble our esimtated uncertainty from the bootstrap distribution.\n\n\n\nR-based uncertainty and NHST.\nWe can use the also use the [confint()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/confint.html) function to find 95% confidence intervals in R as follows.\n\nconfint(snake_regression, level = 0.95)\n\n                 2.5 %     97.5 %\n(Intercept) 0.12564557 0.51347293\nmeal_size   0.02003708 0.03473096\n\n\nWe can test the null and get standard errors with the summary() function or the tidy() function from the broom package to extract this information from our linear model. Here I just show the output from summary().\n\nsummary(snake_regression)\n\n\nCall:\nlm(formula = body_temp ~ meal_size, data = snake_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.39971 -0.11663 -0.04817  0.17535  0.27252 \n\nCoefficients:\n            Estimate Std. Error t value    Pr(&gt;|t|)    \n(Intercept) 0.319559   0.090977   3.513     0.00314 ** \nmeal_size   0.027384   0.003447   7.944 0.000000938 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1914 on 15 degrees of freedom\nMultiple R-squared:  0.808, Adjusted R-squared:  0.7952 \nF-statistic: 63.11 on 1 and 15 DF,  p-value: 0.0000009381\n\n\n\n\n\nUncertainty in predictions\nWe think of the line of a regression as “Predicting” the value of the response variable, given the value of the explanatory variable, \\(\\widehat{Y_i}\\). What does this mean? Well it’s our estimate of the prediction for the population mean value of Y, given it had this value of X Figure @ref(fig:predictionregression)A.\nWe can acknowledge the uncertainty in this prediction by including a standard error on our regression line as in Figure @ref(fig:predictionregression)B.\nBut this is a prediction for the expected mean \\(\\widehat{Y_i}\\), we might want bands to predict the range of 95% of observations. This is called the prediction interval and is presented in maroon in Figure @ref(fig:predictionregression)C. We can get this prediction interval with the predict().\n\nsnake_regression %&gt;% \n  predict(interval = \"predict\")\n\n\n\nWarning in predict.lm(., interval = \"predict\"): predictions on current data refer to _future_ responses\n\n\n\n\n\n\nWe can visualize these different summaries of uncertainty.\n\n\n\n\n\nPredictions from a linear regression (lines). B Shows our uncertainty in the predicted value, \\(\\widehat{Y_i}\\), as the 95% confidence interval. C Shows the expected variability in observations, as capture by the 95% prediction interval. Code here.\n\n\n\n\nNo need to know the equations underlying these – just know (1) they exist, (2) they are different (3) The prediction interval is always wider than the confidence interval (4) the different goals of each",
    "crumbs": [
      "20. Linear Regression"
    ]
  },
  {
    "objectID": "book_sections/linear_regression.html#effect-of-measurement-error-on-estimated-slopes",
    "href": "book_sections/linear_regression.html#effect-of-measurement-error-on-estimated-slopes",
    "title": "20. Linear Regression",
    "section": "Effect of Measurement Error on Estimated Slopes",
    "text": "Effect of Measurement Error on Estimated Slopes\nWe rarely measure all values of (X) and (Y) perfectly. How does measurement error in (X) and/or (Y) affect our estimated correlations and slopes?\n\nFor correlation, measurement error in (X) and/or (Y) both increases the variance of residuals and biases the correlation closer to zero, pulling it away from its true value.\nFor the slope, measurement error in (X) increases the variance of residuals and similarly biases the slope closer to zero, moving it away from its true value.\nFor the slope, measurement error in (Y) does not systematically increase or decrease the slope estimate. However, it tends to increase the standard error, the residual variance, and the p-value.\n\n\nDemonstrating Attenuation\nLet’s see attenuation in action with a simulation! We’ll start with our snake dataset, where the actual slope is approximately round(coef(snake_regression)[2], digits = 4), and treat this as our “true” population slope.\nUsing the jitter() function, we’ll add:\n\nRandom noise in (X) (meal_size).\nRandom noise in (Y) (body_temp).\nRandom noise in both (X) and (Y).\n\nWe’ll then compare these noisy estimates to the true slope and intercept values. Specifically, we’ll add random noise equal to one standard deviation to (X), (Y), or both, estimate the slope and correlation each time, and repeat this simulation many times.\n\nn_reps &lt;- 1000\n\n# Simulate noisy data for X, estimate slope and correlation\nnoisy_x &lt;- replicate(n_reps, simplify = FALSE, \n  expr = snake_data %&gt;% mutate(meal_size = jitter(meal_size, amount = sd(meal_size))) %&gt;%\n  summarise(est_slope = cov(meal_size, body_temp) / var(meal_size),\n            est_cor = cor(meal_size, body_temp))) %&gt;% bind_rows() %&gt;%mutate(noisy = \"Noisy: X\")\n\n# Simulate noisy data for Y, estimate slope and correlation\nnoisy_y &lt;- replicate(n_reps, simplify = FALSE, \n  expr = snake_data %&gt;%  mutate(body_temp = jitter(body_temp, amount = sd(body_temp))) %&gt;%\n  summarise(est_slope = cov(meal_size, body_temp) / var(meal_size), est_cor = cor(meal_size, body_temp))) %&gt;%\n    bind_rows() %&gt;%  mutate(noisy = \"Noisy: Y\")\n\n# Simulate noisy data for both X and Y, estimate slope and correlation\nnoisy_xy &lt;- replicate(n_reps, simplify = FALSE, \n  expr = snake_data %&gt;% \n   mutate(meal_size = jitter(meal_size, amount = sd(meal_size)),\n          body_temp = jitter(body_temp, amount = sd(body_temp))) %&gt;%\n   summarise(est_slope = cov(meal_size, body_temp) / var(meal_size),\n             est_cor = cor(meal_size, body_temp))) %&gt;%bind_rows() %&gt;%  mutate(noisy = \"Noisy: XY\")\n\n\n\n\n\n\nA demonstration of the effect of measurement error on the estimated correlation (left panels) and slope (right panels). Each row represents a different scenario: 1. Noisy: X shows the results when random noise is added only to the predictor variable. 2. Noisy: Y shows the results when random noise is added only to the response variable, 3. Noisy: XY shows the results when random noise is added to both variables. In each plot, the density curves illustrate the distribution of estimated slopes or correlations across 1,000 simulations. The black vertical line represents the true slope or correlation based on the original dataset without added noise. Colored vertical lines show the average estimated slope or correlation for each noise condition, highlighting the effect of attenuation, where added measurement error pulls estimates closer to zero.",
    "crumbs": [
      "20. Linear Regression"
    ]
  },
  {
    "objectID": "book_sections/linear_regression.html#be-wary-of-extrapolation",
    "href": "book_sections/linear_regression.html#be-wary-of-extrapolation",
    "title": "20. Linear Regression",
    "section": "Be Wary of Extrapolation",
    "text": "Be Wary of Extrapolation\n\nDet er vanskeligt at spaa, især naar det gælder Fremtiden.\nIt is difficult to predict, especially when it comes to the future.\n\nThis quote nicely captures the central challenge in regression. Its power is evident from how often it’s been incorrectly attributed to famous thinkers (e.g., Niels Bohr, Yogi Berra, Mark Twain, and even Nostradamus). This is an example of how “famous things that are said… tend to be attributed to even more famous people,” as Grant Barrett discusses in the Under Understood episode The Origins of Oranges and Door Hinges.\n\nIt is difficult to predict from a linear regression, especially when it comes to data outside the model’s range.\n\n— Me\n\n\nIn addition to the usual cautions about uncertainty, bias, nonindependence, and model assumptions, predictions from a linear model are only reliable within the range of the explanatory variable used to build the model.\n\n\n\n\n\n\n\n\n\n\nExample of Extrapolation Risks\nSpecies-Area Relationship: How does the number of fish species change with pool area? Data for small pools from Kodric-Brown & Brown (1993) are shown in Figure @ref(fig:dontextrap)A, where we fit a linear model:\n[ = 1.79 + 3.55 ^{-4} ^2 ]\nExtrapolating this model to predict species in a 50,000 (m^2) pool, we estimate about 20 fish species (Figure @ref(fig:dontextrap)B). However, additional data shows this is a poor prediction for larger pools (Figure @ref(fig:dontextrap)C).\n\n\n\n\n\nBe wary of extrapolation. A regression fits data from small pools well (A). Extrapolation to larger pools (B) poorly predicts diversity in larger ponds (C). Data from Kodric-Brown & Brown (1993) are available here\n\n\n\n\n\nThe number of fish species is poorly predicted by linear extrapolation based on small pools.\nThe model built from small pools does not predict well for larger pools.\nLesson: Avoid extrapolation—predictions are only reliable within the range of (X) values used in the model.",
    "crumbs": [
      "20. Linear Regression"
    ]
  },
  {
    "objectID": "book_sections/two_predictors.html",
    "href": "book_sections/two_predictors.html",
    "title": "21. Two predictors",
    "section": "",
    "text": "Review of Linear Models\nMotivating scenarios:   We have numerous explanatory variables and want to develop an synthetic model.\nLearning goals: By the end of this chapter you should be able to\nA linear model predicts the response variable, as \\(\\widehat{Y_i}\\) by adding up all components of the model.\n\\[\\begin{equation}\n\\hat{Y_i} = a + b_1  y_{1,i} + b_2 y_{2,i} + \\dots{}\n(\\#eq:predlong)\n\\end{equation}\\]\nLinear models we have seen\nSo far we’ve mainly modeled a continuous response variable as a function of one explanatory variable. But linear models can include multiple predictors – for example, we can predict Dragon Weight as a function of both a categorical (spotted: yes/no) and continuous variable in the same model.",
    "crumbs": [
      "21. Two predictors"
    ]
  },
  {
    "objectID": "book_sections/two_predictors.html#review-of-linear-models",
    "href": "book_sections/two_predictors.html#review-of-linear-models",
    "title": "21. Two predictors",
    "section": "",
    "text": "One sample t-tests: \\(\\widehat{Y} = \\mu\\)\n\nTwo sample t-tests: \\(\\widehat{Y_i} = \\mu + A_i\\) (\\(A_i\\) can take 1 of 2 values)\n\nANOVA: \\(\\widehat{Y_i} = \\mu + A_i\\) (\\(A_i\\) can take one of more than two values)\n\nRegression \\(\\widehat{Y_i} = \\mu + X_i\\) (\\(X_i\\) is continuous)\n\n\n\nTest statistics for a linear model\n\nThe \\(t\\) value describes how many standard errors an estimate is from its null value.\n\nThe \\(F\\) value quantifies the ratio of variation in a response variable associated with a focal explanatory variable (\\(MS_{model}\\)), relative to the variation that is not attributable to this variable (\\(MS_{error}\\)).\n\n\n\nAssumptions of a linear model\nRemember that linear models assume\n\nLinearity: That observations are appropriately modeled by adding up all predictions in our equation.\n\nHomoscedasticity: The variance of residuals is independent of the predicted value, \\(\\hat{Y_i}\\) is the same for any value of X.\n\nIndependence: Observations are independent of each other (aside from the predictors in the model).\n\nNormality: That residual values are normally distributed.\n\nData are collected without bias as usual.",
    "crumbs": [
      "21. Two predictors"
    ]
  },
  {
    "objectID": "book_sections/two_predictors.html#polynomial-regression-example",
    "href": "book_sections/two_predictors.html#polynomial-regression-example",
    "title": "21. Two predictors",
    "section": "Polynomial regression example",
    "text": "Polynomial regression example\nWe note that linear models can include e.g. squared, and geometric functions too, so long as we get out predictions by adding up all the components of the model.\nA classic example of a linear model is a polynomial regression, in which we predict some response variable as a function of a predictor and higher order terms of the predictor. The most common polynomial regression includes the explanatory variable and its square value @ref(eq:polynomial).\n\\[\\begin{equation}\n\\begin{split}\n\\widehat{Y_i} = a + b_1 \\times y_{1,i} + b_2 \\times X_{2,i}\\\\ \\\\\n\\text{ where } X_{2,i} = X_{1,i}^2\n\\end{split}\n(\\#eq:polynomial)\n\\end{equation}\\]\n\nOften including a cubic, or even a quadratic term is useful – but be thoughtful before adding too many in – each additional term takes away from our degrees of freedom, complicates interpretation, and may overfit the data. Let your biological intuition and statistical reasoning guide you.\n\n\nPolynomial regression example\nLet’s revisit our example polynomial regression example predicting the number of species from the productivity of the plot to work through these ideas. Recall that\n\nA simple linear regression did not fit the data well AND violated assumptions of a regression, as residuals were large and positive for intermediate predictions and large and negative for large or small predictions.\n\nIncluding a squared term improved the model fit and had the data meet assumptions.\n\nLet’s write a descriptive equation for each model\n\\[\\text{N.SPECIES = CONSTANT + PRODUCTIVITY}\\] \\[\\text{N.SPECIES = CONSTANT + PRODUCTIVITY + PRODUCTIVITY}^2\\]\nWe present these models in Figure 1. See that we can add a polynomial fit to our ggplot by typing formula =  y ~ poly(x, 2, raw = TRUE) into the geom_smooth function.\n\nbmass &lt;- tibble( Biomass = c(192.982,308.772,359.064,377.778,163.743,168.421,128.655,98.246,107.602,93.567,83.041,33.918,63.158,139.181,148.538,133.333,127.485,88.889,138.012),   n.species = c(25.895,12.729,8.342,2.885,21.504,20.434,18.293,16.046,16.046,11.655,12.725,9.515,7.16,16.042,16.042,11.655,12.725,2.88,8.338))\nbase_plot &lt;- ggplot(bmass, aes(x = Biomass, y =  n.species))+  geom_point()+  xlab(\"Productivity (g/15 Days)\" )\n\nlinear_plot &lt;- base_plot +   labs(title = \"Linear term\") +  \n  geom_smooth(method = 'lm')+theme(axis.text = element_text(size = 10),axis.title = element_text(size = 10),plot.title = element_text(size = 12))\n\npolynomial_plot &lt;- base_plot +  labs(title = \"Linear and squared term\") +\n  geom_smooth(method = 'lm',formula =  y ~ poly(x, 2, raw = TRUE))+theme(axis.text = element_text(size = 10),axis.title = element_text(size = 10), plot.title = element_text(size = 12))\nplot_grid(linear_plot, polynomial_plot, labels = c(\"a\",\"b\"))\n\n\n\n\n\n\n\nFigure 1: (A) Fitting a linear regression to predict the number of plant species from prodcutivity of a plot. (B) A Adding a squared term to our linear regression.\n\n\n\n\n\n\nFitting polynomial regressions in R\nFitting a model with a linear term in R should look familiar to you. linear_term &lt;- lm(n.species  ~ Biomass, bmass)\nThere are a bunch of ways to add a polynomial term.\n\nlm(n.species  ~ poly(Biomass, degree = 2, raw = TRUE), bmass) Is what we typed into our geom_smooth function above. If we typed degree = 3, the model would include a cubed term as well.\n\nlm(n.species  ~ Biomass + I(Biomass^2), bmass) Is a more explicit way to do this. When doing math to variables in our linear model we need to wrap them in I() or R gets confused and does weird things.\n\nlm(n.species  ~ Biomass + Biomass2, bmass %&gt;% mutate(Biomass2 = Biomass^2)) Or we can mutate to add a squared transform the data before making our model. NOTE: I did not pipe the mutate into lm(). That’s because lm() does take things from the standard %&gt;% pipe. If you want to pipe into lm(), you will need the magrittr package and then you can use a special pipe, %$%, so… bmass %&gt;% mutate(Biomass2 = Biomass^2) %$% lm(n.species  ~ Biomass + Biomass2) will work.\n\n\nlinear_term &lt;- lm(n.species  ~ Biomass, bmass)\npoly_term   &lt;- lm(n.species  ~ Biomass + Biomass2, bmass %&gt;% mutate(Biomass2 = Biomass^2))\n\n\n\nInterpretting the output of a polynomial regression – model coefficents\nSo, let’s look at this polynomial regression\n\nsummary.lm(poly_term)\n\n\nCall:\nlm(formula = n.species ~ Biomass + Biomass2, data = bmass %&gt;% \n    mutate(Biomass2 = Biomass^2))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.9394 -1.4605 -0.3453  2.5914  7.3527 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.2887063  4.4420728  -0.515 0.613435    \nBiomass      0.2020729  0.0511573   3.950 0.001146 ** \nBiomass2    -0.0004878  0.0001161  -4.200 0.000678 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.384 on 16 degrees of freedom\nMultiple R-squared:  0.5316,    Adjusted R-squared:  0.473 \nF-statistic: 9.078 on 2 and 16 DF,  p-value: 0.002318\n\n\nThe output of this model should look familiar. Our rows are\n\n(Intercept) – the number of species we would have if we followed our curve to 0 productivity. That this value is -2.2.9 highlights the idea that we should not make predictions outside of the range of our data. Of course, we wouldn’t predict a negative number of species ever…\n\nBiomass – This describes how the number of species changes with a linear increase in productivity. It’s critical to see that this DOES not mean that the number of species always increase with productivity. That’s because of the next term,\n\nBiomass2 – This describes how the number of species changes with productivity squared. The negative sign means that the number of species decreases with the square of productivity. Polynomial regressions are often used in these cases where intermediate values are largest or smallest, so it’s normal to see contrasting signs for the linear and squared terms.\n\nWriting down this equation, we predict species number as\n\\[\\widehat{n.species}_i = -2.29 + 0.202 \\times Biomass_i -0.000488 \\times Biomass_i^2\\] So, for example if we had a plot with a productivity of 250 g/15 Days, we would predict it had \\[\\begin{equation}\n\\begin{split}\n\\widehat{n.specie}{s_{|Biomass=250}} &= -2.29  +0.202 \\times 250  -0.000488 \\times 250^2\\\\\n&= 17.71\n\\end{split}\n(\\#eq:polynomial)\n\\end{equation}\\]\nA value which makes sense, as it seems to be where out curve intersects with 250 in Figure 1 B.\nOur columns, Estimate, Std. Error, t value, and Pr(&gt;|t|) should also feel familiar, all interpretations are the same as usual. The standard error describes the uncertainty in the estimate, the t describes how many standard errors away from zero the estimate is, and the p-value describes the probability that a value this many standard errors away from zero would arise if the null where true. One thing though\nThe p-values in this output do not describe the statistical significance of the predictors!! DO NOT INTERPRET THESE P-VALUES AS SUCH\nOne way to think about this is to just look at our simple linear model which shows basically no association between biomass and species number (and the association it shows it slightly negative).\n\nbroom::tidy(linear_term)                                                                                      %&gt;% mutate_at(.vars = c(\"estimate\"), round, digits = 4 ) %&gt;%mutate_at(.vars = c(\"std.error\"), round, digits = 3 ) %&gt;% mutate_at(.vars = c(\"statistic\"), round, digits = 4 )%&gt;% mutate_at(.vars = c(\"p.value\"), round, digits = 5 ) %&gt;%kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n14.4251\n2.777\n5.1942\n0.00007\n\n\nBiomass\n-0.0078\n0.015\n-0.5102\n0.61648\n\n\n\n\n\n\nThe summary.lm() output still usefully provides our estimates and uncertainty in them – so don’t ignore it!\n\n\n\nAn ANOVA approach\nSo how do we get significance of each term? We look at the ANOVA output!\n\nanova(poly_term)\n\nAnalysis of Variance Table\n\nResponse: n.species\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nBiomass    1   9.90    9.90  0.5151 0.4832884    \nBiomass2   1 339.02  339.02 17.6417 0.0006782 ***\nResiduals 16 307.47   19.22                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe now conclude that the association between n.species and the linear term of Biomass would be quite expected from the null. How do we square these ideas? I think of the significance of the linear term as how weird it would be to see a non-zero linear estimate in the absence of a squared term. However, this is not fully correct, as this P-value differs from the one above with just the linear term. To make sense of this, let’s dig into how we calculate the sums of squares for these larger models.\n“Sequential” Type I Sums of squares\nWe’ll see in this and the next section that there’s a real issue in which variable we attribute our sums of squares to in larger linear models.\nIn many cases (see below) Sequential “Type I” sums of squares make the most sense. Here we\n\nCalculate \\(SS_{error}\\) and \\(SS_{total}\\) as we always do! (Figure 2 A, D)\nCalculate the \\(SS_{thing1}\\) (in this case Biomass), as if it where the only thing in the model, \\(\\widehat{Y_{i|bmass}}\\). (Figure 2 B).\n\nCalculate the \\(SS_{thing2}\\) (in this case \\(Biomass^2\\)), as the deviation of predicted values from a model with both things in it, \\(\\widehat{Y_{i|bmass,bmass^2}}\\), minus predictions from a model with just thing1 in it, \\(\\widehat{Y_{i|bmass}}\\) (Figure Figure 2 C).\n\n\n\n\n\n\n\n\n\nFigure 2: Calculating sequential sums of squares for our model. a Total deviations, as usual. b Deviations from predicted values of Biomass alone without considering the squared term (red line) – this makes up \\(SS_{Biomass}\\). c Deviations of predictions from \\(Biomass + Biomass^2\\) (blue line) away from predictions of Biomass alone (red line) – this makes up \\(SS_{Biomass^2}\\). d Deviation of data points from full model (blue line) – this makes up \\(SS_{error}\\)\n\n\n\n\n\nWe can calculate these sums of squares in R as follows, and then compute mean squares and p-values. Before I do this, I make a tibble with prediction from both the simple linear model with just a linear term, and the fuller linear model with the linear and squared term.\n\ncombine_models &lt;- full_join(augment(linear_term) %&gt;%   \n            dplyr::select(n.species, Biomass, .fitted_lin = .fitted,  .resid_lin = .resid),\n          augment(poly_term) %&gt;%\n            dplyr::select(n.species, Biomass, .fitted_full = .fitted, .resid_full= .resid),\n          by = c(\"n.species\", \"Biomass\"))\n\ncombine_models %&gt;%\n  summarise(ss_tot    = sum( (n.species    - mean(n.species))^2 ),\n            ss_bmass  = sum( (.fitted_lin  - mean(n.species))^2 ),\n            ss_bmass2 = sum( (.fitted_full - .fitted_lin )^2 ),\n            ss_error  = sum( (n.species    - .fitted_full)^2 ),\n            #df\n            df_bmass  = 1,     df_bmass2 = 1,     df_error  = n() - 3,\n            #\n            ms_bmass  = ss_bmass  / df_bmass ,\n            ms_bmass2 = ss_bmass2 / df_bmass2 ,\n            ms_error  = ss_error  / df_error,\n            #\n            F_bmass   = ms_bmass / ms_error,     F_bmass2  = ms_bmass2/ ms_error,\n            p_bmass    = pf(q = F_bmass,  df1 = df_bmass,  df2 = df_error, lower.tail = FALSE),\n            p_bmass2   = pf(q = F_bmass2, df1 = df_bmass2, df2 = df_error, lower.tail = FALSE))                                                                   %&gt;% mutate_all(round,digits = 4) %&gt;%DT::datatable( options = list( scrollX='400px'))\n\n\n\n\n\nYou can scroll through the output above to see that our calculations match what anova() tells us!!!",
    "crumbs": [
      "21. Two predictors"
    ]
  },
  {
    "objectID": "book_sections/two_predictors.html#type-i-sums-of-squares-and-others",
    "href": "book_sections/two_predictors.html#type-i-sums-of-squares-and-others",
    "title": "21. Two predictors",
    "section": "Type I Sums of Squares (and others)",
    "text": "Type I Sums of Squares (and others)\nCalculating Sums of Squares sequentially, as we did in Figure Figure 2, is the default way R does things.\nSequential Type I sums of squares calculate the sums of squares for the first thing in your model first, then the second thing, then the third thing etc… This means that while our\n\nSums of square, Mean squares, F values, and p-values might change, depending on the order in which variables are entered into our model.\n\nParameter estimates and uncertainty in them will not change with order.\n\nIn general sequential sums of squares make the most sense when\n\nWe are not interested in the significance of the earlier terms in our model, which we want to take account of, but don’t really care about their statistical significance.\n\nDesigns are “balanced” (Figure 3), as in these cases, we get the same SS, F and P values regardless of the order that we put terms into the model.\n\n\n\n\n\n\n\n\n\nFigure 3: Examples of balanced and unbalanced statistical designs, from statistics how to.\n\n\n\n\n\nIn the next class, we will look into other ways to calculate the sums of squares.",
    "crumbs": [
      "21. Two predictors"
    ]
  },
  {
    "objectID": "book_sections/two_predictors.html#two-categorical-variables-without-an-interaction",
    "href": "book_sections/two_predictors.html#two-categorical-variables-without-an-interaction",
    "title": "21. Two predictors",
    "section": "Two categorical variables without an interaction",
    "text": "Two categorical variables without an interaction\nWe saw that paired t-tests increase our power because they control for extraneous variation impacting each pair. We often want to use a similar study design for a study with more than two explanatory variables.\nFor example, in a randomized “Controlled Blocked Design” each “block” gets all treatments, and by including treatment in our model we can explain variability associated with block unrelated to our main question. In such models we don’t care about the statistical significance of the block, we just want to use block to explain as much variation as possible before considering treatment.\nIn the study below, researchers wanted to know if the presence of a fish predator impacted diversity pf the marine zooplankton in the area. To find out they introduced the zooplankton with no, some, or a lot of fish, in mesh bags in a stream. Each stream got three such bags – one with no, one with some, and the other with many fish. This was replicated at five streams, so each stream is a “Block”.\n\n\n\n\n\nThe prey and none, some or lots of its predator are put in mesh bags in five streams\n\n\n\n\n\n\n\nThe prey and none, some or lots of its predator are put in mesh bags in five streams\n\n\n\n\n\nEstimation and Uncertainty\nThe raw data are presented below, with means for treatments in the final column.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntreatment\nBlock: 1\nBlock: 2\nBlock: 3\nBlock: 4\nBlock: 5\nmean_diversity\n\n\n\n\ncontrol\n4.1\n3.2\n3.0\n2.3\n2.5\n3.02\n\n\nlow\n2.2\n2.4\n1.5\n1.3\n2.6\n2.00\n\n\nhigh\n1.3\n2.0\n1.0\n1.0\n1.6\n1.38\n\n\n\n\n\nWe can conceive of this as a linear model, in which we predict diversity as a function of block and treatment\n\\[DIVERSITY = BLOCK + TREATMENT\\] We enter the model into R as follows\n\nfish_dat &lt;- read_csv(\"https://whitlockschluter3e.zoology.ubc.ca/Data/chapter18/chap18e2ZooplanktonDepredation.csv\") %&gt;%\n  mutate(treatment = fct_relevel(treatment, c(\"control\", \"low\", \"high\")))\nwrong_fish_lm &lt;- lm(diversity ~ block + treatment, data = fish_dat)\nbroom::tidy(wrong_fish_lm)                                                                                       %&gt;% mutate_at(.vars = c(\"estimate\"), round, digits = 3 ) %&gt;%mutate_at(.vars = c(\"std.error\"), round, digits = 3 ) %&gt;% mutate_at(.vars = c(\"statistic\"), round, digits = 3 )%&gt;% mutate_at(.vars = c(\"p.value\"), round, digits = 7 ) %&gt;%kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n3.50\n0.384\n9.109\n0.0000019\n\n\nblock\n-0.16\n0.099\n-1.613\n0.1350936\n\n\ntreatmentlow\n-1.02\n0.344\n-2.968\n0.0127918\n\n\ntreatmenthigh\n-1.64\n0.344\n-4.772\n0.0005788\n\n\n\n\n\nNOTE Uhhohh. Something went wrong here. Why is there only one value for block, when there are five? It’s because R thought bock was a number and ran a regression. Let’s clarify this for R by mutating black to be a factor.\n\nfish_dat &lt;- mutate(fish_dat, block = factor(block))\nfish_lm   &lt;- lm(diversity ~ block + treatment, data = fish_dat)\nbroom::tidy(fish_lm)                                                                                          %&gt;% mutate_at(.vars = c(\"estimate\"), round, digits = 3 ) %&gt;%mutate_at(.vars = c(\"std.error\"), round, digits = 3 ) %&gt;% mutate_at(.vars = c(\"statistic\"), round, digits = 3 )%&gt;% mutate_at(.vars = c(\"p.value\"), round, digits = 6 ) %&gt;%kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n3.42\n0.313\n10.938\n0.000004\n\n\nblock2\n0.00\n0.374\n0.000\n1.000000\n\n\nblock3\n-0.70\n0.374\n-1.873\n0.097945\n\n\nblock4\n-1.00\n0.374\n-2.676\n0.028108\n\n\nblock5\n-0.30\n0.374\n-0.803\n0.445316\n\n\ntreatmentlow\n-1.02\n0.289\n-3.524\n0.007805\n\n\ntreatmenthigh\n-1.64\n0.289\n-5.665\n0.000473\n\n\n\n\n\nWe take these estimates and uncertainty about them seriously, but fully ignore the t and p-values, as above. From here we predict diversity in\n\nControl predation in block one is just the intercept: \\(3.42  -   0  - 0 = 3.42\\).\n\nControl predation in block three is the intercept minus the mean difference between block three and block one: \\(3.42  -   0.70  - 0 = 2.72\\).\n\nHigh predation in block one is the intercept minus the mean difference between high predation and the control, \\(3.42  -   0  - 1.64 = 1.79\\).\n\nLow predation in block four is the intercept, minus the mean difference between block four and block one, minus the mean difference between low predation and the control treatment: 3.42 - 1.00 - 1.02 = 1.40.\n\n\n\nHypothesis testing with two categorical predictors\nIn this model, the null and alternative hypotheses are\n\n\\(H_A:\\) There is an association between predator treatment and zooplankton diversity – i.e. Under the alternative, we predict zooplankton diversity in this experiment as the intercept plus and the deviation associated with stream plus the effect of one or more treatments (Figure 4 b).\n\n\n\n\n\n\n\n\n\nFigure 4: Null (a) and alternative (b) hypotheses for our blocked predation experiment.\n\n\n\n\n\nAgain we test these hypotheses in an ANOVA framework\n\nanova(fish_lm)\n\nAnalysis of Variance Table\n\nResponse: diversity\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nblock      4 2.3400  0.5850  2.7924 0.101031   \ntreatment  2 6.8573  3.4287 16.3660 0.001488 **\nResiduals  8 1.6760  0.2095                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe conclude that predation treatment impacts zooplankton diversity, with diversity decrease as there are more predators. Because this is an experimental manipulation, we can conclude that predation decreased diversity.\n\nCalcualating sums of squares\n\n\n\n\n\nCalculating sequential sums of squares for our zooplankton model. a Total deviations, as usual. b Deviations from predicted values of Block alone without considering the treatment – this makes up \\(SS_{block}\\). c Deviations of predictions from \\(block + treatment\\) away from predictions of block alone – this makes up \\(SS_{treatment}\\). d Deviation of data points from full model (blue line) – this makes up \\(SS_{error}\\)\n\n\n\n\nWe again use the sequential method to calculate sums of squares because we first want to account for block. The code below shows you how anova() got its answer. But in our case we ignore the F-value and significance of block, as it’s in the model to soak up shared variation, not to be tested.\n\nblock_model &lt;- lm(diversity ~ block, fish_dat)\nfull_model  &lt;- lm(diversity ~ block + treatment, fish_dat)\ncombine_models &lt;- full_join(augment(block_model ) %&gt;%   \n            dplyr::select(diversity, block, .fitted_block = .fitted,  .resid_block = .resid),\n          augment(full_model) %&gt;%\n            dplyr::select(diversity, block, treatment, .fitted_full = .fitted, .resid_full= .resid),\n          by = c(\"diversity\", \"block\"))\n\ncombine_models %&gt;%\n  summarise(ss_tot    = sum( (diversity      - mean(diversity))^2 ),\n            ss_block  = sum( (.fitted_block  - mean(diversity))^2 ),\n            ss_treat = sum( (.fitted_full    - .fitted_block)^2 ),\n            ss_error  = sum( (diversity      - .fitted_full)^2 ),\n            #df\n            df_block  = n_distinct(block) - 1,  df_treat = n_distinct(treatment) -1,  \n            df_error  = n() - 1,\n            #\n            ms_block  = ss_block  / df_block,\n            ms_treat  = ss_treat / df_treat ,\n            ms_error  = ss_error  / df_error,\n            #\n            F_treat   = ms_treat / ms_error,\n            p_bmass2   = pf(q = F_treat, df1 = df_treat, df2 = df_error, lower.tail = FALSE))                                                                  %&gt;% mutate_all(round,digits = 4) %&gt;%DT::datatable( options = list( scrollX='400px'))\n\n\n\n\n\n\n\n\nPost-hoc tests for bigger linear models\nSo we rejected the null hypothesis and conclude that predator abundance impacts zooplankton diversity. Which treatments differ? Again we, conduct a post-hoc test.\nInstead of using the aov() function and piping the output to TukeyHSD(), here I’ll show you how to conduct a posthoc test with the glht() function in the multcomp package.\nIn the code below, I say I want to look at all pairwise comparisons between treatments, using the Tukey-Kramer method from Chapter @ref(anova).\n\nlibrary(multcomp)\nglht(full_model, linfct = mcp(treatment = \"Tukey\")) %&gt;%\n  summary()\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lm(formula = diversity ~ block + treatment, data = fish_dat)\n\nLinear Hypotheses:\n                    Estimate Std. Error t value Pr(&gt;|t|)   \nlow - control == 0   -1.0200     0.2895  -3.524  0.01905 * \nhigh - control == 0  -1.6400     0.2895  -5.665  0.00131 **\nhigh - low == 0      -0.6200     0.2895  -2.142  0.14267   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\nWe conclude that diversity in both low and high predation differ significantly from both the control, no predation treatment, treatment. But we fail to reject the hypothesis that low and high predation treatments differ.",
    "crumbs": [
      "21. Two predictors"
    ]
  },
  {
    "objectID": "book_sections/two_predictors.html#quiz",
    "href": "book_sections/two_predictors.html#quiz",
    "title": "21. Two predictors",
    "section": "Quiz",
    "text": "Quiz\nlink",
    "crumbs": [
      "21. Two predictors"
    ]
  },
  {
    "objectID": "book_sections/multiple_regression.html",
    "href": "book_sections/multiple_regression.html",
    "title": "22. Multiple Regression",
    "section": "",
    "text": "Review of Linear Models\nMotivating Scenario:\nWe have numerous continuous explanatory variables and aim to develop a comprehensive model.\nLearning Goals: By the end of this chapter, you should be able to:\nA linear model predicts the response variable, \\(\\widehat{Y_i}\\), by summing all components of the model:\n\\[\\hat{Y_i} = a + b_1 y_{1,i} + b_2 y_{2,i} + \\dots{}\\]",
    "crumbs": [
      "22. Multiple Regression"
    ]
  },
  {
    "objectID": "book_sections/multiple_regression.html#multiple-regression-example-admixed-clarkia",
    "href": "book_sections/multiple_regression.html#multiple-regression-example-admixed-clarkia",
    "title": "22. Multiple Regression",
    "section": "Multiple Regression Example: Admixed Clarkia",
    "text": "Multiple Regression Example: Admixed Clarkia\nHere, I present a snapshot of some ongoing research we are working on with collaborators Dave Moeller, Shelly Sianta, and Brooke Kern.\nOver the past twenty years, research in speciation genomics has revealed that gene flow between sister taxa is quite common. We are investigating what factors influence the extent of gene flow between two sister species of Clarkia xantiana: Clarkia xantiana xantiana and Clarkia xantiana parviflora (hereafter referred to as xan and parv). These taxa show an interesting difference in reproductive biology:\n\nxan predominantly outcrosses, meaning it receives pollen from other individuals of the same (or even different) species.\nparv predominantly self-fertilizes, where one individual often pollinates itself.\n\nOur study focuses on understanding what influences the amount of gene flow we observe. Specifically, we are interested in whether differences in traits within parv associated with its self-fertilization tendency (e.g., petal area, petal color, herkogamy—the distance between a flower’s male and female parts) impact the extent of gene flow. Additionally, we are curious to see if proximity to the other species (xan) increases gene flow.\nTo explore these questions, we sequenced the genomes of a number of plants, noting their collection locations and key phenotypic traits. Here, we will focus on parviflora plants from one population (Site 22) and examine:\n\nWhether physical distance between a parviflora plant and the nearest xantiana plant influences the extent of admixture (i.e., the proportion of its genome derived from xantiana).\nWhether herkogamy, the distance between male (anther) and female (stigma) parts, influences the extent of admixture. Our underlying biological idea is – if you’re fertilizing yourself you cannot be fertilized by the other species.\n\n\n\n\n\n\nOur species. Images taken from Calflora\n\n\n\n\n\nThe data\nThe data are available here. A quick plot shows that\n\nThe admixture proportion appears to decrease as plants are further from the other species and\n\nThe admixture proportion appears to increase as stigma are further from anthers\n\n\nclarkia_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_admix_S22P.csv\"\nclarkia &lt;- read_csv(clarkia_link)\n\na &lt;- ggplot(clarkia,aes(x = herkogamy, y = admixture_proportion, color = distance2hetero))+\n  geom_point(size = 3, alpha = .72)+\n  geom_smooth(method = \"lm\")                                                                                                                                                   + scale_color_viridis_c(option = \"viridis\")+ theme_light()+  theme(axis.text = element_text(size = 12),axis.title = element_text(size = 15),    legend.text = element_text(size = 12),legend.title = element_text(size = 15), legend.position = \"bottom\",legend.key.width = unit(1.5, \"cm\") )\n\n\nb &lt;- ggplot(clarkia,aes(x = distance2hetero, y =admixture_proportion, color =  herkogamy))+\n  geom_point(size = 3, alpha = .72)+\n  geom_smooth(method = \"lm\")                                                                                                                                      + scale_color_viridis_c(option = \"plasma\")+ theme_light()+  theme(axis.text = element_text(size = 12),axis.title = element_text(size = 15),        legend.text = element_text(size = 12),legend.title = element_text(size = 15), legend.position = \"bottom\",legend.key.width = unit(1.5, \"cm\") )\n\na+b\n\n\n\n\n\n\n\n\nOr you can look at a bunch at once with the ggpairs() function in the GGally package.\n\nlibrary(GGally)\nggpairs(clarkia)\n\n\n\n\n\n\n\n\n\n\nModeling Residuals\nOne way to approach multiple regression (especially when using Type II sums of squares, as discussed below) is by aiming to model the variation in each trait after accounting for the influence of the other. For example, if we’re interested in the effect of herkogamy on admixture while accounting for the effect of distance2hetero, we can model the residuals from a model of Admixture = f(distance2hetero) as a function of herkogamy (and vice versa).\n\nmodel_distance2hetoro &lt;- lm(admixture_proportion  ~ distance2hetero, data = clarkia)\naugment(model_distance2hetoro)%&gt;%\n  mutate(herkogamy = pull(clarkia, herkogamy)) %&gt;%\n  ggplot(aes(x = herkogamy, y = .resid))+\n  geom_point(size= 3, alpha = .7)+\n  geom_smooth(method = \"lm\")                                                                                   +  theme(axis.text = element_text(size = 12),axis.title = element_text(size = 15))\n\n\n\n\n\n\n\naugment(model_distance2hetoro)%&gt;%\n  mutate(herkogamy = pull(clarkia, herkogamy)) %&gt;%\n  lm(.resid ~ herkogamy, data = .)%&gt;% \n  tidy()                                                                                                                %&gt;% mutate_at(2:5, round, digits =4) %&gt;% kbl()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.0079\n0.0037\n-2.125\n0.0429\n\n\nherkogamy\n0.0158\n0.0047\n3.360\n0.0023\n\n\n\n\n\n\nmodel_herkogamy &lt;- lm(admixture_proportion  ~ herkogamy, data = clarkia)\naugment(model_herkogamy)%&gt;%\n  mutate(distance2hetero = pull(clarkia,distance2hetero)) %&gt;%\n  ggplot(aes(x = distance2hetero, y = .resid))+\n  geom_point(size= 3, alpha = .7)+\n  geom_smooth(method = \"lm\")                                                                                   +  theme(axis.text = element_text(size = 12),axis.title = element_text(size = 15))\n\n\n\n\n\n\n\naugment(model_herkogamy)%&gt;%\n  mutate(distance2hetero = pull(clarkia, distance2hetero)) %&gt;%\n  lm(.resid ~ distance2hetero, data = .)%&gt;% \n  tidy()                                                                                                                %&gt;% mutate_at(2:5, round, digits =4) %&gt;% kbl()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.0269\n0.0105\n2.5720\n0.0159\n\n\ndistance2hetero\n-0.0010\n0.0004\n-2.6752\n0.0125",
    "crumbs": [
      "22. Multiple Regression"
    ]
  },
  {
    "objectID": "book_sections/multiple_regression.html#building-and-evaluating-our-multiple-regression-model",
    "href": "book_sections/multiple_regression.html#building-and-evaluating-our-multiple-regression-model",
    "title": "22. Multiple Regression",
    "section": "Building and evaluating our multiple regression model",
    "text": "Building and evaluating our multiple regression model\nRather than modeling these pieces separately, multiple regression allows us to jointly model the influence of two continuous explanatory predictors on a continuous response variable. We can build this in R as we’re accustomed to:\n\nmodel_dist2_herk &lt;- lm(admixture_proportion ~ distance2hetero + herkogamy, data = clarkia)\n\n\nEvaluating assumptions\n\nMulticollinearity\nLet’s first check for multicollinearity. Remember, it’s difficult to interpret a multiple regression model if the explanatory variables are correlated. Fortunately, in this case, they appear to be independent! We have three pieces of evidence:\n\nThe correlation is weak,\n\nThere is no obvious pattern in the plot\n\nThe variance inflation factor (a measure of multicolinearity) is small (it is approximately 1 which means theres no correlation). We get a bit worried when it is more than 5 and very worried when it is more than 10)!\n\n\nlibrary(car)\nclarkia %&gt;% \n  summarise(cor_herk_dist = cor(herkogamy,distance2hetero))\n\n# A tibble: 1 × 1\n  cor_herk_dist\n          &lt;dbl&gt;\n1       -0.0448\n\nggplot(clarkia, aes(x = distance2hetero, y = herkogamy))+\n  geom_point(size= 3, alpha = .7)+\n  geom_smooth(method = \"lm\")                                                                                   +  theme(axis.text = element_text(size = 12),axis.title = element_text(size = 15))\n\n\n\n\n\n\n\ncar::vif(model_dist2_herk)\n\ndistance2hetero       herkogamy \n       1.002007        1.002007 \n\n\n\n\nResidual variation\nOur model diagnostics look pretty good. Points are all largely on the qq line, the value and the absolute value of residuals are largely independent of predictors. So, lets get to work!\n\nautoplot(model_dist2_herk)\n\n\n\n\n\n\n\n\n\n\n\nEstimation\n\nemtrends(model_dist2_herk,var = \"herkogamy\")%&gt;%\n  summary(infer = c(TRUE, TRUE))                                                                                              %&gt;%  select(herkogamy, lower.CL, upper.CL )                                                           %&gt;% mutate_all(round,digits=5)%&gt;% kbl()\n\n\n\n\nherkogamy\nlower.CL\nupper.CL\n\n\n\n\n0.5\n0.00597\n0.02565\n\n\n\n\n\n\nemtrends(model_dist2_herk,var = \"distance2hetero\")%&gt;%\n  summary(infer = c(TRUE, TRUE))                                                                                               %&gt;%  select(distance2hetero.trend, lower.CL, upper.CL )                                                           %&gt;% mutate_all(round,digits=5)%&gt;% kbl()\n\n\n\n\ndistance2hetero.trend\nlower.CL\nupper.CL\n\n\n\n\n-0.00097\n-0.00173\n-0.00021\n\n\n\n\n\n\n\nTypes of sums of squares.\nBy default, the anova() function uses Type I Sums of Squares, which we introduced earlier. Remember, with Type I Sums of Squares, you calculate the sums of squares for the first predictor by fitting a model with only that predictor. Then, to find the sums of squares for the second predictor, you subtract the sums of squares from the first predictor’s model from the sums of squares of a model including both predictors. This means the order in which predictors are entered into the model affects the results. I demonstarted this below\n\nlm(admixture_proportion ~ distance2hetero + herkogamy, data = clarkia) %&gt;%\n  anova()\n\nAnalysis of Variance Table\n\nResponse: admixture_proportion\n                Df    Sum Sq    Mean Sq F value   Pr(&gt;F)   \ndistance2hetero  1 0.0019218 0.00192177  7.7233 0.009991 **\nherkogamy        1 0.0027128 0.00271278 10.9023 0.002796 **\nResiduals       26 0.0064695 0.00024883                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nlm(admixture_proportion ~ herkogamy + distance2hetero , data = clarkia) %&gt;%\n  anova()\n\nAnalysis of Variance Table\n\nResponse: admixture_proportion\n                Df    Sum Sq    Mean Sq F value   Pr(&gt;F)   \nherkogamy        1 0.0029154 0.00291536 11.7164 0.002062 **\ndistance2hetero  1 0.0017192 0.00171919  6.9092 0.014204 * \nResiduals       26 0.0064695 0.00024883                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis ordering is ok if we’re dealing with a covariate we don’t care much about. However, it’s not ideal for testing interesting biological models, as we don’t want our answers to depend on the order things went into lm().",
    "crumbs": [
      "22. Multiple Regression"
    ]
  },
  {
    "objectID": "book_sections/multiple_regression.html#type-ii-sums-of-squares-are-usually-better",
    "href": "book_sections/multiple_regression.html#type-ii-sums-of-squares-are-usually-better",
    "title": "22. Multiple Regression",
    "section": "Type II sums of squares are usually better",
    "text": "Type II sums of squares are usually better\nIn contrast, Type II Sums of Squares calculates the sums of squares for each factor after accounting for the other factor. It’s as if both factors are treated as the “second” predictor in a Type I Sums of Squares model. That is, each factor’s effect is measured independently of the order in which they are entered. We can calculate type II sums of squares with the Anova() function in the cars package:\n\nlm(admixture_proportion ~ distance2hetero + herkogamy, data = clarkia) %&gt;%\n  Anova(type = \"II\")\n\nAnova Table (Type II tests)\n\nResponse: admixture_proportion\n                   Sum Sq Df F value   Pr(&gt;F)   \ndistance2hetero 0.0017192  1  6.9092 0.014204 * \nherkogamy       0.0027128  1 10.9023 0.002796 **\nResiduals       0.0064695 26                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNote that our entries both match the results when each was second in a Type I sums of squares model.\n\nVisualizing multiple regression: Added Variable plots\nVisualizing multivariate relationships can be challenging. Above, we explored a few approaches to address this. For example, in our first plot, we placed the response variable (admixture proportion) on the y-axis, one explanatory variable on the x-axis, and represented another explanatory variable using color. However, this approach can be difficult to interpret. To provide a more comprehensive view, we used a ggpairs plot to display all combinations of pairwise relationships, as well as residual plots.\nHere, I introduce the added variable plot (AV plot). An AV plot is similar to a residual plot. On the y-axis, it shows the residuals from a model where the response is predicted by all variables except the focal predictor (the explanatory variable of interest). The x-axis displays the residuals of the focal predictor after it has been modeled as a function of all other predictors. This plot allows us to visualize the unique contribution of the focal predictor to the response, independent of the other variables in the model.\n\na &lt;- tibble(\n  resid_y = lm(admixture_proportion ~ herkogamy, data = clarkia) %&gt;% augment() %&gt;% pull(.resid),\n  resid_x = lm(distance2hetero ~ herkogamy, data = clarkia) %&gt;% augment() %&gt;% pull(.resid))%&gt;%\n  ggplot(aes(x = resid_x, y = resid_y))+\n  geom_point()+\n  labs(x =  \"Distance to hetero | others\", y = \"Admixture proportion | others\")+\n  geom_smooth(method = \"lm\")\n\n\nb &lt;- tibble(\n  resid_y = lm(admixture_proportion ~ distance2hetero, data = clarkia) %&gt;% augment() %&gt;% pull(.resid),\n  resid_x = lm(herkogamy ~ distance2hetero, data = clarkia) %&gt;% augment() %&gt;% pull(.resid))%&gt;%\n  ggplot(aes(x = resid_x, y = resid_y))+\n  geom_point()+\n  labs(x =  \"Herkogamy | others\", y = \"Admixture proportion | others\")+\n  geom_smooth(method = \"lm\")\n\na + b\n\n\n\n\n\n\n\n\n\n\nComparing coefficients\n\n\n\n\n\nvariable\nlower.CL\nestimate\nupper.CL\n\n\n\n\ndistance2hetero\n-0.00173\n-0.00097\n-0.00021\n\n\nherkogamy\n0.00597\n0.01581\n0.02565\n\n\n\n\n\nLooking at our model coefficients, we see that the admixture proportion changes much more rapidly with herkogamy than it does with distance to a heterosubspecific. Does this mean that distance to heterosubspecifics is less important? Not necessarily! These variables have different ranges, variances, etc. A standardized regression coefficient allows for an apples-to-apples comparison. We can find this by z-transforming all x and y variables. You can think of this value as a correlation—in fact, it’s the correlation when there is only one explanatory variable.\nNow we see that the effects are quite similar!\n\n# first we write our own function to z-transform\nzTransform &lt;- function(X){(X-mean(X,na.rm = TRUE)) / sd(X,na.rm = TRUE) } \n\nclarkia%&gt;% \n  mutate_all(zTransform) %&gt;%\n  lm(admixture_proportion ~ herkogamy + distance2hetero, data = .)                                                                                                      %&gt;% coef()%&gt;%   round(digits=3)%&gt;%kable()\n\n\n\n\n\nx\n\n\n\n\n(Intercept)\n0.000\n\n\nherkogamy\n0.495\n\n\ndistance2hetero\n-0.394\n\n\n\n\n\n\n\nConclusion\nWe find more admixture in parviflora plants closer to a hetero-subspecific, and more admixture in these plants that have more distance between their anthers and stigmas. There are numerous potential causal interpretations of these trends.",
    "crumbs": [
      "22. Multiple Regression"
    ]
  },
  {
    "objectID": "book_sections/interactions.html",
    "href": "book_sections/interactions.html",
    "title": "23. Interactions",
    "section": "",
    "text": "Review of Linear Models\nMotivating scenarios:   We have numerous explanatory variables and want to develop an synthetic model. We are particularly interested in hypotheses in which the reponse variable may depend on an interaction between explanatory variables.\nLearning goals: By the end of this chapter you should be able to\nA linear model predicts the response variable, as \\(\\widehat{Y_i}\\) by adding up all components of the model.\n\\[\\begin{equation}\n\\hat{Y_i} = a + b_1  y_{1,i} + b_2 y_{2,i} + \\dots{}\n(\\#eq:predlong)\n\\end{equation}\\]\nLinear models we have seen\nWe have recently extended our single factor linear models (e.g. one and two sample t-tests, ANOVAs, regression, etc.) to include more factors:\nHere we look at these in more detail. Specifically, we consider",
    "crumbs": [
      "23. Interactions"
    ]
  },
  {
    "objectID": "book_sections/interactions.html#review-of-linear-models",
    "href": "book_sections/interactions.html#review-of-linear-models",
    "title": "23. Interactions",
    "section": "",
    "text": "We could include a linear term and its squared value to predict a quantitative outcome in a polynomial regression. \\(\\widehat{Y_{i}} = a + b_{1,i} \\times y_1 +  b_{2,i} \\times y_1^2\\)…\n\nA linear model in which we predict a quantitative response as a function of two categorical variables (two factor ANOVA without an interaction)\n\nA linear model in which we predict a quantitative response as a function of one continuous variable that we want o account for (aka a covariate) before considering the effect of a categorical predictor (ANCOVA).\n\n\n\nCases in which the response is predicted by an interaction between explanatory variables\n\nDifferent ways to attribute sums of squares, depending on your goals / motivation.",
    "crumbs": [
      "23. Interactions"
    ]
  },
  {
    "objectID": "book_sections/interactions.html#statistical-interactions",
    "href": "book_sections/interactions.html#statistical-interactions",
    "title": "23. Interactions",
    "section": "Statistical interactions",
    "text": "Statistical interactions\n\n\n\n\n\n\n\n\n\nI like orange juice in the morning. I also like the taste of minty fresh toothpaste. But drinking orange juice after brushing my teeth tastes terrible.\nThis is an example of an interaction.\n\nOn its own, toothpaste tastes good.\nOn its own OJ is even better.\nPut them together, and you have something gross.\n\nThis is an extreme case. More broadly, an interaction is any case in which the slope of two lines differ – that is to say when the effect of one variable on an outcome depends on the value of another variable.\n\n\n\n\n\n\n\n\n\nAnother example of an interaction Getting intoxicated is a bit dangerous. Driving is a bit dangerous. Driving while intoxicated is more dangerous than adding these up individually.\n\nVisualizing main & interactive effects\n\n\n\n\n\n\n\n\n\nThere are many possible outcomes when looking into a model with two predictors and the potential for an interaction. I outline a few extreme possibilities, plotted on the right.\n\nWe can see an effect of only variable A on Y. That is, lines can have nonzero slopes (“Main effect of A”).\n\nWe can see a effect of only variable B on Y. That is, lines can have zero slopes but differing intercepts (“Main effect of B”).\n\nWe can see an effect of only variable B on Y, but an interaction between that variable and the other. That is, intercepts and slopes can differ (or vice versa) in such a way that the mean of Y only differs by one of the explanatory variables (e.g. “Main effect of B, Interaction between A & B”). and/or\n\nWe can have only an interaction. That is, on their own values of A or B have no predictive power, but together they do. (different slopes)",
    "crumbs": [
      "23. Interactions"
    ]
  },
  {
    "objectID": "book_sections/interactions.html#interaction-case-study",
    "href": "book_sections/interactions.html#interaction-case-study",
    "title": "23. Interactions",
    "section": "Interaction case study",
    "text": "Interaction case study\n\n\n\n\n\n\n\n\n\nFemales of the yellow dung fly, Scathophaga atercoraria, mate with multiple males and the sperm of different males “compete” to fertilize her eggs.\nWhat determines whether a sperm is competitive? To find out, Hosken et al. (2002) tested if/how the origin (from UK or Switzerland) of males and females (and their interaction) influence the percentage of offspring sired by the second male.\nSo our model is:\n\\[\\text{SIRING } 2^\\text{ND}\\text{ MALE} = \\text{FEMALE} + \\text{MALE} + \\text{FEMALE} \\times \\text{MALE}\\]\n\nBiological hypotheses\nThere are a few possibilities.\n\nPerhaps females from the UK populations reject (or accept) more sperm from the second male than do females from Sweden (main effect of female population).\n\nPerhaps sperm from UK males have more (or less) siring success than sperm from UK males second male (main effect of male population).\n\nOr maybe, sperm from Swedish males has high siring success with Swedish females, and sperm from UK males has high siring success with UK females (interaction), suggesting harmonious co-adaptation.\n\nOr maybe females have evolved to resist local sperm so sperm from Swedish males has high siring success with UK females but low siring success with Swedish females, suggesting a conflict between the sexes.\n\netc… etc…\n\n\nThe data\nThe raw data are available here if you want to follow along.\n\n\nBefore we analyze it, I am confessing to changing it for teaching purposes. Specifically I am taking the first data point and saying it came from a mating between a UK female and a swiss male.\n\ndung &lt;- tibble(female = c(\"uk\",rep(c(\"swiss\",\"swiss\",\"uk\",\"uk\"), each= 15)[-1]),\n               male = rep(c(\"uk\",\"swiss\",\"uk\",\"swiss\"), each= 15),\n               sire.2nd.male =  c(74, 77, 72, 81, 75, 76, 78, 68, 81, 74, 71, 58,  \n                  67, 63, 83, 74, 63, 51, 63, 69, 61, 34, 51, 62, 61, 69, 59, 72,  \n                  51, 56, 58, 73, 88, 80, 67, 65, 95, 72, 75, 69, 70, 56, 66, 60,\n                  81, 92, 96, 98, 99, 85, 84, 93, 75, 93, 99, 92, 93, 93, 100, 97))\n\nLooking at the means and standard errors below, we see that\n\nUK males have similar mating success with both UK and Swedish females.\nSwedish males seem to have remarkably low success with Swedish females and remarkably high success with UK females. This suggests a history of conflict between the sexes over the success of the second male.\n\n\n\n\nMean (se) male siring success by female population of origin\n\n\nfemale\nswiss male\nuk male\n\n\n\n\nswiss\n59.73 (2.6)\n73.14 (1.9)\n\n\nuk\n92.6 (1.7)\n71.81 (2.6)\n\n\n\n\n\n\n\nFitting a linear model with an interaction in R\nIn R we can designate an interaction in addition to main effects with a colon, :. Or we can have R do a full model with a *. So the two models below are identical\n\nlm_dung_fmi &lt;- lm(sire.2nd.male ~ female + male + female:male, dung)\nlm_dung_fmi &lt;- lm(sire.2nd.male ~ female*male , dung)\nbroom::tidy(lm_dung_fmi)\n\n# A tibble: 4 × 5\n  term            estimate std.error statistic  p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)         59.7      2.30     26.0  6.21e-33\n2 femaleuk            32.9      3.25     10.1  3.05e-14\n3 maleuk              13.4      3.31      4.05 1.57e- 4\n4 femaleuk:maleuk    -34.2      4.60     -7.43 6.70e-10\n\n\nAs usual, we can look at our estimates and uncertainty in them with the summary.lm() or equivalently, the tidy() function. As like most linear model we know that these p and t-values (which shows up at statistic in tidy()) do not necessarily describe things we care about, and may be misleading.\nStill, we can use this output to make predictions. The first terms should look familiar. For the final term femaleuk:maleuk we multiply femaleuk (0 or 1), by maleuk (0 or 1) as the thing we multiply this effect by. That is, when the male and female are from the uk this is 1, and second males in this cross have 34.2% less siring success than we would predict if we added up the effect of male uk and female uk. Because multiplying femaleuk (0 or 1), by maleuk (0 or 1) is zero in all other cases, it only shows up in this one cross.\nWe can see this in the design matrix (below). Remember for the linear algebra fans that we make predictions as the dot product of our model coefficients and the design matrix.\n\nmodel.matrix(lm_dung_fmi)                                                                                                        %&gt;%  DT::datatable(options = list(autoWidth = TRUE,pageLength = 5, lengthMenu = c(5, 25, 50)))\n\n\n\n\n\nOr, if you don’t want to mess with linear algeba, the model predicts siring success of the second male as follows\n\\[\\begin{equation}\n\\begin{split}\n                                               & \\text{intercept}   &\\text{female UK?}&\\text{male UK?}&\\text{both UK?}&\\widehat{Y_i}\\\\\n\\widehat{Y}_\\text{female.swiss x male.swiss} =  &59.733 \\times 1+  &32.867 \\times  0 + &13.410 \\times 0 -&34.197\\times 0  =&59.733\\\\\n\\widehat{Y}_\\text{female.swiss x male.uk} =     &59.733 \\times 1+  &32.867 \\times  0 + &13.410 \\times 1 -&34.197\\times 0  =&73.14\\\\\n\\widehat{Y}_\\text{female.uk    x male.swiss} =  &59.733 \\times 1+  &32.867 \\times  1 + &13.410 \\times 0 -&34.197\\times 0  = &92.60\\\\\n\\widehat{Y}_\\text{female.uk    x male.uk} =     &59.733 \\times 1+  &32.867 \\times  1 + &13.410 \\times 1 -&34.197\\times 1  = &71.83\\\\\n\\end{split}\n\\end{equation}\\]\nWe can see that the model predictions simply describe our four sample means.",
    "crumbs": [
      "23. Interactions"
    ]
  },
  {
    "objectID": "book_sections/interactions.html#hypothesis-testing",
    "href": "book_sections/interactions.html#hypothesis-testing",
    "title": "23. Interactions",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\n\nStatistical hypotheses\nWe can translate the biological hypotheses into three pairs of statistical null and alternative hypotheses.\nWe examine three null and alternative hypotheses\n\nMain effect of Male\n\n\\(H_0:\\) Siring success is independent of male origin.\n\n\\(H_A:\\) Siring success differs by male origin.\n\n\nMain effect of Female\n\n\\(H_0:\\) Siring success is independent of female origin.\n\n\\(H_A:\\) Siring success differs by female origin.\n\n\nInteraction between Male and Female\n\n\\(H_0:\\) The influence of male origin on siring success does not differ by female origin.\n\n\\(H_0:\\) The influence of male origin on siring success is depends on female origin.\n\n\nWe visualize these hypotheses below\n\n\n\n\n\n\n\n\n\n\n\nEvaluating assumptions\nRemember that linear models assume\n\nLinearity: That observations are appropriately modeled by adding up all predictions in our equation.\n\nHomoscedasticity: The variance of residuals is independent of the predicted value, \\(\\hat{Y_i}\\) is the same for any value of X.\n\nIndependence: Observations are independent of each other (aside from the predictors in the model).\n\nNormality: That residual values are normally distributed.\n\nData are collected without bias as usual.\n\nTaking a look at our diagnostic plots, it looks like data meet test assumptions:\n\nautoplot(lm_dung_fmi,nrow = 1, which =c(2,3,5), label = FALSE)\n\n\n\n\n\n\n\n\n\n\nHypothesis testing in an ANOVA framework: Types of Sums of Squares\ntl/dr\n\nWe need to come up with fair ways to attribute variance in larger linear models.\n\nType I Sums of Squares (calculated with the anova() function) are most appropriate when we want to take a “covariate into account”. The order we enter terms into our model can change p and f values with Type I sums of Squares.\n\nType II Sums of Squares are most appropriate when we are equally interested in all main effects as interesting hypotheses.\n\nType III Sums of Squares are most appropriate when we are equally interested in all main and interaction effects as interesting hypotheses. want to take a “covariate into account”.\n\n\nWe can specify which type of sums of squares we want to use with the Anova() function in the car package.\n\nIf our study is “balanced” type I and type II sums of squares will give identical answers.\n\nAll of these can go a bit weird in different cases, especially with unbalanced designs.\n\nFor our case, a Type III Sum of Squares is probably most approriate.\n\nType I and Type II Sums of Squares\nIn the previous chapter, we calculated Type I and Type II sums of squares, and saw that Type I Sums of Square – in which the order of the variables changed our significance claims – are rarely a good idea.\nRecal that with Type II sums of squares, we found \\(SS_\\text{focal variable}\\) as \\(SS_\\text{full model}- SS_\\text{full model except focal variable}\\). If we are also modlling an interaction, we calculate type II sums of sqaures as:\n\\[SS_\\text{focal variable} = SS_\\text{model with all main effects} - SS_\\text{model with all main effects except the focal variable}\\]\nThen, after calculating sums of squares for the main effects, we calculate the sums of squares for the interaction as:\n\\[SS_\\text{interaction} = SS_\\text{full model with focal interaction} - SS_\\text{full model without focal interaction}\\]\nWe can calculate any type of sums of squares with the Anova() function in the car package.\n\nlibrary(car)\nAnova(lm_dung_fmi, type = \"II\")\n\nAnova Table (Type II tests)\n\nResponse: sire.2nd.male\n            Sum Sq Df F value    Pr(&gt;F)    \nfemale      3739.2  1 47.1967 5.674e-09 ***\nmale         271.9  1  3.4324    0.0692 .  \nfemale:male 4375.6  1 55.2292 6.695e-10 ***\nResiduals   4436.7 56                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThere are two weird things about Type II Sums of squares:\n\nThey don’t sum to total sums of squares.\n\nIt’s a bit weird to talk about the significance of a main effect without considering an interaction.\n\n\n\nType III Sums of Squares\nIf we care about an interaction and main effects it makes sense to not leave interactions for last. In this case, we calculate Type III Sums of Squares for each variable and interaction by comparing the predictions with the full model to predictions with everything but the variable (or interaction) we’re interested in. Again we can do this with the Anova() function in the car package. Again, order won’t matter.\n\nAnova(lm_dung_fmi, type = \"III\")\n\nAnova Table (Type III tests)\n\nResponse: sire.2nd.male\n            Sum Sq Df F value    Pr(&gt;F)    \n(Intercept)  53521  1 675.545 &lt; 2.2e-16 ***\nfemale        8102  1 102.259 3.046e-14 ***\nmale          1302  1  16.435  0.000157 ***\nfemale:male   4376  1  55.229 6.695e-10 ***\nResiduals     4437 56                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nType II or Type III Sums of Squares\nI find it difficult to think about which type of sums of squares to use. In general, I prefer to use Type II sums of squares when interactions are subtle, and Type III when interactions are strong enough that main effects become difficult to interpret or explain.",
    "crumbs": [
      "23. Interactions"
    ]
  },
  {
    "objectID": "book_sections/interactions.html#biological-coclusions-for-case-study",
    "href": "book_sections/interactions.html#biological-coclusions-for-case-study",
    "title": "23. Interactions",
    "section": "Biological coclusions for case study",
    "text": "Biological coclusions for case study\nWe conclude that female (F = 102.259, df = 1, p = \\(3\\times 10e^{-14}\\)) and male (F = 16.4, df = 1, p = 0.00016) population of origin, and their interaction (F = 55.2, df =1, p = \\(6.7 \\times 10^{-10}\\)) all impact the siring success of the second male. Most notably, the interaction between male and female population of origin seemed to matter substantially – when females and second males where both from the UK siring success was way lower that expected given each main effect. This suggests that sexual conflict in the UK might modulate the competitiveness of second sperm. Future work should address why the siring success of second males from Sweden was not notably impacted by maternal population of origin.",
    "crumbs": [
      "23. Interactions"
    ]
  },
  {
    "objectID": "book_sections/interactions.html#quiz",
    "href": "book_sections/interactions.html#quiz",
    "title": "23. Interactions",
    "section": "Quiz",
    "text": "Quiz\n\n\n\n\n\n\n\n\nHosken, D. J., Blanckenhorn, W. U., & Garner, T. W. J. (2002). Heteropopulation males have a fertilization advantage during sperm competition in the yellow dung fly (scathophaga stercoraria). Proc. R. Soc. Lond. B, 269, 1701–1707.",
    "crumbs": [
      "23. Interactions"
    ]
  },
  {
    "objectID": "book_sections/model_evaluation.html",
    "href": "book_sections/model_evaluation.html",
    "title": "24. Model Evaluation",
    "section": "",
    "text": "Goals of model building\nBiology is messy and complex. Rarely can we fully grasp the workings of any biological phenomenon. Our brains need help to navigate this complexity. A simplified mathematical or conceptual representation of a biological system, known as a “model”, can simplify complex systems to allow us to understand them. Good models highlight key relationships, test hypotheses, or make predictions from a few key features of a system. By focusing on the most relevant factors and ignoring extraneous details, models allow us to simplify complex biological systems into things we can understand.\nFor example, statistical models (e.g. regression) can show how a trait (e.g. body size) is influenced by environmental factors (e.g. calories consumed per day), while a biological model like a Holliday junction helps us visualize the molecular mechanics of DNA recombination. Neither of these models completely describes reality, but I cannot get my head inside a developing organism or the process of meiosis, so these models are a good start.\nBut what are the “most relevant factors” to focus on, and what “extraneous details” can we ignore? To a large extent, this depends on the type of model we are building and what we want the model to achieve. Models are tools, and just like trying to bang in a nail with a saw would be ineffective and dangerous, the wrong model can mislead and confuse.\nWhile there are some guidelines for what makes a good or bad model, the things we care about most in our statistical models can depend on which of these missions we are undertaking.",
    "crumbs": [
      "24. Model Evaluation"
    ]
  },
  {
    "objectID": "book_sections/model_evaluation.html#model-fit-vs-model-adequacy",
    "href": "book_sections/model_evaluation.html#model-fit-vs-model-adequacy",
    "title": "24. Model Evaluation",
    "section": "Model fit vs model adequacy",
    "text": "Model fit vs model adequacy\nThere are three rough ways we can think about answering the question “Is my model any good?” We can ask\n\n“Does my model fit the data well?”\n\n“Is my model an adequate description of the process?”\n\n“Are my conclusions from null hypothesis significance testing and/or my uncertainty estimates justified?”\n\n\n\nI won’t focus on “Are my conclusions from null hypothesis significance testing and/or my uncertainty estimates justified?” here. You can evaluate this question by permutation (for nhst) and bootstrapping (for estimating uncertainty).\n\nVisualize your data and your model!\nThe first step in evaluating the quality of your model is to visually inspect it along with the raw data. Your eyes are often better at detecting unusual patterns than most calculations!\nFor example, suppose you want to determine whether sepal width reliably predicts petal length in the iris dataset. You might observe a significantly negative association: as sepal width increases, petal length decreases (\\(r^2 = 0.18\\)). Not what I would have expected, but ok…\n\nlm(Petal.Length ~ Sepal.Width, data = iris) %&gt;%\n  tidy()                                                                                                                                  %&gt;% mutate_at(2:4, round,digits=3)%&gt;% mutate(p.value = p.value* c(10^17,10^8), p.value =round(p.value, digits = 2), p.value = paste(p.value,c(\"x 10^-17\", \"x 10^-8\")))%&gt;% gt()\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n9.063\n0.929\n9.757\n1.13 x 10^-17\n\n\nSepal.Width\n-1.735\n0.301\n-5.768\n4.51 x 10^-8\n\n\n\n\n\n\n\nHowever, looking at the data reveals that this model is pretty bad. As you can see in Figure 1.1. The overall negative relationship is driven by differences between three clusters (which we know to correspond to species). Within each cluster, the relationship appears positive.\n\n\n\n\n\n\n\n\nFigure 1.1: Showing the data helps us evaluate model fit.\n\n\n\n\n\n\n\nSimulation to help the eye\nOur eyes and brains are often quite good at spotting unusual patterns in plots. However, they can sometimes over-interpret what they see. Simple simulations are a great way to train ourselves to recognize normal variability, such as that arising from sampling error. The simplest simulation models observations as their expected values from the model plus random noise, based on the variability in the residuals.\nAs shown in Figure 1.2, the patterns in the real data differ noticeably from the patterns in three simulated datasets generated from the model.\n\nlm(Petal.Length ~ Sepal.Width, data = iris) %&gt;% \n  augment() %&gt;%\n  mutate(\n    sim_petal_length_a = rnorm(n(), mean = .fitted, sd = sd(.resid)),\n    sim_petal_length_b = rnorm(n(), mean = .fitted, sd = sd(.resid)),\n    sim_petal_length_c = rnorm(n(), mean = .fitted, sd = sd(.resid))\n  ) %&gt;%\n  ggplot(aes(x = Sepal.Width, y = Petal.Length)) +\n  geom_smooth(method = 'lm', se = FALSE) +\n  geom_point(size = 2, alpha = 0.8) +\n  geom_point(aes(y = sim_petal_length_a), color = \"red\", alpha = 0.3) +\n  geom_point(aes(y = sim_petal_length_b), color = \"pink\", alpha = 0.3) +\n  geom_point(aes(y = sim_petal_length_c), color = \"firebrick\", alpha = 0.3) +\n  theme(axis.title = element_text(size = 15),axis.text = element_text(size = 12))\n\n\n\n\n\n\n\nFigure 1.2: Simulation helps us evaluate model fit. Real data are shown with large black points; data from three simulations are shown in reddish colors.\n\n\n\n\n\nBy contrast, the data and simulations look more similar when we include species in our model (Figure 1.3).\n\n\n\n\n\n\n\n\nFigure 1.3: Simulation helps us evaluate model fit. Real data are shown with large black points; data from three simulations are shown in reddish colors.\n\n\n\n\n\n\n\nMultivariate visualizations\nThe visualizations above can be pretty helpful, but we are most often worried about the fit and performance of more complex models with numerous predictors. What to do then? One approach is to use residual plots to check for patterns in the unexplained variation. For example, you can create residual plots (i.e. residuals are on the y and a focal predictor is on the x) for each predictor in your model tocheck for non-linear trends and/or.\nFor example Figure 1.4 clearly shows the difference in variance across species in our residuals from model predictions of petal length as a function of sepal width and species.\n\na &lt;- lm(Petal.Length ~ Species + Sepal.Width, data = iris) %&gt;% \n  augment()%&gt;%\n  ggplot(aes(x = Sepal.Width, y = .resid))+\n  geom_point()+\n  theme(axis.title = element_text(size = 15), axis.text = element_text(size = 12))\n\nb &lt;- lm(Petal.Length ~ Species + Sepal.Width, data = iris) %&gt;% \n  augment()%&gt;%\n  ggplot(aes(x = Species, y = .resid))+\n  geom_sina() +\n  theme(axis.title = element_text(size = 15), axis.text = element_text(size = 12))\n\na+b +plot_annotation(tag_levels = \"A\")\n\n\n\n\n\n\n\nFigure 1.4: Residual plots can help us evaluate assumptions.",
    "crumbs": [
      "24. Model Evaluation"
    ]
  },
  {
    "objectID": "book_sections/study_design.html",
    "href": "book_sections/study_design.html",
    "title": "25. Study Design",
    "section": "",
    "text": "Review / Set Up\nSo far we have focused on two goals of statistics:\nThese are reasonable aims. But too much focus on significance testing and uncertainty can blur the bigger picture. Our statistical expertise and efforts are wasted if we don’t critically evaluate the implications of a study. This brings us to our third – and in my view, the most important – goal of statistics:\nAs we work through this goal, we will transition from dealing with other people’s data to designing our own scientific studies. There are two excellent reasons for this: (1) Many of you will conduct some form of scientific study during your career. (2) Even if you never conduct a scientific study, learning how to design a good study helps you understand both the reasoning behind and the shortcomings of scientific studies and claims.",
    "crumbs": [
      "25. Study Design"
    ]
  },
  {
    "objectID": "book_sections/study_design.html#experiments-one-weird-trick-to-infer-causation",
    "href": "book_sections/study_design.html#experiments-one-weird-trick-to-infer-causation",
    "title": "25. Study Design",
    "section": "Experiments: One weird trick to infer causation",
    "text": "Experiments: One weird trick to infer causation\nBy randomly assigning individuals to treatment groups, experiments can provide evidence of causation, because randomization breaks any link between the treatment and other variables that might otherwise confound the results.\nOf course, like all of statistics, an experiment represents a single sample. So we must be aware of potential issues regarding sampling bias and sampling error, and plan accordingly.\n\nPotential Biases in Experiments\nPoorly executed experiments can introduce bias. This is bad because the whole point of an experiment is to remove confounds and bias. Here are some common ways in which experiments can introduce bias and how to avoid these issues\n\nTime heals Whenever I feel terribly sick, I call the doctor, and usually get an appointment the following week. Most of the time I get better before seeing the doctor. I therefore joke that the best way for me to get better is to schedule a doctor appointment. Of course, calling the Doctor didn’t heal me. Rather I called the Doctor when I felt my worst, and got better with time. This is because we tend to get better. \n“Regression to the mean” The most extreme observations in a study are biased estimates of the true parameter values. That’s because being exceptional requires both an expectation of being exceptional (extreme \\(\\widehat{Y_i}\\)) and a residual in the same direction (a large positive residual for high values or a large negative residual for low ones). Because extreme values partly reflect random noise, that noise is unlikely to repeat in the same direction on a second measurement. As a result, extreme observations tend to move closer to the average, even without any experimental intervention. \nExperimental artifacts The experimental manipulation itself, rather than the treatment we care about could be causing the experimental result. Says we hypothesize that birds are attracted to red, so we glue red feathers onto some birds and see that that increases their mating success. We want to make sure that it is the red color, not the glue or added feathers that drives this elevated attractiveness. \nKnown treatments are a special kind of experimental artifact. Knowledge of the treatment by either the experimental subject or the experimenter, can introduce a bias. For example, people who think they have gotten treated might prime themselves for improvement. Processes like these are known as a placebo effect. Or, if the researchers know the treatment they may subconsciously bias their measurements or the way the treat their subjects. \n\n\n\nListen to the 7 minute clip from radiolab, below for examples of the placebo and how it may work. \n\n\n\n\n\n\n\nEliminating Bias in Experiments: To minimize bias and allow for causal interpretation of experimental results, strong experimental designs usually include\n\nIntroducing effective controls It’s usually a good idea to have a “do nothing” treatment as a control, but this is not enough. We should also include a “sham treatment” or “placebo” is identical to the treatment in every way but the treatment itself. Taking our bird feathers example, we would introduce a treatment in which we glued blue feathers, and maybe ne where we glued on black feathers, to ensure the color red was responsible for the elevated attractiveness observed in our study. \n“Blinding” If possible, we should do all we can to ensure that neither the experimenter nor the subjet knows which treatment they received. \n\n\nWatch this video, on controls, placebos and blinding in experimental design from Crash Course Statistics, below\n\n\n\n\n\n\nSampling error in experiments\nSampling bias isn’t our only consideration when planning a study, we would also like to increase our precision by decreasing sampling error.\nRecall that sampling error is the chance deviation between an estimate and its true parameter which arises because of the process of sampling, and that we can summarize this as the standard deviation of the sampling distribution (aka the standard error). The standard error is something like the standard deviation divided by the square root of the sample size, so we can minimize sampling error by:\n\nIncreasing the sample size (and avoid pseudoreplication)\n\nDecreasing variability\n\n\nIncrease the sample size and avoid pseudo-replication\nWell of course! We learned this as the law of large numbers. But beware the samples you add should be independent, otherwise you will have “pseudo replicated” your study.\nPseudo-replication occurs when we treat non-independent samples as if they were independent replicates. For example, if we fertilize one garden and leave another unfertilized, then measure many plants within each, we might be tempted to treat each plant as a replicate (top of Figure 1.2). However, these plants share the same soil, light, and microclimate — so they are not independent. The true sample size is the number of gardens, not the number of plants. Because pseudo-replication inflates our apparent sample size, it leads to artificially small standard errors and an excess of false positives.\nTo avoid pseudo-replication:\n\nHave the unit of analysis matches the unit of randomization (e.g., the garden, not the plants within it, if treatments are applied at the garden level).\nRandomize treatments across independent experimental units (bottom of Figure 1.2).\nReplicate at the level of the treatment (when possible), not just within it. E.g., several fertilized and unfertilized gardens, not just several plants per garden.\n\n\n\n\n\n\n\n\n\nFigure 1.2: Comparing pseudo-replication (top) to independent replication (bottom). With pseudo-replication the effect of our experimental intervention (in this case fertilizer), is tied up with differences between environments unrelated to treatment (in this case, differences between gardens). Separating treatment and garden, and randomly placing treatments onto garden removes this issue.\n\n\n\n\n\n\n\nBe sure to read Interleaf 2 on pseudo-replication from Whitlock & Schluter (2020).\n\n\n\n\n\n\n\n\nFigure 1.3\n\n\n\n\n\n\n\nDecrease (external) variability.\nWe want to minimize the variability unrelated to our treatment, but how? Well, we only have so much control of this because some variability is natural. Still there are things we can do to increase the precision of our estimated effect and minimize extraneous variation.\n\nMore precise measurements More careful counting, fancier machines etc etc can provide more accurate measurements for each individual in our sample and eliminating this extraneous variability should decrease variability in our sample.\n\n\n\n\n\n\n\n\n\n\n\nBalanced design Balance refers to the similarity in sample sizes for each treatment. Recall that \\(SE_{\\overline{Y_1}-\\overline{Y}_2} = \\sqrt{s_p^2(\\frac{1}{n_1}+\\frac{1}{n_2})}\\). So, for a fixed total sample size, more balanced experiments decrease uncertainty mean differences between treatments. \nMatching / Blocking A standard experimental design is two randomize who gets the treatment and who get the control. But we can do even better than that!!! We have seen that a paired t-test gets extra power by comparing natural pairs who are alike in many ways except the treatment. This design decreases variability in our estimated difference because in each pairs we minimize extraneous variation unrelated to treatment. We can scale that principle up to larger studies ANOVAs etc….",
    "crumbs": [
      "25. Study Design"
    ]
  },
  {
    "objectID": "book_sections/study_design.html#planning-for-power-and-precision",
    "href": "book_sections/study_design.html#planning-for-power-and-precision",
    "title": "25. Study Design",
    "section": "Planning for power and precision",
    "text": "Planning for power and precision\nWe could maximize our power and precision by having an infinitely large sample, but this is obviously silly. We’d be wasting a bunch of time and resources over-doing one study and will miss out on so many others. So, how do we plan a study that is big enough to satisfy our goals, without overdoing it?\nWe need to think about the effect size we care about and the likely natural variability\n\nHow precise of an estimate do we need for our biological story? In some cases, small difference mean a lot, in other cases, ballpark estimates are good enough. It’s your job as the researcher to know what is needed for your question.\n\nWhat effect size is worth knowing about? The null hypothesis is basically always false. With an infinite sample size, we’d probably always reject it. But in many of these cases, the true population parameter is so close to zero, that the null hypothesis might as well be true. Again, it’s your job as a researcher to consider what size of an effect you would like to be able to see.\n\nHow much variability do we expect between measurements. Again, your biological knowledge is required here (or you could consider difference relative to variability when considering precision and effect size of interest)\n\n\nEstimating an appropriate sample size.\nWe use power analyses to plan appropriate sample sizes for a study. A power analysis basically finds the sample size necessary so that the sampling distribution of your experiment has\n\nSome specified power to differentiate between the null model and the smallest effect size that you would like to be able to identify and/or\nProbability of being as or more precise than you want.\n\nThe traditional specified power researchers shot for is 80%, but in my opinion that is quite low and aiming for 90% power seems more reasonable.\nThe are numerous mathematical rules of thumb for power analyses, as well as online plugins e.g. this one from UBC and R packages (pwr is most popular)\n\nThe sample size we start with is rarely the sample size we end with – plants die, people drop out, RNA degrades etc etc. Keep this in mind when designing your experiment, and increase your sample size to accommodate the expected number of lost data points.\n\n\n\nExample with the pwr package\nThe pwr package helps you evlaute power for numerous standard statistical procedures. Say you wanted to design a study that you would analyze with a t-test, and you wanted a ninety percent chance of rejecting the null if the true population value of Cohen’s D was 1.\n\nlibrary(pwr)\npwr.t.test(power = .9, \n         d = 1,  # Effect size (Cohen's d)\n         sig.level = 0.05, # Significance level\n         alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 22.02109\n              d = 1\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nOr say you wanted to know the power you had to reject the null if the true COhen’s D was 2 and you did and experiment with two groups of five samples.\n\nlibrary(pwr)\npwr.t.test(n = 5, \n         d = 2,  # Effect size (Cohen's d)\n         sig.level = 0.05, # Significance level\n         alternative = \"two.sided\")%&gt;%\n  tidy()%&gt;%\n  mutate(n = 5, g = 2, sig_level = 0.05)                                                                                                                           %&gt;% mutate(power = round(power, digits = 3))%&gt;%gt()\n\n\n\n\n\n\n\nn\nsig.level\npower\ng\nsig_level\n\n\n\n\n5\n0.05\n0.791\n2\n0.05\n\n\n\n\n\n\n\nSee Chapter 20: Chapter 20 Sample Size Calculations with {pwr} from Higgins (2024) for more.\n\n\n\n\n\n\nUnfold this section to learn more about simulating to estimate power and precision (optional)\n\n\n\n\n\nOften experimental design is more complex than the off-shelf options in the pwr package. Of course, we could try to find a package better suited to our study, but sometime we will fail. Here I focus on one way we can estimate power and precision – we can simulate!!! There is a bit on new R in here, including writing functions. Enjoy if you like, skim if you don’t care. I also note that there are more efficient ways to code this in R. I ca provide examples if there is enough demand.\nLet’s first write our own function to\n\nSimulate data from two populations with a true mean difference of x (the minimal value we care about) and a standard deviation of s from a normal distribution.\n\nRun a two sample t.test.\n\n\nsimTest &lt;- function(n1, n2, x, s){\n  sim_id  &lt;- runif(1) # picka random id, in case you want it\n  sim_dat &lt;- tibble(treatment   = rep(c(\"a\",\"b\"), times = c(n1, n2)),\n                    exected_val = case_when(treatment == \"a\" ~ 0,\n                                            treatment == \"b\" ~ x)) %&gt;%\n    mutate(sim_val     = rnorm(n = n(),mean = exected_val, sd = s))\n  tidy_sim_lm &lt;- lm(sim_val ~ treatment, data = sim_dat) %&gt;%\n    broom::tidy() %&gt;%\n    mutate(n1 = n1, n2 = n2, x = x, s = s, sim_id = sim_id)\n  return(tidy_sim_lm)\n}\n\nWe can see the outcome of one random experiment, with a sd of 2, and a difference of interest equal to one, and a sample size of twenty for each treatment.\n\none_sim &lt;- simTest(n1 = 20, n2 = 20, x = 1, s = 2)\none_sim                                                                                                                                                               %&gt;% mutate_if(is.numeric,round,digits = 4) %&gt;%DT::datatable( options = list( scrollX='400px'))\n\n\n\n\n\nWe probably want to filter for just treatmentb, because we don’t care about the intercept\n\nfilter(one_sim, term == \"treatmentb\")\n\n# A tibble: 1 × 10\n  term       estimate std.error statistic p.value    n1    n2     x     s sim_id\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 treatmentb   0.0138     0.649    0.0212   0.983    20    20     1     2  0.198\n\n\nWe can replicate this many times\n\nn_reps &lt;- 500\nmany_sims &lt;- replicate(simTest(n1 = 20, n2 = 20, x = 1, s = 2), n = n_reps, simplify = FALSE) %&gt;%\n  bind_rows() %&gt;% # shoving output togther \n  filter(term == \"treatmentb\")\n\nmany_sims                                                                                                                                                      %&gt;%                                               mutate_if(is.numeric,round,digits = 4) %&gt;%DT::datatable( options = list(pageLength = 5, lengthMenu = c(5, 25, 50), scrollX='400px'))\n\n\n\n\n\nWe can summarize this output to look at our power and the standard deviation, and upport and lower 2.5% quantiles to estimate our precision\n\nmany_sims %&gt;% \n  summarise(power        = mean(p.value &lt; 0.05),\n            mean_est     = mean(estimate),\n            sd_est       = sd(estimate),\n            lower_95_est = quantile(estimate, prob = 0.025),\n            upper_95_est = quantile(estimate, prob = 0.975))\n\n# A tibble: 1 × 5\n  power mean_est sd_est lower_95_est upper_95_est\n  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 0.378     1.02  0.678       -0.364         2.23\n\n\nWe can turn this last bit into a function and try it for a bunch of sample sizes\n\n\n# A tibble: 4 × 6\n      n power mean_est sd_est lower_95_est upper_95_est\n  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1    10 0.178    1.03   0.900       -0.677         2.81\n2    20 0.3      0.951  0.603       -0.155         2.07\n3    50 0.694    1.00   0.405        0.249         1.88\n4   100 0.946    0.999  0.276        0.427         1.55",
    "crumbs": [
      "25. Study Design"
    ]
  },
  {
    "objectID": "book_sections/study_design.html#inferring-causation-when-we-cant-do-experiments",
    "href": "book_sections/study_design.html#inferring-causation-when-we-cant-do-experiments",
    "title": "25. Study Design",
    "section": "Inferring causation when we can’t do experiments",
    "text": "Inferring causation when we can’t do experiments\nExperiments are our best way to learn about causation, but we can’t always do experiments. Sometimes experiments are unethical, impractical, or impossible. Do we have any hope of inferring causation in these cases? The short answer is – sometimes, if we’re lucky.\nOne good way to make causal claims from observational studies is to find matches, or “natural experiments” in which the variable we cared about changed in one set of cases, but did not in paired cases that are similar in every way except this change. If we cannot make a rigorous causal claim it is best not to pretend we can. Rather, we should honestly describe what we can and cannot infer from our data.\nThe next chapter goes into the problem of “causal inference” in more detail.\n\n\n\n\nHiggins, P. D. R. (2024). Reproducible medical research with r. Bookdown. https://bookdown.org/pdr_higgins/rmrwr/\n\n\nWhitlock, M. C., & Schluter, D. (2020). The analysis of biological data (Third). Macmillan.",
    "crumbs": [
      "25. Study Design"
    ]
  },
  {
    "objectID": "book_sections/causal_inference.html",
    "href": "book_sections/causal_inference.html",
    "title": "26. Cause",
    "section": "",
    "text": "What is a Cause?\nLike much of statistics, understanding causation requires a healthy dose of imagination. Imagine (as we did in the Crash Course Statistics video laste chapter) a series of parallel worlds: in one, a specific treatment is applied (e.g., drinking coffee, receiving a vaccine, raising taxes), and in another, it is not. By observing how an outcome of interest responds in each world, we can start to see whether the treatment influences that outcome. We say that a treatment causes the outcome if changing it would alter the outcome, on average.\nThis concept of evaluating what would happen if we had changed a treatment is known as counter-factual thinking, and it is the foundation of reasoning about causation. In the previous chapter, we showed that experiments best approximate this “parallel world” model as individuals are randomly assigned to treatment or control group. In this chapter, we consider the best practices in inferring causation when we cannot do experiments.",
    "crumbs": [
      "26. Cause"
    ]
  },
  {
    "objectID": "book_sections/causal_inference.html#when-correlation-is-not-good-enough",
    "href": "book_sections/causal_inference.html#when-correlation-is-not-good-enough",
    "title": "26. Cause",
    "section": "When Correlation Is (Not) Good Enough",
    "text": "When Correlation Is (Not) Good Enough\nWe don’t always need to know causation. We can build a multiple regression model and use it to predict outcomes without needing to know the underlying causes. For example, if I want to buy good corn, I can go to a farm stand that reliably sells tasty corn without needing to know if its quality comes from the soil, sunlight, or the farmers playing Taylor Swift each morning to “excite” the corn. Similarly, if I were selling insurance, I would only need to predict who is likely to develop lung cancer, not the underlying cause.\nWe need to understand causation when we want to intervene (or make causal claims). If I want to grow my own tasty corn, I’d need to know what specifically makes the farm’s corn delicious. No need to fertilize the soil if corn just need some T-Swizzle tunes. Similarly, I would need to know that smoking causes cancer to credibly recommend that people quit smoking to reduce their risk of lung cancer.\nThis chapter provides us with the tools to bring causal thinking into statistical analyses.",
    "crumbs": [
      "26. Cause"
    ]
  },
  {
    "objectID": "book_sections/causal_inference.html#dags-confounds-and-experiments",
    "href": "book_sections/causal_inference.html#dags-confounds-and-experiments",
    "title": "26. Cause",
    "section": "DAGs, confounds, and experiments",
    "text": "DAGs, confounds, and experiments\n\nTHE curious associations with lung cancer found in relation to smoking habits do not, in the minds of some of us, lend themselves easily to the simple conclusion that the products of combustion reaching the surface of the bronchus induce, though after a long interval, the development of a cancer. If, for example, it were possible to infer that smoking cigarettes is a cause of this disease, it would equally be possible to infer on exactly similar grounds that inhaling cigarette smoke was a practice of considerable prophylactic value in preventing the disease, for the practice of inhaling is rarer among patients with cancer of the lung than with others.\nSuch results suggest that an error has been made,of an old kind, in arguing from correlation to causation, and that the possibility should be explored that the different smoking classes, non-smokers, cigarette smokers, cigar smokers, pipe smokers, etc., have adopted their habits partly by reason of their personal temperaments and dispositions, and are not lightly to be assumed to be equivalent in their genotypic composition. Such differences in genetic make-up between these classes would naturally be associated with differences of disease incidence without the disease being causally connected with smoking. It would then seem not so paradoxical that the stronger fumes of pipes or cigars should be so much less associated with cancer than those of cigarettes, or that the practice of drawing cigarette smoke in bulk into the lung should have apparently a protective effect.\n“Cancer and Smoking” Fisher (1958)\n\n\n\n\n\n\n\n\n\nFigure 1.1: R.A. Fisher—a pipe enthusiast, notorious asshole, eugenicist, and widely regarded as the father of modern statistics and population genetics.\n\n\n\n\n\nIt is now well established that cigarette smoking causes cancer—but this wasn’t always the consensus. One of the most prominent statisticians in history, R.A. Fisher, famously argued that the observed relationship between smoking and cancer was spurious, insisting that smoking did not cause cancer. The extent to which his flawed logic was influenced by his own fondness for smoking, financial ties to the tobacco industry, or simple skepticism is unclear (Stolley (1991)). What is clear, however, is that Fisher raised a plausible argument (which was ultimately wquite wrong).\n\nA cause is neither a guarantee nor the only pathway to an outcome. For example, many smokers have not developed lung cancer, and many non-smokers have (because there are multiple ways to develop lung cancer). We can interpret a cause as a thing that – if changed – increases the probability of some outcome.",
    "crumbs": [
      "26. Cause"
    ]
  },
  {
    "objectID": "book_sections/causal_inference.html#randomized-controlled-experiments",
    "href": "book_sections/causal_inference.html#randomized-controlled-experiments",
    "title": "26. Cause",
    "section": "Randomized Controlled Experiments",
    "text": "Randomized Controlled Experiments\nRandomized controlled experiments are our strongest tool for learning about causation. By randomly assigning individuals to treatments, we approximate the parallel worlds we imagined earlier – one in which a treatment is applied and one in which it is not. Differences in outcomes between these groups can be attributed to the treatment itself because, in expectation, all other variables are balanced by randomization.\nFor example, to test whether smoking causes lung cancer, we would randomly assign some people to smoke and others not to. Because genetics and environment would be randomly distributed, any systematic difference in cancer rates could be causally attributed to smoking. Since we cannot do this experiment, we rely on observational data and careful reasoning to identify causes.",
    "crumbs": [
      "26. Cause"
    ]
  },
  {
    "objectID": "book_sections/causal_inference.html#dags",
    "href": "book_sections/causal_inference.html#dags",
    "title": "26. Cause",
    "section": "DAGs",
    "text": "DAGs\nThe Directed Acyclic Graph (hereafter DAG) is a key tool to guide this careful reasoning. We use a DAG to represent causal relationships between variables, as a “visual map” of our assumptions about how variables influence one another. A DAG\n\nEach node represents a variable.\nEach arrow (→) represents a causal influence.\nDirected means arrows point in the direction of cause and effect.\nAcyclic means no arrow loops back to its origin—nothing causes itself.\n\nBy forcing us to carefully laying out our casual story, DAGs can show us where potential biases or confounding might exist. As such DAGs can guide us in deciding which variables we need to include or exclude in an analysis. For example, by looking at a DAG, you should be able to decide which variables to include (or exclude) in a regression model.\n\nSmoking, Cancer, and Competing Causal Stories\nLet’s return to Fisher’s arguments about smoking and lung cancer. We can reframe this debate as a few competing causal models which we can present as DAGs:\n\nA) Smoking causes lung cancer. The simplest model: smoking → cancer (Figure 1.2 A). This represents a front-door causal path. This direct effect does not imply that smoking is the only cause of lung cancer, nor that all smokers will develop lung cancer or that all non-smokers won’t. Rather, it suggests that if we could create a “clone” for each person, with one clone smoking and the other not, we would expect more lung cancer cases among the smoking clones due to this direct effect.\nB) Genetics confounds the relationship. Fisher argued that both smoking and cancer were influenced by a shared genetic predisposition: smoking ← genes → cancer (Figure 1.2 B). This is a back-door path that creates a false correlation between smoking and cancer.\n\nThese are not the only plausible causal models for an association between smoking and cancer. I present three other possibilities in Figure 1.2.\n\nC) Genetics influences smoking, which then causes cancer. A pipe (or “mediator”) model (genes → smoking → cancer) — smoking still causes cancer, but genetics affects exposure (Figure 1.2 C).\nD) Both genetics and smoking contribute to cancer. A collider model (smoking → cancer ← genes), where both arrows “collide” at cancer (Figure 1.2 D).\nE) A more realistic model. Includes multiple interacting causes (e.g., smoking, genetics, environment), showing that biological causation is usually complex (Figure 1.2 E).\n\nDAGs make these structures explicit so we can decide which variables belong—and which don’t—in our statistical models.\n\n\n\n\n\n\n\n\nFigure 1.2: Five DAGs illustrating potential causal structures for the lung cancer example To make this easier to read, I colored the focal outcome (cancer) in red.  (A) Direct causation: smoking directly influences cancer risk.  (B) A confounder. Here, genetic factors influence both smoking behavior and cancer but smoking does not cause cancer. This would create a spurious association if not controlled.  (C) A mediator (pipe). Here genes affect smoking, which then affects cancer, meaning smoking lies on the causal pathway.  (D) A collider. Both genes and smoking independently influence cancer; conditioning on cancer would introduce a false association between genes and smoking.  (E) Complex reality, with genes, environment, and smoking all contributing to cancer risk through multiple pathways.",
    "crumbs": [
      "26. Cause"
    ]
  },
  {
    "objectID": "book_sections/causal_inference.html#set-up",
    "href": "book_sections/causal_inference.html#set-up",
    "title": "26. Cause",
    "section": "Set Up",
    "text": "Set Up\nIn evolutionary studies, fitness is the central metric of interest. Although it is challenging to measure and define directly, we can often assess traits related to it, such as the number of offspring an organism produces. For this example, let’s assume this measure is sufficiently representative of fitness.\nNow, suppose we are studying a species of fish and want to determine if being larger (in terms of length) increases fitness, measured as the number of eggs produced. To make things more interesting, let’s assume that fish inhabit environments whose quality we can measure. For this example, we’ll assume that all measurements are accurate, unbiased, and follow a normal distribution.\n\nCausal Model 1: The Confound\n\n\n\n\n\n\n\n\n\nLet’s begin with a simple confound. Here a good environment leads to larger fish and increased fitness (more eggs), but size itself has no direct effect on fitness. Here’s how we’ll set up the simulation:\nIn this model, fish length has no causal impact on the number of eggs laid. We use the rnorm() function to simulate data from a normal distribution as follows:\n\nOne hundred fish (n=100)\nEach fish grows in environments of varying quality (normally distributed, with a mean, \\(\\mu = 50\\), and standard deviation, \\(\\sigma = 5\\)).\nLength is influenced by environmental quality (predicted length for each fish matches environmental quality, with a standard deviation of two).\nThe number of eggs is also influenced by environmental quality (predicted egg count for each fish is half the environmental quality, with a standard deviation of six, rounded to the nearest integer because fish cannot lay a fractional number of eggs).\n\n\nn_fish          &lt;- 100\nconfounded_fish &lt;- tibble(env_quality = rnorm(n = n_fish, mean = 50, sd = 5), # simulating environment quality\n                          fish_length = rnorm(n = n_fish, mean = env_quality, sd = 2),\n                          fish_eggs   = rnorm(n = n_fish, mean = env_quality / 2, sd = 6) %&gt;% round())\n\nBecause we simulated the data, we know that fish length does not cause fish to lay more eggs. Nonetheless, a plot (Figure 2.1) and a statistical test show a strong association between length and egg count if we exclude environmental quality from the model.\n\nOur statistical analysis will not show cause: Confounds\n\nWrong model 1: Omitting the confound\n\n\n\n\n\n\n\n\n\n\nFigure 2.1: Confounds are confounding.\n\n\n\n\nWe can build a simple linear model predicting the number of fish eggs as a function of fish length. We can see that the prediction is good, and makes sense – egg number reliably increases (0.198) with fish length (slope = 0.465, df = (1,98), F = 24.18, p-value = 0). But we know this is not a causal relationship (because we didn’t have this cause in our simulation).\n\n\n\n\n\n\n\n\nModel with confound only\n\n\nlm(fish_eggs~ fish_length, data = confounded_fish)\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nfish_length\n0.465\n0.095\n4.917\n0\n\n\n\n\n\n\n\n\n\nA Model Including the True Cause\nLet’s say we didn’t have a clear idea of which trait causally increased the number of eggs a fish laid. In this example, it’s plausible (although not necessarily true) to believe that both environmental quality and fish length influence the number of eggs laid. So, let’s build a model that includes both variables, including the confounding variable, environmental quality. Then, we’ll find our estimates and use the appropriate Type II sums of squares to evaluate significance. Finally, we’ll use the emtrends() and confint() functions from the emmeans package to estimate uncertainty in our slope estimates.\n\nlibrary(emmeans)\nfish_lm_w_confound &lt;- lm(fish_eggs ~ env_quality + fish_length, confounded_fish)  \nanova_table        &lt;- Anova(fish_lm_w_confound, type = \"II\")\nenv_qual_est       &lt;- emtrends(fish_lm_w_confound, var = \"env_quality\") %&gt;% confint()\nfish_length_est    &lt;- emtrends(fish_lm_w_confound, var = \"fish_length\") %&gt;% confint()\n\n\n\n\n\n\n\n\n\nANOVA Table: Model with Cause & Confound\n\n\nModel: lm(fish_eggs ~ env_quality + fish_length, data = confounded_fish)\n\n\nterm\nsumsq\ndf\nF_value\nP_value\n\n\n\n\nenv_quality\n4.042\n1\n0.118\n0.732\n\n\nfish_length\n56.334\n1\n1.643\n0.203\n\n\nResiduals\n3324.963\n97\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlope Estimates and Uncertainty: Model with Cause & Confound\n\n\nModel: lm(fish_eggs ~ env_quality + fish_length, data = confounded_fish)\n\n\nvariable\nslope\nSE\ndf\nlower.CL\nupper.CL\n\n\n\n\nenv_quality\n0.113\n0.330\n97\n-0.542\n0.769\n\n\nfish_length\n0.371\n0.289\n97\n-0.203\n0.945\n\n\n\n\n\n\n\n\nThe Good News is that now that we’ve added the true cause, we no longer find that fish_length is statistically associated with the number of eggs laid.\nThe Bad News is that, because we still have the wrong variable in our model, we lose the ability to correctly identify that environmental quality strongly predicts the number of eggs laid.\n\n\nThe results above are from a simulation. If you run this multiple times, you’ll see different outcomes. Sometimes, we will correctly reject the false null hypothesis in favor of the truth—that environmental quality increases the number of eggs laid. However, including the confounder in our model reduces our power to identify the true cause.\n\n\n\nWhat to do?\nFirst let’s look at all the relationships in our data.\n\n\n\n\n\n\n\n\n\nThe right thing to do in is to model the confounder (e.g. with an ANCOVA or multiple regression).\n\n\n\n\n\n\n\n\nModel with cause only\n\n\nlm(fish_eggs~ env_quality, data = confounded_fish)\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nenv_quality\n0.513\n0.109\n4.721\n0\n\n\n\n\n\n\n\n\n\nMulticolinearity: This example also shows a statistical problem of multicolinearity – that is our predictors are correlated. This makes building and interpreting a model challenging.\n\n\n\n\nCausal model 2: The pipe (mediator)\n\n\n\n\n\n\n\n\n\nSo now let’s look at a pipe in which the environment causes fish length and fish length causes fitness, but environment itself has has no impact on fitness. First let’s simulate:\n\nOne hundred fish (n = 100)\nEnvironmental quality for each fish (normally distributed with mean \\(\\mu = 50\\) and standard deviation \\(\\sigma = 5\\))\nFish length based on environmental quality (predicted length = environmental quality with standard deviation 2)\nEgg count based on fish length (predicted egg count = half the fish length, standard deviation 5, rounded to the nearest integer)\n\n\npipe_fish &lt;- tibble(\n  env_quality = rnorm(n = n_fish, mean = 50, sd = 5),  # Simulating environment\n  fish_length = rnorm(n = n_fish, mean = env_quality, sd = 2),\n  fish_eggs   = rnorm(n = n_fish, mean = fish_length / 2, sd = 5) %&gt;% round()\n)\n\n\n\n\n\n\n\n\n\n\nFigure 2.2: Pipes are fun.\n\n\n\n\nIn this scenario, we know that environmental quality does not directly cause fish to lay more eggs, as the model does not include a direct link from quality to egg count. However, both a plot and a statistical test reveal a strong association between environmental quality and egg count when fish length is not included in the model.\n\nThinking about cause: In a real way you could say that environmental quality was a cause here. If we did an experiment and changed environmental quality we would get more eggs. Of course, we would get more eggs because fish length increased with environmental quality and this increase in length would be the cause of more eggs. If we could somehow increase fish length other means (e.g., selective breeding) without improving environmental quality, or if environmental quality improved but length remained constant, we would not – according to this DAG – see an increase in egg numbers.\n\nOur Statistical Analysis Cannot Prove Causation\nWe can fit a simple linear model predicting egg count based solely on environmental quality. This model will show a positive and reliable association (slope = 0.575; \\(r^2 =\\) 0.29) —- egg count appears to increase with environmental quality. Of course, this is not a causal relationship, as we did not model environmental quality as a direct cause of egg production.\n\n\n\n\n\n\n\n\nModel with indirect cause only\n\n\nlm(fish_eggs ~ env_quality, pipe_fish)\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nenv_quality\n0.575\n0.091\n6.331\n0\n\n\n\n\n\n\n\nAdding the immediate cause into our model\nSo, let’s build a model including both the immediate cause, fish length, as well as the indirect cause, environmental quality. We see pretty strange behavior here - when both variables are in the statistical model, neither is significantly associated with the number of eggs laid.\n\nlibrary(emmeans)\nfish_lm_w_cause    &lt;- lm(fish_eggs~ fish_length + env_quality, pipe_fish)  \nanova_table        &lt;- Anova(fish_lm_w_cause, type = \"II\")\nfish_length_est    &lt;- emtrends(fish_lm_w_cause, var = \"fish_length\") %&gt;% confint()\nenv_qual_est       &lt;- emtrends(fish_lm_w_cause, var = \"env_quality\") %&gt;% confint()\n\n\n\n\n\n\n\n\n\nANOVA Table: Model with Indirect & Direct Cause\n\n\nModel: lm(fish_eggs ~ env_quality + fish_length, data = pipe_fish)\n\n\nterm\nsumsq\ndf\nF_value\nP_value\n\n\n\n\nfish_length\n187.846\n1\n7.841\n0.006\n\n\nenv_quality\n0.593\n1\n0.025\n0.875\n\n\nResiduals\n2323.823\n97\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlope Estimates and Uncertainty: Model with Indirect & Direct Cause\n\n\nModel: lm(fish_eggs ~ env_quality + fish_length, data = pipe_fish)\n\n\nvariable\nslope\nSE\ndf\nlower.CL\nupper.CL\n\n\n\n\nfish_length\n0.621\n0.222\n97\n0.181\n1.061\n\n\nenv_quality\n-0.037\n0.236\n97\n-0.505\n0.430\n\n\n\n\n\n\n\nWhat to do?\nFirst let’s look at all the relationships in our data\n\n\n\n\n\n\n\n\n\nThe right thing to do in this case is to just build a model with the fish length.\n\n\n\n\n\n\n\n\nModel with direct cause only\n\n\nlm(fish_eggs~ fish_length, data = pipe_fish)\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nfish_length\n0.588\n0.082\n7.156\n0\n\n\n\n\n\n\n\n\n\n\nCausal model 3: Collider\nA collider is something caused by two variables. Say in our example both fish length and environmental quality caused fish to lay more eggs. Here egg number is a colider. Lets simulate it!\n\nOne thousand fish\nEnvironmental quality for each fish (normally distributed with mean \\(\\mu = 50\\) and standard deviation \\(\\sigma = 5\\))\nFish length is independent of quality (ormally distributed with mean \\(\\mu = 50\\) and standard deviation \\(\\sigma = 5\\))\nEgg count based on fish length and environmental quality (predicted egg count = (Fish length + Environmental quality) /4, standard deviation 8, rounded to the nearest integer)\n\n\nn_fish &lt;-1000\ncollider_fish &lt;- tibble(\n  env_quality = rnorm(n = n_fish, mean = 50, sd = 8),  # Simulating environment\n  fish_length = rnorm(n = n_fish, mean = 50, sd = 8),\n  fish_eggs   = rnorm(n = n_fish, mean = (env_quality+fish_length) / 4, sd = 2) %&gt;% round()\n) %&gt;%\n  mutate(fish_eggs = ifelse(fish_eggs &lt;0,0,fish_eggs))\n\nLet’s check out our data:\n\n\n\n\n\n\n\n\n\nWe see everyhting that is expected, and our stats back this up.\n\nlibrary(emmeans)\nfish_lm_w_collider &lt;- lm(fish_eggs~ fish_length + env_quality, collider_fish)  \nanova_table        &lt;- Anova(fish_lm_w_collider, type = \"II\")\nfish_length_est    &lt;- emtrends(fish_lm_w_collider, var = \"fish_length\") %&gt;% confint()\nenv_qual_est       &lt;- emtrends(fish_lm_w_collider, var = \"env_quality\") %&gt;% confint()\n\n\n\n\n\n\n\n\n\nANOVA Table: Modeling a collider\n\n\nModel: lm(fish_eggs ~ env_quality + fish_length, data = collider_fish)\n\n\nterm\nsumsq\ndf\nF_value\nP_value\n\n\n\n\nfish_length\n4666.372\n1\n1159.520\n0\n\n\nenv_quality\n4070.688\n1\n1011.502\n0\n\n\nResiduals\n4012.327\n997\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlope Estimates and Uncertainty: Modeling a collider\n\n\nModel: lm(fish_eggs ~ env_quality + fish_length, data = collider_fish)\n\n\nvariable\nslope\nSE\ndf\nlower.CL\nupper.CL\n\n\n\n\nfish_length\n0.254\n0.007\n997\n0.240\n0.269\n\n\nenv_quality\n0.250\n0.008\n997\n0.235\n0.266\n\n\n\n\n\n\n\nHere the correct model is to include both causes. exliding one will likely lead to unbiased estimates of the effect of the other, but it will reduce power and increase uncertainty.\n\nBeware of Collider Bias\nSuppose we’re interested in fish that lay a large number of eggs, and we want to explore factors that might help them lay even more. If we focus only on fish that laid more than the average number of eggs (e.g., 30), we might observe an unexpected pattern:\n\nggplot(collider_fish %&gt;% filter(fish_eggs &gt;= 30),\n       aes(x = env_quality, y = fish_length)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\nCollider bias: Colliders can have funny consequences when we condition on an outcome. Assume, there is no association between smoking and a genetic propensity to get lung cancer for reasons unrelated to smoking. If we only looked at lung cancer patients, it would appear that there is a negative correlation between smoking and a genetic risk for cancer unrelated to smoking because we do not see non-smokers with low genetic risk for lung cancer. This is known as “selection bias”, “M bias”, or “collider bias”.\n\n\n\n\n\n\n\n\n\nModel with direct cause only\n\n\nlm(fish_length ~ env_quality, data = collider_fish %&gt;% filter(fish_eggs&gt;=30))\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nenv_quality\n-0.447\n0.082\n-5.422\n0",
    "crumbs": [
      "26. Cause"
    ]
  },
  {
    "objectID": "book_sections/causal_inference.html#what-to-do-1",
    "href": "book_sections/causal_inference.html#what-to-do-1",
    "title": "26. Cause",
    "section": "What to do?",
    "text": "What to do?\nThe examples above show the complexity in deciphering causes without experiments. But they also show us the light about how we can infer causation, because causal diagrams can point to testable hypotheses. If we cannot do experiments, causal diagrams offer us a glimpse into how we can infer causation. Perhaps the best way to do this is by matching – if we can match subjects that are identical for all causal paths except the one we are testing, we can then test for a statistical association, ad make a causal claim we can believe in.\nThe field of causal inference is developing rapidly. If you want to hear more, the popular book, The Book of Why (Pearl and Mackenzie 2018) is a good place to start. The field of causal inference offers many exciting techniques for deciphering cause. For now we will just say that a good biological intuition and carefull thinking about your model and causation is a great starting place.\n\nThree common DAGs & What to do with them\n\nConfounder: A variable that causes both the predictor and the outcome, creating a back-door path (a spurious association).\n\nExample DAG: Genetics → Smoking and Genetics → Cancer.\n\nWhat to do? Control for confounder.\n\nMediator (or “Pipe”): A variable through which the causal effect flows. Removing it breaks the causal chain and estimates only the direct effect.\n\nExample DAG: Genes → Smoking → Cancer (Smoking is the mediator).\n\nWhat to do? Think!! If you include the mediator you estimate direct effects. If you omit the mediator you estimate total effects.\n\nCollider: A variable that is caused by two or more others; conditioning on it creates a false association.\n\nExample DAG: Smoking → Cancer ← Genes. In this example there would be a negative relationship between smoking and genetic predisposition to lung cancer among lung cancer patients, but not in the broader population.\nWhat to do? Do not condition on the “colliding variable” and do no include it in your model.\n\n\n\n\n\n\n\nFisher, A., Ronald. (1958). Cancer and smoking. Nature, 182(4635), 596–596. https://doi.org/10.1038/182596a0\n\n\nStolley, P. D. (1991). When Genius Errs: R. A. Fisher and the Lung Cancer Controversy. American Journal of Epidemiology, 133(5), 416–425. https://doi.org/10.1093/oxfordjournals.aje.a115904",
    "crumbs": [
      "26. Cause"
    ]
  },
  {
    "objectID": "book_sections/probability_and_likelihood.html",
    "href": "book_sections/probability_and_likelihood.html",
    "title": "27. Probability and Likelihood",
    "section": "",
    "text": "A Very Brief Intro to Probability\nA theme of this book (and of all of statistics) is that nothing is truly certain. Outcomes are not inevitable. Rather, each possible outcome in a “sample space” carries some probability.\nThis – of course – does not mean that all outcomes are equally probable. For example, in NHST we reject the null hypothesis when our observations (or something more extreme) are an improbable outcome of the null sampling distribution. As such, the rules of probability play a key role in NHST specifically and all of statistics, more broadly.",
    "crumbs": [
      "27. Probability and Likelihood"
    ]
  },
  {
    "objectID": "book_sections/probability_and_likelihood.html#a-very-brief-intro-to-probability",
    "href": "book_sections/probability_and_likelihood.html#a-very-brief-intro-to-probability",
    "title": "27. Probability and Likelihood",
    "section": "",
    "text": "Probability: Terms and Conditions Apply\n\nConditional probabilities\nThere is a true probability that a specific outcome occurs. This “parameter” is the proportion of observations in an infinite number of trials that lead to our focal outcome. So \\(P(A)\\) is the probability that thing A occurs.\nA simple example is a coin flip with two possible outcomes: heads or tails. If the coin is “fair” these outcomes are equally probable. So, we say:\n\\[P(\\text{Heads} \\mid \\text{fair coin}) = 0.5\\]\nTo unpack this notation a bit:\n\n\n\n\n\n\n\n\n\nFigure 1: The result of a coin flip is a classic example of a probabilistic outcome.\n\n\n\n\n\n\\(P\\) stands for probability.\n\\(P(\\text{Heads})\\) shows which outcome we are considering (in this case, seeing heads),\n\n\\(|\\_\\_\\). The vertical bar “|” means “given” or “under the condition of.” This is the probability the coin shows heads, given that we know it is fair. Another word for this is that this is a conditional probability.\n\nOf course, not all coins are fair. If I gave you a trick coin that came up heads 75% of the time, we would have:\n\\[P(\\text{Heads} \\mid \\text{Yaniv's trick coin}) = \\frac{3}{4}\\].\nAs for any parameter, we almost never KNOW the true probability. Rather, we usually estimate it as the proportion \\(\\hat{p}\\):\n\\[\\widehat{p} = \\frac{\\# \\text{ of times the focal outcome occurred}}{\\# \\text{ of trials}}\\]\n\nIndependence and non-independence:\nIn some cases, knowing a condition does not provide additional information about the probability of the outcome. For example, my fair coin is equally likely to come up heads any day of the week. So:\n\\[P(\\text{Heads} \\mid \\text{fair on monday}) = P(\\text{Heads} \\mid \\text{fair on tuesday}) = 0.5\\]\nHere the probability of heads, \\(P(\\text{Heads})\\), is independent of the day of the week. Other times, the condition matters. For example \\(P\\text{(rain | sunny)} \\neq P\\text{(rain | cloudy)}\\). So, rain and clouds are not independent.\n\n\n\n\nCombining probabilities\nWe are often interested in more complex probabilities. For example:\n\nWhat is the probability it rained or was cloudy?\nWhat is the probability it rained two days in a row?\n\n\n\nAdd probabilities to find the probability of “OR”\nTo find the probability that A OR B happened, we add the probability of A and the probability of B, and then subtract the probability of both happening so we don’t double-count:\n\\[P(\\text{A OR B}) = P(\\text{A}) + P(\\text{B}) - P(\\text{A AND B})\\]\nAs a concrete example, consider the probability that it’s raining or cloudy. These aren’t mutually exclusive events—rain usually comes with clouds—so:\n\\[P(\\text{rain OR cloudy})= P(\\text{rain}) + P(\\text{cloudy}) - P(\\text{rain AND cloudy})\\]\n\n\nHere the term, \\(P(\\text{rain AND cloudy})\\), is non-zero because it can be cloudy and rainy at the same time.\nWhen A and B cannot happen at the same time (i.e., when they’re mutually exclusive), the overlap is zero. In that special case, the formula simplifies to:\n\\[P(\\text{A OR B}) = P(\\text{A}) + P(\\text{B})\\]\nThe last “OR” rule we need is the law of total probability. This says that the probability of some outcome is the sum of all the mutually exclusive ways that the outcome can occur. For example:\n\n\nMore broadly, the law of total probability states: \\[P(B) = \\sum P(B|A_i) \\times P(A_i)\\]\n\\[P(\\text{rain}) = P(\\text{rain AND sunny}) + P(\\text{rain AND cloudy})\\] \\[=P(\\text{rain | sunny})\\times P(\\text{sunny}) + P(\\text{rain | cloudy})\\times P(\\text{cloudy})\\]\nSeeing this idea to its logical conclusion, the probabilities of all mutually exclusive outcomes have to add up to one. For example if each day is either sunny or cloudy, but not both or neither, \\(P(\\text{sunny}) + P(\\text{cloudy}) = 1\\).\nThis leads to the rule for NOT: the probability of not having some outcome is simply one minus the probability of that outcome:\n\n\nHere we treat “sunny” and “cloudy” as mutually exclusive conditions. Of course, real weather can include things like “partly sunny,” “partly cloudy,” or days with both sun and clouds etc… We’re ignoring those complications for now to keep the example simple.\n\\[P(\\text{NOT A}) = 1 - P(A)\\]\n\n\nMultiply probabilities for the probability of AND\nTo find the probability that two things happen together, we multiply the probability of one event (say A) by the probability of the second event given that the first one has already happened.\n\\[P(\\text{A AND B}) = P(A)\\times P(B|A) = P(B)\\times P(A|B)\\]\nFor example, the probability that it is raining and cloudy is \\(P(\\text{rain AND cloudy})= P(\\text{cloudy})\\times P(\\text{rain | cloudy})= P(\\text{rain})\\times P(\\text{cloudy | rain})\\)\n\n\nBayes’ theorm: Flipping conditional probabilities\nIf we know it’s not raining, we can use “Bayes’ Theorem” to flip these conditional probabilities. For example, we can use Bayes’ theorem to find the probability it’s sunny given that it’s not raining. I find that the easiest way to think about it is as follows:\n\nFind the expected number of rain-free sunny days in a year. This will be our numerator.\nFind the number of rain-free days in a year. This will be our denominator\n\n\\[P(\\text{Sunny | No rain}) = \\frac{\\#\\text{ sunny rain-free days}}{\\# \\text{rain-free days}}= \\frac{\\cancel{365} \\times P(\\text{No rain AND sunny}) }{\\cancel{365} \\times P(\\text{No rain})}\\]\n\n\nThe 365 is there to get days in a year. This helps me think concretely - but is mathematically unnecessary (because it’s in the numerator and denominator) and cancels out.\nNow we simply apply our rules from above:\n\nMultiplication rule: \\(P(\\text{No rain AND sunny}) = P(\\text{No rain | sunny }) \\times P(\\text{sunny})\\).\nLaw of Total Probability: \\(P(\\text{No rain})\\) \\(=P(\\text{No rain AND sunny}) + P(\\text{No rain AND cloudy})\\) \\(=P(\\text{No rain | sunny }) \\times P(\\text{sunny}) +P(\\text{No rain | cloudy }) \\times P(\\text{cloudy})\\).\n\n\\[P(\\text{Sunny | No rain}) = \\frac{P(\\text{No rain | sunny }) \\times P(\\text{sunny})}{P(\\text{No rain | sunny }) \\times P(\\text{sunny}) +P(\\text{No rain | cloudy }) \\times P(\\text{cloudy})}\\]\nMore broadly, Bayes’ theorem allows us to flip conditional probabilities as:\n\\[P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}\\]",
    "crumbs": [
      "27. Probability and Likelihood"
    ]
  },
  {
    "objectID": "book_sections/probability_and_likelihood.html#likelihood-based-inference",
    "href": "book_sections/probability_and_likelihood.html#likelihood-based-inference",
    "title": "27. Probability and Likelihood",
    "section": "Likelihood-Based Inference",
    "text": "Likelihood-Based Inference\nThe brief intro to probability theory, above, could have helped our statistical understanding up until now. But I think we did OK with the probability intuition and knowledge you already had. However, probability isn’t just for thinking about sampling distributions, we can use probability theory to develop richer and more flexible statistical analyses.\nSpecifically, we can use probabilities in “likelihood based inference” to go beyond the assumptions of linear models and build inferences under any model we can describe through an equation. This approach allows us to model more complex scenarios, like phylogenies, genome sequences, or any structured data. For now, however, we’ll stick to our simple normal distribution so that we can follow along.\n\nProbabilities and Likelihoods\nA probability represents the proportion of times a model would produce a particular result if we did the same thing many many times.\n\nWhen thinking about probabilities, we consider one model and all possible outcomes. For a given model, all probabilities (or probability densities) sum (or integrate) to one.\n\nMathematically, calculating a likelihood is identical to calculating a probability. The key difference lies in our perspective.\n\nWith likelihood, we view the outcome as fixed and consider the various models that might have generated it.\n\nIn this way, a likelihood represents a conditional probability.\n\\[P(\\text{Data} | \\text{Model}) = \\mathscr{L}(\\text{Model} | \\text{Data})\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog-Likelihoods\nIf our data points are independent, we can calculate the likelihood of all data points by taking the product of each individual likelihood. However, this approach introduces some practical challenges:\n\nMultiplying many probabilities often results in very small numbers, which can become so small that computers struggle to handle them accurately (underflow).\n\nProducts of probabilities are also less convenient in mathematical operations – (remember the central limit theorem works when we add bits, not when we multiply bits).\n\nTo address these issues, we typically take the log of each likelihood and sum them all. In log space, addition replaces multiplication — for example, 0.5 * 0.2 = 0.1 becomes log(0.5) + log(0.2) = -2.302585, and \\(e^{-2.302585} = 0.1\\). So you will almost always see people talking about log-likelihoods instead of raw likelihoods.\n\nLog Likelihood of \\(\\mu\\)\nHow can we calculate likelihoods for a parameter of a normal distribution? Here’s how!\nSuppose we have a sample with values 0.01, 0.07, and 2.2, and we know the population standard deviation is one, but we don’t know the population mean. We can find the likelihood of a proposed mean by multiplying the probability of each observation, given the proposed mean. For example, the likelihood of \\(\\mu = 0 | \\sigma = 1, \\text{ and } Data = \\{0.01, 0.07, 2.2\\}\\) is:\n\ndnorm(x = 0.01, mean = 0, sd = 1) * \n  dnorm(x = 0.07, mean = 0, sd = 1) * \n  dnorm(x = 2.20, mean = 0, sd = 1)\n\n[1] 0.00563186\n\n\nA more compact way to write this is\n\ndnorm(x = c(0.01, 0.07, 2.20), mean = 0, sd = 1) |&gt; \n  prod()\n\n[1] 0.00563186\n\n\nRemember, we multiply because we assume that all observations are independent.\nAs discussed above, we typically work with log likelihoods rather than linear likelihoods. Because multiplying on the linear scale is equivalent to adding on the log scale, the log likelihood in this case is:\n\ndnorm(x = c(0.01, 0.07, 2.20), mean = 0, sd = 1, log = TRUE) |&gt; \n  sum()\n\n[1] -5.179316\n\n\nReassuringly, \\(ln(0.00563186) =\\) -5.1793155, and similarly \\(e^{5.179316} =\\) 0.0056319\n\n\nThe Likelihood Profile\nWe can consider a range of plausible parameter values and calculate the (log) likelihood of the data for each of these “models.” This is called a likelihood profile. We can create a likelihood profile with the following:\n\nobs            &lt;- c(0.01, 0.07, 2.2)    # our observations\nproposed.means &lt;- seq(-1, 2, .001)      # proposing means from -1 to 2 in 0.001 increments\n\nlikelihood_profile &lt;- tibble(proposed_mean = proposed.means)|&gt;\n  group_by(proposed_mean)|&gt;\n  # find the loglikelihood of each proposed \"model\"\n  mutate(log_lik = sum(dnorm(x = obs, mean = proposed_mean , sd = 1, log = TRUE)))|&gt;\n  ungroup()\n\nNow you can scroll through and see the log likelihoods of each proposed mean\n\n\n\n\n\n\n\nAbove I just said the standard deviation was one. But more often we find it from our data. However\nThe standard deviation in our likelihood calculations is a bit different than we are used to.\n\nFirst, we consider our sums of squares as deviations away from the proposed mean, rather than the estimated mean.\n\nSecond we divide by n, not not n-1 (because we state the mean and do not estimate it)\n\n\n\n\nMaximum Likelihood Estimate\nHow can we use this for inference? One estimate of our parameter is the value that “maximizes the likelihood” of our data (e.g., the x-value corresponding to the highest point on the plot above).\nTo find the maximum likelihood estimate, we find the biggest value (i.e. the smallest negative number) in our likelihood profile. We can find this by arranging our data set from largest to smallest number\n\nlikelihood_profile%&gt;% \n  arrange(desc(log_lik))\n\n\n\n\n\n\n\nOr by filtering our data so we only have the proposed mean that maximizes the likelihood of our data\n\nMLE &lt;- likelihood_profile %&gt;%\n  filter(log_lik == max(log_lik))\n\n\n\n# A tibble: 1 × 2\n  proposed_mean log_lik\n          &lt;dbl&gt;   &lt;dbl&gt;\n1          0.76   -4.31\n\n\nThis equals the mean of our observations: mean(c(0.01, 0.07, 2.2)) = 0.76. In general, for normally distributed data, the maximum likelihood estimate of the population mean is the sample mean.\n\nNOTE The maximum likelihood estimate is not the parameter with the best chance of being correct (that’s a Bayesian question); rather, it’s the parameter that, if true, would have the greatest likelihood of producing our observed data.\n\n\n\n\nLikelihood based inference: Uncertainty and hypothesis testing.\n\nUncertainty\nWe need one more trick to use the likelihood profile to estimate uncertainty – log likelihoods are roughly \\(\\chi^2\\) distributed with degrees of freedom equal to the number of parameters we’re trying to guess (here, just one – corresponding to the mean). So for 95% confidence intervals are everything within qchisq(p = .95, df =1) /2 = 1.92 log likelihood units of the maximum likelihood estimate.\n\nin95CI &lt;- pull(MLE, log_lik) -1.92\n\nCI &lt;- likelihood_profile             |&gt;\n  filter(log_lik &gt; in95CI)           |&gt;\n  reframe(CI = range(proposed_mean))\n\n\n\n\n\n\n\n\n\nCI\n\n\n\n\n-0.371\n\n\n1.891\n\n\n\n\n\n\n\nNow we can plot our likelihood profile, noting our MLE (in red) and 95% confidence intervals (the shaded area):\n\nggplot(likelihood_profile)+\n  geom_rect(data = .  %&gt;% summarise(ymin = min(log_lik),ymax = max(log_lik),\n                                 xmin = min(pull(CI )),xmax = max(pull(CI ))),\n            aes(xmin = xmin,xmax = xmax, ymin = ymin, ymax = ymax), \n            fill= \"lightgrey\", alpha = .4)+\n  geom_line(aes(x = proposed_mean, y = log_lik))+\n  geom_vline(xintercept = pull(MLE, proposed_mean), color = \"red\")+\n  theme_light()\n\n\n\n\n\n\n\n\n\n\nNull hypothesis significance testing.\nWe can find a p-value and test the null hypothesis by comparing the likelihood of our MLE (\\(log\\mathscr{L}(MLE|D)\\)) to the likelihood of the null model (\\(log\\mathscr{L}(H_0|D)\\)). We call this a likelihood ratio test, because we divide the likelihood of the MLE by the likelihood of the null – but we’re doing this in logs, so we subtract rather than divide. For this arbitrary example, lets pretend we want to test the null that the true mean is zero:\n\nLog likelihood of the MLE: \\(log\\mathscr{L}(MLE|D)\\) = Sum the log-likelihood of each observation under the MLE = pull(MLE, log_lik) = -4.313.\nLog likelihood of the Null: \\(log\\mathscr{L}(H_0|D)\\) = Sum the log-likelihood of each observation under the null = likelihood_profile %&gt;% filter(proposed_mean == 0)%&gt;% pull(log_lik) = -5.179.\n\nWe then calculate \\(D\\) which is simply two times this difference in lof likelihoods, and calculate a p-value with it by noting that \\(D\\) is \\(\\chi^2\\) distributed with degrees of freedom equal to the number of parameters we’re inferring (here, just one – corresponding to the mean). We see that our p value is greater than 0.05 so we fail to reject the null.\n\nlog_lik_MLE &lt;- pull(MLE, log_lik)\nlog_lik_H0  &lt;- likelihood_profile |&gt; \n  filter(proposed_mean == 0) |&gt; \n  pull(log_lik)\nD      &lt;- 2 * (log_lik_MLE - log_lik_H0)\np_val  &lt;- pchisq(q = D, df = 1, lower.tail = FALSE)\n\n\n\n\n\n\n\n\n\nD\np_val\n\n\n\n\n1.733\n0.188\n\n\n\n\n\n\n\nUnfortunately, the LRT test give poorly calibrated p-values – especially when we have a small sample size.\nCompare our LRT p-value of 0.188 to that’s from a one sample t-test\n\nt.test(obs,mu=0)   |&gt; \n  tidy()           |&gt; \n  pull(p.value)\n\n[1] 0.401964\n\n\nA simple solution is applying “Bartlett’s Correction for Small Samples” correction, here \\(D_\\text{corrected} = \\frac{D}{1+\\frac{1}{2\\times n}}\\). This gets us closer, but we still must recognize that the LRT is a very rough approximtation\n\n\nThis correction is necessary because, with small samples, the \\(\\chi^2\\) approximation breaks down because the log-likelihood surface is not close to quadratic. With large samples, the LRT behaves extremely well.\n\nBartlett_D      &lt;- D / (1+1/(2 * length(obs)))\nBartlett_p_val  &lt;- pchisq(q = Bartlett_D, df = 1, lower.tail = FALSE)\nprint(Bartlett_p_val)\n\n[1] 0.2229538",
    "crumbs": [
      "27. Probability and Likelihood"
    ]
  },
  {
    "objectID": "book_sections/probability_and_likelihood.html#example-1-are-species-moving-uphill",
    "href": "book_sections/probability_and_likelihood.html#example-1-are-species-moving-uphill",
    "title": "27. Probability and Likelihood",
    "section": "Example 1: Are species moving uphill?",
    "text": "Example 1: Are species moving uphill?\nChen et al. (2011) tested the idea that organisms move to higher elevation as the climate warms. To test this, they collected data (available here) from 31 species, plotted below (Figure 2).\n\n\n\n\n\n\n\n\nFigure 2: Change in the elevation of 31 species. Data from Chen et al. (2011).\n\n\n\n\n\nWe could conduct a one sample t-test against of the null hypothesis that there has been zero net change on average.\n\nlm(elevationalRangeShift ~ 1, data = range_shift) |&gt;\n  tidy()                                                                                                                                                                           |&gt; mutate(p.value = p.value* 10^8)%&gt;%mutate_at(2:5, round, digits = 2) %&gt;% mutate(p.value = paste(p.value,\"x 10^-8\"))|&gt; gt()\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n39.33\n5.51\n7.14\n6.06 x 10^-8\n\n\n\n\n\n\n\n\nCalculate log likelihoods for each model\nThere is nothing wrong with that one-sample t-test, but let’s use this as an opportunity to learn about how to apply likelihood. First we grab our observations, write down our proposed means – lets say from negative one hundred to two hundred in increments of .01.\n\nobservations   &lt;-  pull(range_shift, elevationalRangeShift)\nproposed_means &lt;- seq(-100,200,.01)\nn               &lt;- length(observations)\n\nWe then go on to make a likelihood surface by.\n\nMaking a vector of proposed means\nCalculate the population standard deviation (sigma) for each proposed parameter value,\nFor each proposed mean, find the log likelihood of each observation (with dnorm)\nSum the log likelihoods of each observation for a given proposed mean to find the log likelihood of that parameter estimate given the data.\n\nJust for fun well arrange them with the MLE on top!\n\nlog_lik_uphill &lt;- tibble(mu = proposed_means)%&gt;%      # Step 1 \n  group_by(mu)%&gt;%                                        # R stuff to make sure we only work within a parameter\n  mutate(sigma   = sqrt(sum((observations-mu)^2) / n) ,  # Step 2: Find the standard deviation  \n         log_lik = dnorm(x = observations,               # Step 3: Find the \n                         mean = mu,                      #    log-likelihood\n                         sd = sigma,                     #    if each data \n                         log= TRUE)%&gt;%                   #    point\n           sum())%&gt;%                                     # Step 4: Sum the log-likelihoods\n  ungroup()%&gt;%\n  arrange(desc(log_lik))\n\n\n\n\n\nLog likelihoods of for 100 proposed means (a subst of those investigated above), sorted from highest to lowest log likelihood.\n\n\n\nMLE\nThis sorted list shows that our maximum likelihood estimate is about 39 meters. The actual MLE is about\n\nMLE &lt;- log_lik_uphill %&gt;%\n  filter(log_lik == max(log_lik))\n\n\n\n\n\n\n\n\n\nmu\nsigma\nlog_lik\n\n\n\n\n39.3\n30.2\n-149.6\n\n\n\n\n\n\n\n\n\n95% CI\nAgain, because log likelihoods are roughly \\(\\chi^2\\) distributed with one degree of freedom, everything within qchisq(p = .95, df =1) /2 = 1.92 log likelihood units of the maximum likelihood estimate is in the 95% confidence interval:\n\nin95CI &lt;- pull(MLE, log_lik) -1.92\n\nCI &lt;- log_lik_uphill  %&gt;%\n  filter(log_lik &gt; in95CI) %&gt;%\n  reframe(CI = range(mu))\n\n\n\n\n\n\n\n\n\nCI\n\n\n\n\n28.38\n\n\n50.28\n\n\n\n\n\n\n\nNow we can plot our likelihood profile, noting our MLE (in red) and 95% confidence intervals (the shaded area):\n\nggplot(log_lik_uphill)+\n  geom_rect(data = .  %&gt;% summarise(ymin = min(log_lik),ymax = max(log_lik),\n                                 xmin = min(pull(CI )),xmax = max(pull(CI ))),\n            aes(xmin = xmin,xmax = xmax, ymin = ymin, ymax = ymax), \n            fill= \"lightgrey\", alpha = .4)+\n  geom_line(aes(x = mu, y = log_lik))+\n  geom_vline(xintercept = pull(MLE, mu), color = \"red\")+\n  theme_light()\n\n\n\n\n\n\n\n\n\n\nNull hypothesis significance testing.\nAs above we find the p-value by comparing two times the difference in log likelihoods under the MLE and the null model (\\(D\\)), to the \\(\\chi^2\\) distribution with one degress of freedom. This value is in the same ballpart as what we got from the one sample t-test (\\(6.06 \\times 10^{-8}\\))\n\nlog_lik_MLE &lt;- pull(MLE, log_lik)\nlog_lik_H0  &lt;- log_lik_uphill %&gt;% filter(mu == 0)%&gt;% pull(log_lik)\nD      &lt;- 2 * (log_lik_MLE - log_lik_H0)\np_val  &lt;- pchisq(q = D, df = 1, lower.tail = FALSE)\n\n\n\n\n\n\n\n\n\nlog_lik_MLE\nlog_lik_H0\nD\np_val\n\n\n\n\n-149.59\n-164.99\n30.79\n2.87 x 10^-8\n\n\n\n\n\n\n\n\n\n\nR tricks\nYou can grag the log likelihoods of a model with the logLikfunction.\n\nMLE_model &lt;- lm(elevationalRangeShift ~ 1, data = range_shift) # Estimate the intercept\nlogLik(MLE_model ) %&gt;% as.numeric()\n\n[1] -149.5937\n\nNull_model &lt;- lm(elevationalRangeShift ~ 0, data = range_shift) # Set intercept to zero\nlogLik(Null_model ) %&gt;% as.numeric()\n\n[1] -164.9888\n\n\nOr with the glance() function from broom\n\nlibrary(broom)\nglance(MLE_model) %&gt;% select(logLik) %&gt;% pull()\n\n[1] -149.5937\n\n\nYou can also use the lrtest function in the lmtest package to conduct this test:\n\nlibrary(lmtest)\nlrtest(Null_model,MLE_model )\n\nLikelihood ratio test\n\nModel 1: elevationalRangeShift ~ 0\nModel 2: elevationalRangeShift ~ 1\n  #Df  LogLik Df Chisq    Pr(&gt;Chisq)    \n1   1 -164.99                           \n2   2 -149.59  1 30.79 0.00000002875 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "27. Probability and Likelihood"
    ]
  },
  {
    "objectID": "book_sections/probability_and_likelihood.html#bayesian-inference",
    "href": "book_sections/probability_and_likelihood.html#bayesian-inference",
    "title": "27. Probability and Likelihood",
    "section": "Bayesian inference",
    "text": "Bayesian inference\nWe often care about how probable our model is given the data, rather than the reverse. Likelihoods can help us approach this! Remember Bayes’ theorem:\n\\[P(\\text{Model|Data}) = \\frac{P(\\text{Data|Model}) \\times P(\\text{Model})}{P(\\text{Data})}\\]\nTaking this apart:\n\n\\(P(\\text{Model|Data})\\): The posterior probability—the probability of our model after observing the data.\n\n\\(P(\\text{Data|Model})\\): The likelihood, which we’ve just calculated. Mathematically, it’s written as \\(\\mathscr{L}(Model|Data)\\).\n\n\\(P(\\text{Model})\\): The prior probability—our belief about the model’s probability before observing the data. Since we often don’t know this, we usually assign a value that seems reasonable.\n\n\\(P(\\text{Data})\\): The evidence, or the probability of the data. This is calculated using the law of total probability.\n\nToday, we’ll assign an arbitrary prior probability for demonstration purposes. This is not ideal—Bayesian inferences are only meaningful to the extent that we can meaningfully interpret the posterior, which depends on having a well-justified prior. However, for the sake of this example, let’s assume a prior: the parameter is normally distributed with a mean of 0 and a standard deviation of 30.\n\nbayes_uphill &lt;- log_lik_uphill |&gt;\n  mutate(lik   = exp(log_lik),\n         prior = dnorm(x = mu, mean = 0, sd = 30) / sum(dnorm(x = mu, mean = 0, sd = 30) ),\n         evidence  = sum(lik * prior),\n         posterior =  (lik *  prior) / evidence) \n\n\n\n\n\n\n\n\n\n\nWe can grab interesting thing from the posterior distribution.\n\nFor example we can find the maximum a posteriori (MAP) estimate as\n\n\nbayes_uphill |&gt;\n  filter(posterior == max(posterior))                                                                                                  |&gt; data.frame()\n\n     mu    sigma   log_lik          lik         prior     evidence    posterior\n1 38.08 30.19035 -149.6203 1.048907e-65 0.00005944371 8.550126e-67 0.0007292396\n\n\nNote that this MAP estimate does not equal our MLE as it is pulled away from it by our prior.\n\nWe can grab the 95% credible interval. Unlike the 95% confidence intervals, the 95% credible interval has a 95% chance of containing the true parameter (if our prior is correct).\n\n\nbayes_uphill                                  |&gt;\n  mutate(cumProb = cumsum(posterior))         |&gt;\n  filter(cumProb &gt; 0.025 & cumProb &lt; 0.975)   |&gt;\n  summarise(lower_95cred = min(mu),\n            upper_95cred = max(mu))                                                                                                               |&gt; data.frame()\n\n  lower_95cred upper_95cred\n1        26.18        52.48\n\n\n\nPrior sensitivity\nIn a good world our priors are well calibrated.\nIn a better world, the evidence in the data is so strong, that our priors don’t matter.\nA good thing to do is to compare our posterior distributions across different prior models. The plot below shows that if our prior is very tight, we have trouble moving the posterior away from it. Another way to say this, is that if your prior believe is strong, it would take loads of evidence to gr you to change it.\n\n\n\n\n\n\n\n\n\nMCMC / STAN / brms\nWith more complex models, we usually can’t use the math above to solve Bayesian problems. Rather we use computer ticks – most notably the Markov Chain Monte Carlo MCMC to approximate the posterior distribution.\nThe programming here can be tedious so there are many programs – notable WINBUGS, JAGS and STAN – that make the computation easier. But even those can be a lot of work. Here I use the R package brms, which runs stan for us, to do an MCMC and do Bayesian stats. I suggest looking into this if you want to get stared, and learning STAN for more serious analyses\n\nlibrary(brms)\nchange.fit &lt;- brm(elevationalRangeShift ~ 1, \n    data   = range_shift,\n    family = gaussian(),\n    prior  = set_prior(\"normal(0, 30)\", \n                      class = \"Intercept\"),\n    chains = 4,\n    iter   = 5000)\n\nchange.fit$fit\n\n\n\n\n\n\n\n\n\nterm\nmean\nse_mean\nsd\n2.5%\n25%\n50%\n75%\n97.5%\nn_eff\nRhat\n\n\n\n\nb_Intercept\n37.82\n0.06\n5.69\n26.56\n34.11\n37.86\n41.59\n48.82\n7863\n1\n\n\nsigma\n31.70\n0.05\n4.25\n24.84\n28.68\n31.24\n34.20\n41.34\n6444\n1\n\n\nlp__\n-156.70\n0.02\n1.04\n-159.48\n-157.09\n-156.38\n-155.97\n-155.70\n4340\n1",
    "crumbs": [
      "27. Probability and Likelihood"
    ]
  },
  {
    "objectID": "book_sections/probability_and_likelihood.html#quiz",
    "href": "book_sections/probability_and_likelihood.html#quiz",
    "title": "27. Probability and Likelihood",
    "section": "QUIZ",
    "text": "QUIZ\n\n\n\n\n\n\n\n\nChen, I.-C., Hill, J. K., Ohlemüller, R., Roy, D. B., & Thomas, C. D. (2011). Rapid range shifts of species associated with high levels of climate warming. Science, 333(6045), 1024–1026. https://doi.org/10.1126/science.1206432",
    "crumbs": [
      "27. Probability and Likelihood"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Amrhein, V., Greenland, S., & McShane, B. (2019). Scientists rise up\nagainst statistical significance. Nature, 567(7748),\n305.\n\n\nAsmuth, J., Morson, E. M., & Rips, L. J. (2018). Children’s\nunderstanding of the natural numbers’ structure. Cognitive\nScience, 42(6), 1945–1973. https://doi.org/https://doi.org/10.1111/cogs.12615\n\n\nBabbage, C. (1864). Passages from the life of a philosopher.\nLongman; Co.\n\n\nBakker, J. D. (2024). Applied multivariate statistics in\nR. Pressbooks.\n\n\nBeall, C. M. (2006). Andean, Tibetan, and Ethiopian\npatterns of adaptation to high-altitude hypoxia. Integrative\nand Comparative Biology, 46(1), 18–24. https://doi.org/10.1093/icb/icj004\n\n\nBehrouzi, P., & Wit, E. (2017). Detecting epistatic selection with\npartially observed genotype data using copula graphical models.\nJournal of the Royal Statistical Society: Series C (Applied\nStatistics), 68. https://doi.org/10.1111/rssc.12287\n\n\nBergstrom, C. T., & West, J. D. (2020). Calling bullshit: The\nart of skepticism in a data-driven world. Random House.\n\n\nBjörklund, M. (2019). Be careful with your principal components.\nEvolution, 73(10), 2151–2158. https://doi.org/https://doi.org/10.1111/evo.13835\n\n\nBroman, K. W., & Woo, K. H. (2018). Data organization in\nspreadsheets. The American Statistician, 72(1), 2–10.\nhttps://doi.org/10.1080/00031305.2017.1375989\n\n\nBryan, J. J. (2020). STAT 545: Data wrangling, exploration, and\nanalysis with r. Bookdown. https://stat545.com\n\n\nChang, W. (2020). R graphics cookbook: Practical recipes for\nvisualizing data. https://r-graphics.org/\n\n\nChari, L., Tara AND Pachter. (2023). The specious art of single-cell\ngenomics. PLOS Computational Biology, 19(8), 1–20. https://doi.org/10.1371/journal.pcbi.1011288\n\n\nChen, I.-C., Hill, J. K., Ohlemüller, R., Roy, D. B., & Thomas, C.\nD. (2011). Rapid range shifts of species associated with high levels of\nclimate warming. Science, 333(6045), 1024–1026. https://doi.org/10.1126/science.1206432\n\n\nD’Hont, A., Denoeud, F., Aury, J.-M., Baurens, F.-C., Carreel, F.,\nGarsmeur, O., Noel, B., Bocs, S., Droc, G., Rouard, M., Da Silva, C.,\nJabbari, K., Cardi, C., Poulain, J., Souquet, M., Labadie, K., Jourda,\nC., Lengellé, J., Rodier-Goud, M., … Wincker, P. (2012). The banana\n(musa acuminata) genome and the evolution of monocotyledonous plants.\nNature, 488(7410), 213–217. https://doi.org/10.1038/nature11241\n\n\nFarine, D. R., & Carter, G. G. (2022). Permutation tests for\nhypothesis testing with animal social network data: Problems and\npotential solutions. Methods in Ecology and Evolution,\n13(1), 144–156. https://doi.org/https://doi.org/10.1111/2041-210X.13741\n\n\nFieberg, J. (2024). Statistics for ecologists: A frequentist and\nbayesian treatment of modern regression models. University of\nMinnesota Libraries Publishing. Retrieved from the University of\nMinnesota Digital Conservancy. https://doi.org/10.24926/9781959870029\n\n\nFieberg, J. R., Vitense, K., & Johnson, D. H. (2020).\nResampling-based methods for biologists. PeerJ, 8,\ne9089. https://doi.org/10.7717/peerj.9089\n\n\nFisher, A., Ronald. (1958). Cancer and smoking. Nature,\n182(4635), 596–596. https://doi.org/10.1038/182596a0\n\n\nFisher, R. A. (1926). The arrangement of field experiments. Journal\nof the Ministry of Agriculture, 33, 503–515.\n\n\nFreeman, M. (2006). A visual comparison of normal and paranormal\ndistributions. J Epidemiol Community Health, 60(1), 6.\n\n\nGelman, A., & Carlin, J. (2017). Some natural solutions to the\np-value communication problem—and why they won’t work. Journal of\nthe American Statistical Association, 112(519), 899–901.\nhttps://doi.org/10.1080/01621459.2017.1311263\n\n\nGould, P. (1981). Letting the data speak for themselves. Annals of\nthe Association of American Geographers, 71(2), 166–176.\nhttps://doi.org/https://doi.org/10.1111/j.1467-8306.1981.tb01346.x\n\n\nGrolemund, G. (2014). Hands-on programming with r: Write your own\nfunctions and simulations. \" O’Reilly Media, Inc.\".\n\n\nGrolemund, G., & Wickham, H. (2018). R for data science.\n\n\nHart, J. D. A., Weiss, M. N., Brent, L. J. N., & Franks, D. W.\n(2022). Common permutation methods in animal social network analysis do\nnot control for non-independence. Behavioral Ecology and\nSociobiology, 76(11), 151.\n\n\nHealy, J., & McInnes, L. (2024). Uniform manifold approximation and\nprojection. Nature Reviews Methods Primers, 4(1), 82.\nhttps://doi.org/10.1038/s43586-024-00363-x\n\n\nHealy, K. (2018). Data visualization: A practical introduction.\nPrinceton University Press.\n\n\nHiggins, P. D. R. (2024). Reproducible medical research with r.\nBookdown. https://bookdown.org/pdr_higgins/rmrwr/\n\n\nHosken, D. J., Blanckenhorn, W. U., & Garner, T. W. J. (2002).\nHeteropopulation males have a fertilization advantage during sperm\ncompetition in the yellow dung fly (scathophaga stercoraria). Proc.\nR. Soc. Lond. B, 269, 1701–1707.\n\n\nIsmay, C., & Kim, A. Y. (2019). Statistical inference via data\nscience: A ModernDive into r and the tidyverse. CRC Press.\n\n\nJohn, M., Korte, A., & Grimm, D. G. (2024). The benefits of\npermutation-based genome-wide association studies. Journal of\nExperimental Botany, 75(17), 5377–5389. https://doi.org/10.1093/jxb/erae280\n\n\nKabacoff, R. (2024). Modern data visualization with r. CRC\nPress.\n\n\nKodric-Brown, A., & Brown, J. H. (1993). Highly structured fish\ncommunities in australian desert springs. Ecology,\n74(6), 1847–1855. https://doi.org/https://doi.org/10.2307/1939942\n\n\nLever, J., Krzywinski, M., & Altman, N. (2017). Principal component\nanalysis. Nature Methods, 14(7), 641–642. https://doi.org/10.1038/nmeth.4346\n\n\nLewis, C. (2024). Data management in large-scale education\nresearch. CRC Press.\n\n\nMarx, V. (2024). Seeing data as t-SNE and UMAP do. Nature\nMethods, 21(6), 930–933. https://doi.org/10.1038/s41592-024-02301-x\n\n\nMatthews, R. (2021). The p-value statement, five years on.\nSignificance, 18(2), 16–19. https://doi.org/https://doi.org/10.1111/1740-9713.01505\n\n\nNovembre, J., Johnson, T., Bryc, K., Kutalik, Z., Boyko, A. R., Auton,\nA., Indap, A., King, K. S., Bergmann, S., Nelson, M. R., Stephens, M.,\n& Bustamante, C. D. (2008). Genes mirror geography within europe.\nNature, 456(7218), 98–101. https://doi.org/10.1038/nature07331\n\n\nNovembre, J., & Stephens, M. (2008). Interpreting principal\ncomponent analyses of spatial population genetic variation. Nature\nGenetics, 40(5), 646–649. https://doi.org/10.1038/ng.139\n\n\nSaccenti, E. (2024). A gentle introduction to principal component\nanalysis using tea-pots, dinosaurs, and pizza. Teaching\nStatistics, 46(1), 38–52. https://doi.org/https://doi.org/10.1111/test.12363\n\n\nSandve, A. A. T., Geir Kjetil AND Nekrutenko. (2013). Ten simple rules\nfor reproducible computational research. PLOS Computational\nBiology, 9(10), 1–4. https://doi.org/10.1371/journal.pcbi.1003285\n\n\nSianta, S. A., Moeller, D. A., & Brandvain, Y. (2024). The extent of\nintrogression between incipient &lt;i&gt;clarkia&lt;/i&gt; species is\ndetermined by temporal environmental variation and mating system.\nProceedings of the National Academy of Sciences,\n121(12), e2316008121. https://doi.org/10.1073/pnas.2316008121\n\n\nSievert, C. (2020). Interactive web-based data visualization with r,\nplotly, and shiny. Chapman; Hall/CRC.\n\n\nStolley, P. D. (1991). When Genius Errs: R. A.\nFisher and the Lung Cancer Controversy. American Journal of\nEpidemiology, 133(5), 416–425. https://doi.org/10.1093/oxfordjournals.aje.a115904\n\n\nSuzuki, Y., Endo, M., Cañas, C., Ayora, S., Alonso, J. C., Sugiyama, H.,\n& Takeyasu, K. (2014). Direct analysis of holliday junction\nresolving enzyme in a DNA origami nanostructure. Nucleic Acids\nResearch, 42(11), 7421–7428. https://doi.org/10.1093/nar/gku320\n\n\nSwierk, L., & Langkilde, T. (2019). Fitness\ncosts of mating with preferred females in a scramble mating\nsystem. Behavioral Ecology, 30(3), 658–665. https://doi.org/10.1093/beheco/arz001\n\n\nTattersall, G. J., Milsom, W. K., Abe, A. S., Brito, S. P., &\nAndrade, D. V. (2004). The thermogenesis of digestion in rattlesnakes.\nJournal of Experimental Biology, 207(4), 579–585. https://doi.org/10.1242/jeb.00790\n\n\nTufte, E. R. (1983). The visual display of quantitative\ninformation (p. 197). pub-gp.\n\n\nTufte, E. R. (1990). Envisioning information. Graphics Press.\n\n\nWainer, H. (2007). The most dangerous equation. American\nScientist, 95(3), 249.\n\n\nWasserstein, R. L., & Lazar, N. A. (2016). The ASA statement on\np-values: Context, process, and purpose. The American\nStatistician, 70(2), 129–133. https://doi.org/10.1080/00031305.2016.1154108\n\n\nWattenberg, M., Viégas, F., & Johnson, I. (2016). How to use t-SNE\neffectively. Distill. https://doi.org/10.23915/distill.00002\n\n\nWhitlock, M. C., & Schluter, D. (2020). The analysis of\nbiological data (Third). Macmillan.\n\n\nWickham, H. (2014b). Tidy data. Journal of Statistical\nSoftware, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10\n\n\nWickham, H. (2014a). Tidy data. Journal of Statistical Software,\nArticles, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10\n\n\nWickham, H. (2016). ggplot2: Elegant graphics for data\nanalysis. Springer-Verlag New York. https://ggplot2.tidyverse.org\n\n\nWilke, C. O. (2019). Fundamentals of data visualization: A primer on\nmaking informative and compelling figures. O’Reilly Media.\n\n\nYi, X., & Latch, E. K. (2022). Nonrandom missing data can bias\nprincipal component analysis inference of population genetic structure.\nMol Ecol Resour, 22(2), 602–611. https://doi.org/10.1111/1755-0998.13498",
    "crumbs": [
      "References"
    ]
  }
]